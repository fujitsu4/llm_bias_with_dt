sentence_id;sentence;dataset
0;In this paper we propose $\epsilon$-Consistent Mixup ($\epsilon$mu).;arxiv
1;$\epsilon$mu is a data-based structural regularization technique that combines Mixup's linear interpolation with consistency regularization in the Mixup direction, by compelling a simple adaptive tradeoff between the two.;arxiv
2;This learnable combination of consistency and interpolation induces a more flexible structure on the evolution of the response across the feature space and is shown to improve semi-supervised classification accuracy on the SVHN and CIFAR10 benchmark datasets, yielding the largest gains in the most challenging low label-availability scenarios.;arxiv
3;Empirical studies comparing $\epsilon$mu and Mixup are presented and provide insight into the mechanisms behind $\epsilon$mu's effectiveness.;arxiv
4;In particular, $\epsilon$mu is found to produce more accurate synthetic labels and more confident predictions than Mixup.;arxiv
5;Autoencoders have useful applications in high energy physics in anomaly detection, particularly for jets - collimated showers of particles produced in collisions such as those at the CERN Large Hadron Collider.;arxiv
6;"We explore the use of graph-based autoencoders, which operate on jets in their ""particle cloud"" representations and can leverage the interdependencies among the particles within a jet, for such tasks.";arxiv
7;Additionally, we develop a differentiable approximation to the energy mover's distance via a graph neural network, which may subsequently be used as a reconstruction loss function for autoencoders.;arxiv
8;Exciting contemporary machine learning problems have recently been phrased in the classic formalism of tree search -- most famously, the game of Go.;arxiv
9;Interestingly, the state-space underlying these sequential decision-making problems often posses a more general latent structure than can be captured by a tree.;arxiv
10;In this work, we develop a probabilistic framework to exploit a search space's latent structure and thereby share information across the search tree.;arxiv
11;We empirically find our algorithm to compare favorably to existing non-probabilistic alternatives in Tic-Tac-Toe and a feature selection application.;arxiv
12;Around the globe, ticks are the culprit of transmitting a variety of bacterial, viral and parasitic diseases.;arxiv
13;The incidence of tick-borne diseases has drastically increased within the last decade, with annual cases of Lyme disease soaring to an estimated 300,000 in the United States alone.;arxiv
14;As a result, more efforts in improving lesion identification approaches and diagnostics for tick-borne illnesses is critical.;arxiv
15;The objective for this study is to build upon the approach used by Burlina et al.;arxiv
16;by using a variety of convolutional neural network models to detect tick-borne skin lesions.;arxiv
17;We expanded the data inputs by acquiring images from Google in seven different languages to test if this would diversify training data and improve the accuracy of skin lesion detection.;arxiv
18;The final dataset included nearly 6,080 images and was trained on a combination of architectures (ResNet 34, ResNet 50, VGG 19, and Dense Net 121).;arxiv
19;We obtained an accuracy of 80.72% with our model trained on the DenseNet 121 architecture.;arxiv
20;Deep neural networks have gained tremendous importance in many computer vision tasks.;arxiv
21;However, their power comes at the cost of large amounts of annotated data required for supervised training.;arxiv
22;In this work we review and compare different techniques available in the literature to improve training results without acquiring additional annotated real-world data.;arxiv
23;This goal is mostly achieved by applying annotation-preserving transformations to existing data or by synthetically creating more data.;arxiv
24;Diagnosis and therapeutic effect assessment of Parkinson disease based on voice data are very important,but its few-shot learning problem is challenging.;arxiv
25;Although deep learning is good at automatic feature extraction, it suffers from few-shot learning problem.;arxiv
26;Therefore, the general effective method is first conduct feature extraction based on prior knowledge, and then carry out feature reduction for subsequent classification.;arxiv
27;To solve these two problems, based on the existing Parkinson speech feature data set, a deep double-side learning ensemble model is designed in this paper that can reconstruct speech features and samples deeply and simultaneously.;arxiv
28;As to feature reconstruction, an embedded deep stacked group sparse auto-encoder is designed in this paper to conduct nonlinear feature transformation, so as to acquire new high-level deep features, and then the deep features are fused with original speech features by L1 regularization feature selection method.;arxiv
29;As to speech sample reconstruction, a deep sample learning algorithm is designed in this paper based on iterative mean clustering to conduct samples transformation, so as to obtain new high-level deep samples.;arxiv
30;Finally, the bagging ensemble learning mode is adopted to fuse the deep feature learning algorithm and the deep samples learning algorithm together, thereby constructing a deep double-side learning ensemble model.;arxiv
31;At the end of this paper, two representative speech datasets of Parkinson's disease were used for verification.;arxiv
32;The experimental results show that the proposed algorithm are effective.;arxiv
33;We investigate the problems of identity and closeness testing over a discrete population from random samples.;arxiv
34;Our goal is to develop efficient testers while guaranteeing Differential Privacy to the individuals of the population.;arxiv
35;We describe an approach that yields sample-efficient differentially private testers for these problems.;arxiv
36;Our theoretical results show that there exist private identity and closeness testers that are nearly as sample-efficient as their non-private counterparts.;arxiv
37;We perform an experimental evaluation of our algorithms on synthetic data.;arxiv
38;Our experiments illustrate that our private testers achieve small type I and type II errors with sample size sublinear in the domain size of the underlying distributions.;arxiv
39;Smart cities around the world have begun monitoring parking areas in order to estimate available parking spots and help drivers looking for parking.;arxiv
40;However, existing approaches are limited by the high cost of sensors that need to be installed throughout the city in order to achieve an accurate estimation.;arxiv
41;This work investigates the extension of estimating parking information from areas equipped with sensors to areas where they are missing.;arxiv
42;Using the derived similarity values, we analyze the adaptation of occupancy rates from monitored- to unmonitored parking areas.;arxiv
43;The ability to measure similarity between documents enables intelligent summarization and analysis of large corpora.;arxiv
44;Past distances between documents suffer from either an inability to incorporate semantic similarities between words or from scalability issues.;arxiv
45;As an alternative, we introduce hierarchical optimal transport as a meta-distance between documents, where documents are modeled as distributions over topics, which themselves are modeled as distributions over words.;arxiv
46;We then solve an optimal transport problem on the smaller topic space to compute a similarity score.;arxiv
47;We give conditions on the topics under which this construction defines a distance, and we relate it to the word mover's distance.;arxiv
48;We evaluate our technique for k-NN classification and show better interpretability and scalability with comparable performance to current methods at a fraction of the cost.;arxiv
49;Speaker diarization, which is to find the speech segments of specific speakers, has been widely used in human-centered applications such as video conferences or human-computer interaction systems.;arxiv
50;In this paper, we propose a self-supervised audio-video synchronization learning method to address the problem of speaker diarization without massive labeling effort.;arxiv
51;We improve the previous approaches by introducing two new loss functions: the dynamic triplet loss and the multinomial loss.;arxiv
52;We test them on a real-world human-computer interaction system and the results show our best model yields a remarkable gain of +8%F1-scoresas well as diarization error rate reduction.;arxiv
53;Finally, we introduce a new large scale audio-video corpus designed to fill the vacancy of audio-video datasets in Chinese.;arxiv
54;Software engineering of network-centric Artificial Intelligence (AI) and Internet of Things (IoT) enabled Cyber-Physical Systems (CPS) and services, involves complex design and validation challenges.;arxiv
55;In this paper, we propose a novel approach, based on the model-driven software engineering paradigm, in particular the domain-specific modeling methodology.;arxiv
56;We focus on a sub-discipline of AI, namely Machine Learning (ML) and propose the delegation of data analytics and ML to the IoT edge.;arxiv
57;This way, we may increase the service quality of ML, for example, its availability and performance, regardless of the network conditions, as well as maintaining the privacy, security and sustainability.;arxiv
58;We let practitioners assign ML tasks to heterogeneous edge devices, including highly resource-constrained embedded microcontrollers with main memories in the order of Kilobytes, and energy consumption in the order of milliwatts.;arxiv
59;Furthermore, we show how software models with different levels of abstraction, namely platform-independent and platform-specific models can be used in the software development process.;arxiv
60;Finally, we validate the proposed approach using a case study addressing the predictive maintenance of a hydraulics system with various networked sensors and actuators.;arxiv
61;Despite considerable progress in maternal healthcare, maternal and perinatal deaths remain high in low-to-middle income countries.;arxiv
62;Fetal ultrasound is an important component of antenatal care, but shortage of adequately trained healthcare workers has limited its adoption.;arxiv
63;"We developed and validated an artificial intelligence (AI) system that uses novice-acquired ""blind sweep"" ultrasound videos to estimate gestational age (GA) and fetal malpresentation.";arxiv
64;We further addressed obstacles that may be encountered in low-resourced settings.;arxiv
65;Using a simplified sweep protocol with real-time AI feedback on sweep quality, we have demonstrated the generalization of model performance to minimally trained novice ultrasound operators using low cost ultrasound devices with on-device AI integration.;arxiv
66;The GA model was non-inferior to standard fetal biometry estimates with as few as two sweeps, and the fetal malpresentation model had high AUC-ROCs across operators and devices.;arxiv
67;Our AI models have the potential to assist in upleveling the capabilities of lightly trained ultrasound operators in low resource settings.;arxiv
68;In this paper, we present the case for a declarative foundation for data-intensive machine learning systems.;arxiv
69;Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems.;arxiv
70;By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine.;arxiv
71;As a proof of concept, we consider two programming models--Pregel and Iterative Map-Reduce-Update---from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan.;arxiv
72;Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.;arxiv
73;Data poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set.;arxiv
74;We consider differential privacy as a defensive measure against this type of attack.;arxiv
75;We show that such learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items.;arxiv
76;However, this protection degrades as the adversary poisons more data.;arxiv
77;To illustrate, we design attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning.;arxiv
78;Experiments show that our methods are effective when the attacker is allowed to poison sufficiently many training items.;arxiv
79;Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold.;arxiv
80;The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering.;arxiv
81;In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero.;arxiv
82;We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants.;arxiv
83;However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator.;arxiv
84;We introduce two-scale loss functions for use in various gradient descent algorithms applied to classification problems via deep neural networks.;arxiv
85;This new method is generic in the sense that it can be applied to a wide range of machine learning architectures, from deep neural networks to support vector machines for example.;arxiv
86;These two-scale loss functions allow to focus the training onto objects in the training set which are not well classified.;arxiv
87;This leads to an increase in several measures of performance for appropriately-defined two-scale loss functions with respect to the more classical cross-entropy when tested on traditional deep neural networks on the MNIST, CIFAR10, and CIFAR100 data-sets.;arxiv
88;We revisit lower bounds on the regret in the case of multi-armed bandit problems.;arxiv
89;We obtain non-asymptotic, distribution-dependent bounds and provide straightforward proofs based only on well-known properties of Kullback-Leibler divergences.;arxiv
90;These bounds show in particular that in an initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase.;arxiv
91;The proof techniques come to the essence of the information-theoretic arguments used and they are deprived of all unnecessary complications.;arxiv
92;Reparameterization of variational auto-encoders with continuous random variables is an effective method for reducing the variance of their gradient estimates.;arxiv
93;In the discrete case, one can perform reparametrization using the Gumbel-Max trick, but the resulting objective relies on an $\arg \max$ operation and is non-differentiable.;arxiv
94;In contrast to previous works which resort to softmax-based relaxations, we propose to optimize it directly by applying the direct loss minimization approach.;arxiv
95;Our proposal extends naturally to structured discrete latent variable models when evaluating the $\arg \max$ operation is tractable.;arxiv
96;We demonstrate empirically the effectiveness of the direct loss minimization technique in variational autoencoders with both unstructured and structured discrete latent variables.;arxiv
97;This paper reviews a wide selection of machine learning models built to predict both the presence of diabetes and the presence of undiagnosed diabetes using eight years of National Health and Nutrition Examination Survey (NHANES) data.;arxiv
98;Models are tuned and compared via their Brier Scores.;arxiv
99;The most relevant variables of the best performing models are then compared.;arxiv
100;A Support Vector Machine with a linear kernel performed best for predicting diabetes, returning a Brier score of 0.0654 and an AUROC of 0.9235 on the test set.;arxiv
101;An elastic net regression performed best for predicting undiagnosed diabetes with a Brier score of 0.0294 and an AUROC of 0.9439 on the test set.;arxiv
102;Similar features appear prominently in the models for both sets of models.;arxiv
103;Blood osmolality, family history, the prevalance of various compounds, and hypertension are key indicators for all diabetes risk.;arxiv
104;For undiagnosed diabetes in particular, there are ethnicity or genetic components which arise as strong correlates as well.;arxiv
105;Federated learning (FL) is an outstanding distributed machine learning framework due to its benefits on data privacy and communication efficiency.;arxiv
106;Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case.;arxiv
107;This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm.;arxiv
108;We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates.;arxiv
109;A complexity reduction mechanism is also incorporated into FL-FDMS, making it both theoretically sound and practically useful.;arxiv
110;Experiments on MNIST and CIFAR-10 confirmed the superior performance of FL-FDMS in handling client dropout in FL.;arxiv
111;Given the rapid rise in energy demand by data centers and computing systems in general, it is fundamental to incorporate energy considerations when designing (scheduling) algorithms.;arxiv
112;Machine learning can be a useful approach in practice by predicting the future load of the system based on, for example, historical data.;arxiv
113;However, the effectiveness of such an approach highly depends on the quality of the predictions and can be quite far from optimal when predictions are sub-par.;arxiv
114;On the other hand, while providing a worst-case guarantee, classical online algorithms can be pessimistic for large classes of inputs arising in practice.;arxiv
115;Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing.;arxiv
116;This raises many concerns from various perspectives, e.g., financial costs and carbon emissions.;arxiv
117;Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention.;arxiv
118;In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one.;arxiv
119;Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efficiency during compression.;arxiv
120;Our compressed BERT with ${1}/{7}$ parameters in Transformer layers performs on-par with, sometimes slightly better than the original BERT in GLUE benchmark.;arxiv
121;To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the benefit of the proposed method on a distilled BERT.;arxiv
122;Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning.;arxiv
123;Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions.;arxiv
124;However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum.;arxiv
125;To seek a solution of higher accuracy than the nuclear norm, in this paper, we propose a rank approximation based on Logarithm-Determinant.;arxiv
126;We consider using this rank approximation for subspace clustering application.;arxiv
127;Our framework can model different kinds of errors and noise.;arxiv
128;Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point.;arxiv
129;The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms.;arxiv
130;We propose two general and falsifiable hypotheses about expectations on generalization error when learning in the context of concept drift.;arxiv
131;One posits that as drift rate increases, the forgetting rate that minimizes generalization error will also increase and vice versa.;arxiv
132;The other posits that as a learner's forgetting rate increases, the bias/variance profile that minimizes generalization error will have lower variance and vice versa.;arxiv
133;These hypotheses lead to the concept of the sweet path, a path through the 3-d space of alternative drift rates, forgetting rates and bias/variance profiles on which generalization error will be minimized, such that slow drift is coupled with low forgetting and low bias, while rapid drift is coupled with fast forgetting and low variance.;arxiv
134;We present experiments that support the existence of such a sweet path.;arxiv
135;We also demonstrate that simple learners that select appropriate forgetting rates and bias/variance profiles are highly competitive with the state-of-the-art in incremental learners for concept drift on real-world drift problems.;arxiv
136;We introduce Coarse-Grained Nonlinear Dynamics, an efficient and universal parameterization of nonlinear system dynamics based on the Volterra series expansion.;arxiv
137;Our efficient parameterization of nonlinear dynamics can be used for regularization, leading to Coarse-Grained Nonlinear System Identification, a technique which requires very little experimental data to identify accurate nonlinear dynamic models.;arxiv
138;We demonstrate the properties of this approach on a simple synthetic problem.;arxiv
139;We also demonstrate this approach experimentally, showing that it identifies an accurate model of the nonlinear voltage to luminosity dynamics of a tungsten filament with less than a second of experimental data.;arxiv
140;However, the improved robustness does not come for free but rather is accompanied by a decrease in overall model accuracy and performance.;arxiv
141;Recent work has shown that, in practical robot learning applications, the effects of adversarial training do not pose a fair trade-off but inflict a net loss when measured in holistic robot performance.;arxiv
142;This work revisits the robustness-accuracy trade-off in robot learning by systematically analyzing if recent advances in robust training methods and theory in conjunction with adversarial robot learning can make adversarial training suitable for real-world robot applications.;arxiv
143;We evaluate a wide variety of robot learning tasks ranging from autonomous driving in a high-fidelity environment amenable to sim-to-real deployment, to mobile robot gesture recognition.;arxiv
144;Our results demonstrate that, while these techniques make incremental improvements on the trade-off on a relative scale, the negative side-effects caused by adversarial training still outweigh the improvements by an order of magnitude.;arxiv
145;We conclude that more substantial advances in robust learning methods are necessary before they can benefit robot learning tasks in practice.;arxiv
146;Adversarial learning has emerged as one of the successful techniques to circumvent the susceptibility of existing methods against adversarial perturbations.;arxiv
147;However, the majority of existing defense methods are tailored to defend against a single category of adversarial perturbation (e.g. $\ell_\infty$-attack).;arxiv
148;In safety-critical applications, this makes these methods extraneous as the attacker can adopt diverse adversaries to deceive the system.;arxiv
149;Moreover, training on multiple perturbations simultaneously significantly increases the computational overhead during training.;arxiv
150;To address these challenges, we propose a novel meta-learning framework that explicitly learns to generate noise to improve the model's robustness against multiple types of attacks.;arxiv
151;Its key component is Meta Noise Generator (MNG) that outputs optimal noise to stochastically perturb a given sample, such that it helps lower the error on diverse adversarial perturbations.;arxiv
152;By utilizing samples generated by MNG, we train a model by enforcing the label consistency across multiple perturbations.;arxiv
153;We validate the robustness of models trained by our scheme on various datasets and against a wide variety of perturbations, demonstrating that it significantly outperforms the baselines across multiple perturbations with a marginal computational cost.;arxiv
154;Increasing the explainability of deep neural networks (DNNs) requires evaluating whether they implement symbolic computation.;arxiv
155;One central symbolic capacity is variable binding: linking an input value to an abstract variable held in system-internal memory.;arxiv
156;Prior work on the computational abilities of DNNs has not resolved the question of whether their internal processes involve variable binding.;arxiv
157;We argue that the reason for this is fundamental, inherent in the way experiments in prior work were designed.;arxiv
158;We provide the first systematic evaluation of the variable binding capacities of the state-of-the-art Transformer networks BERT and RoBERTa.;arxiv
159;Our experiments are designed such that the model must generalize a rule across disjoint subsets of the input vocabulary, and cannot rely on associative pattern matching alone.;arxiv
160;"These findings indicate that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external ""memory"" rather than network-internal symbolic operations involving variable binding.";arxiv
161;Therefore, we propose a novel direction for future work: augmenting the inputs available to circumvent the lack of network-internal variable binding.;arxiv
162;Visual Question Answering (VQA) is of tremendous interest to the research community with important applications such as aiding visually impaired users and image-based search.;arxiv
163;In this work, we explore the use of scene graphs for solving the VQA task.;arxiv
164;We conduct experiments on the GQA dataset which presents a challenging set of questions requiring counting, compositionality and advanced reasoning capability, and provides scene graphs for a large number of images.;arxiv
165;We adopt image + question architectures for use with scene graphs, evaluate various scene graph generation techniques for unseen images, propose a training curriculum to leverage human-annotated and auto-generated scene graphs, and build late fusion architectures to learn from multiple image representations.;arxiv
166;We present a multi-faceted study into the use of scene graphs for VQA, making this work the first of its kind.;arxiv
167;The ubiquitous deployment of monitoring devices in urban flow monitoring systems induces a significant cost for maintenance and operation.;arxiv
168;A technique is required to reduce the number of deployed devices, while preventing the degeneration of data accuracy and granularity.;arxiv
169;In this paper, we present an approach for inferring the real-time and fine-grained crowd flows throughout a city based on coarse-grained observations.;arxiv
170;This task exhibits two challenges: the spatial correlations between coarse- and fine-grained urban flows, and the complexities of external impacts.;arxiv
171;This structure provides outstanding effectiveness and efficiency for small scale upsampling.;arxiv
172;However, the single-pass upsampling used by UrbanFM is insufficient at higher upscaling rates.;arxiv
173;Therefore, we further present UrbanPy, a cascading model for progressive inference of fine-grained urban flows by decomposing the original tasks into multiple subtasks.;arxiv
174;Compared to UrbanFM, such an enhanced structure demonstrates favorable performance for larger-scale inference tasks.;arxiv
175;Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update.;arxiv
176;We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain.;arxiv
177;We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.;arxiv
178;Language Identification (LI) is an important first step in several speech processing systems.;arxiv
179;With a growing number of voice-based assistants, speech LI has emerged as a widely researched field.;arxiv
180;To approach the problem of identifying languages, we can either adopt an implicit approach where only the speech for a language is present or an explicit one where text is available with its corresponding transcript.;arxiv
181;This paper focuses on an implicit approach due to the absence of transcriptive data.;arxiv
182;This paper benchmarks existing models and proposes a new attention based model for language identification which uses log-Mel spectrogram images as input.;arxiv
183;We also present the effectiveness of raw waveforms as features to neural network models for LI tasks.;arxiv
184;For training and evaluation of models, we classified six languages (English, French, German, Spanish, Russian and Italian) with an accuracy of 95.4% and four languages (English, French, German, Spanish) with an accuracy of 96.3% obtained from the VoxForge dataset.;arxiv
185;This approach can further be scaled to incorporate more languages.;arxiv
186;In this paper we develop methods to solve two problems related to time series (TS) analysis using quantum computing: reconstruction and classification.;arxiv
187;We formulate the task of reconstructing a given TS from a training set of data as an unconstrained binary optimization (QUBO) problem, which can be solved by both quantum annealers and gate-model quantum processors.;arxiv
188;We accomplish this by discretizing the TS and converting the reconstruction to a set cover problem, allowing us to perform a one-versus-all method of reconstruction.;arxiv
189;Using the solution to the reconstruction problem, we show how to extend this method to perform semi-supervised classification of TS data.;arxiv
190;We present results indicating our method is competitive with current semi- and unsupervised classification techniques, but using less data than classical techniques.;arxiv
191;We consider the problem of joint source and channel coding of structured data such as natural language over a noisy channel.;arxiv
192;The typical approach to this problem in both theory and practice involves performing source coding to first compress the text and then channel coding to add robustness for the transmission across the channel.;arxiv
193;This approach is optimal in terms of minimizing end-to-end distortion with arbitrarily large block lengths of both the source and channel codes when transmission is over discrete memoryless channels.;arxiv
194;However, the optimality of this approach is no longer ensured for documents of finite length and limitations on the length of the encoding.;arxiv
195;We will show in this scenario that we can achieve lower word error rates by developing a deep learning based encoder and decoder.;arxiv
196;While the approach of separate source and channel coding would minimize bit error rates, our approach preserves semantic information of sentences by first embedding sentences in a semantic space where sentences closer in meaning are located closer together, and then performing joint source and channel coding on these embeddings.;arxiv
197;I train models for the task of neural machine translation for English-Hungarian and Hungarian-English, using the Hunglish2 corpus.;arxiv
198;The main contribution of this work is evaluating different data augmentation methods during the training of NMT models.;arxiv
199;I propose 5 different augmentation methods that are structure-aware, meaning that instead of randomly selecting words for blanking or replacement, the dependency tree of sentences is used as a basis for augmentation.;arxiv
200;I start my thesis with a detailed literature review on neural networks, sequential modeling, neural machine translation, dependency parsing and data augmentation.;arxiv
201;After a detailed exploratory data analysis and preprocessing of the Hunglish2 corpus, I perform experiments with the proposed data augmentation techniques.;arxiv
202;The best model for Hungarian-English achieves a BLEU score of 33.9, while the best model for English-Hungarian achieves a BLEU score of 28.6.;arxiv
203;Integer quantization of neural networks can be defined as the approximation of the high precision computation of the canonical neural network formulation, using reduced integer precision.;arxiv
204;It plays a significant role in the efficient deployment and execution of machine learning (ML) systems, reducing memory consumption and leveraging typically faster computations.;arxiv
205;In this work, we present an integer-only quantization strategy for Long Short-Term Memory (LSTM) neural network topologies, which themselves are the foundation of many production ML systems.;arxiv
206;Our quantization strategy is accurate (e.g. works well with quantization post-training), efficient and fast to execute (utilizing 8 bit integer weights and mostly 8 bit activations), and is able to target a variety of hardware (by leveraging instructions sets available in common CPU architectures, as well as available neural accelerators).;arxiv
207;Automated design synthesis has the potential to revolutionize the modern engineering design process and improve access to highly optimized and customized products across countless industries.;arxiv
208;Successfully adapting generative Machine Learning to design engineering may enable such automated design synthesis and is a research subject of great importance.;arxiv
209;We present a review and analysis of Deep Generative Machine Learning models in engineering design.;arxiv
210;Deep Generative Models (DGMs) typically leverage deep networks to learn from an input dataset and synthesize new designs.;arxiv
211;Recently, DGMs such as feedforward Neural Networks (NNs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and certain Deep Reinforcement Learning (DRL) frameworks have shown promising results in design applications like structural optimization, materials design, and shape synthesis.;arxiv
212;The prevalence of DGMs in engineering design has skyrocketed since 2016.;arxiv
213;Anticipating continued growth, we conduct a review of recent advances to benefit researchers interested in DGMs for design.;arxiv
214;We structure our review as an exposition of the algorithms, datasets, representation methods, and applications commonly used in the current literature.;arxiv
215;In particular, we discuss key works that have introduced new techniques and methods in DGMs, successfully applied DGMs to a design-related domain, or directly supported the development of DGMs through datasets or auxiliary methods.;arxiv
216;We further identify key challenges and limitations currently seen in DGMs across design fields, such as design creativity, handling constraints and objectives, and modeling both form and functional performance simultaneously.;arxiv
217;In our discussion, we identify possible solution pathways as key areas on which to target future work.;arxiv
218;Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs.;arxiv
219;Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT.;arxiv
220;In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.;arxiv
221;The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings.;arxiv
222;Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks.;arxiv
223;We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity).;arxiv
224;We instantiate RobEn to defend against a large family of adversarial typos.;arxiv
225;Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.;arxiv
226;The work discussed and presented in this paper focuses on the history matching of reservoirs by integrating 4D seismic data into the inversion process using machine learning techniques.;arxiv
227;A new integrated scheme for the reconstruction of petrophysical properties with a modified Ensemble Smoother with Multiple Data Assimilation (ES-MDA) in a synthetic reservoir is proposed.;arxiv
228;The permeability field inside the reservoir is parametrised with an unsupervised learning approach, namely K-means with Singular Value Decomposition (K-SVD).;arxiv
229;This is combined with the Orthogonal Matching Pursuit (OMP) technique which is very typical for sparsity promoting regularisation schemes.;arxiv
230;Moreover, seismic attributes, in particular, acoustic impedance, are parametrised with the Discrete Cosine Transform (DCT).;arxiv
231;This novel combination of techniques from machine learning, sparsity regularisation, seismic imaging and history matching aims to address the ill-posedness of the inversion of historical production data efficiently using ES-MDA.;arxiv
232;In the numerical experiments provided, I demonstrate that these sparse representations of the petrophysical properties and the seismic attributes enables to obtain better production data matches to the true production data and to quantify the propagating waterfront better compared to more traditional methods that do not use comparable parametrisation techniques.;arxiv
233;Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012.;arxiv
234;Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue.;arxiv
235;While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all.;arxiv
236;As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others.;arxiv
237;Our first contribution is a benchmark of $8$ NAS methods on $5$ datasets.;arxiv
238;To overcome the hurdle of comparing methods with different search spaces, we propose using a method's relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols.;arxiv
239;Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline.;arxiv
240;We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline.;arxiv
241;To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls.;arxiv
242;While sample sizes in randomized clinical trials are large enough to estimate the average treatment effect well, they are often insufficient for estimation of treatment-covariate interactions critical to studying data-driven precision medicine.;arxiv
243;Observational data from real world practice may play an important role in alleviating this problem.;arxiv
244;One common approach in trials is to predict the outcome of interest with separate regression models in each treatment arm, and estimate the treatment effect based on the contrast of the predictions.;arxiv
245;Unfortunately, this simple approach may induce spurious treatment-covariate interaction in observational studies when the regression model is misspecified.;arxiv
246;Motivated by the need of modeling the number of relapses in multiple sclerosis patients, where the ratio of relapse rates is a natural choice of the treatment effect, we propose to estimate the conditional average treatment effect (CATE) as the ratio of expected potential outcomes, and derive a doubly robust estimator of this CATE in a semiparametric model of treatment-covariate interactions.;arxiv
247;We also provide a validation procedure to check the quality of the estimator on an independent sample.;arxiv
248;We conduct simulations to demonstrate the finite sample performance of the proposed methods, and illustrate their advantages on real data by examining the treatment effect of dimethyl fumarate compared to teriflunomide in multiple sclerosis patients.;arxiv
249;Code Smell, similar to a bad smell, is a surface indication of something tainted but in terms of software writing practices.;arxiv
250;This metric is an indication of a deeper problem lies within the code and is associated with an issue which is prominent to experienced software developers with acceptable coding practices.;arxiv
251;Recent studies have often observed that codes having code smells are often prone to a higher probability of change in the software development cycle.;arxiv
252;In this paper, we developed code smell prediction models with the help of features extracted from source code to predict eight types of code smell.;arxiv
253;Our work also presents the application of data sampling techniques to handle class imbalance problem and feature selection techniques to find relevant feature sets.;arxiv
254;Previous studies had made use of techniques such as Naive - Bayes and Random forest but had not explored deep learning methods to predict code smell.;arxiv
255;A total of 576 distinct Deep Learning models were trained using the features and datasets mentioned above.;arxiv
256;The study concluded that the deep learning models which used data from Synthetic Minority Oversampling Technique gave better results in terms of accuracy, AUC with the accuracy of some models improving from 88.47 to 96.84.;arxiv
257;We consider $Q$-learning with knowledge transfer, using samples from a target reinforcement learning (RL) task as well as source samples from different but related RL tasks.;arxiv
258;We propose transfer learning algorithms for both batch and online $Q$-learning with offline source studies.;arxiv
259;The proposed transferred $Q$-learning algorithm contains a novel re-targeting step that enables vertical information-cascading along multiple steps in an RL task, besides the usual horizontal information-gathering as transfer learning (TL) for supervised learning.;arxiv
260;We establish the first theoretical justifications of TL in RL tasks by showing a faster rate of convergence of the $Q$ function estimation in the offline RL transfer, and a lower regret bound in the offline-to-online RL transfer under certain similarity assumptions.;arxiv
261;Empirical evidences from both synthetic and real datasets are presented to back up the proposed algorithm and our theoretical results.;arxiv
262;Through the quantification of physical activity energy expenditure (PAEE), health care monitoring has the potential to stimulate vital and healthy ageing, inducing behavioural changes in older people and linking these to personal health gains.;arxiv
263;To be able to measure PAEE in a monitoring environment, methods from wearable accelerometers have been developed, however, mainly targeted towards younger people.;arxiv
264;Since elderly subjects differ in energy requirements and range of physical activities, the current models may not be suitable for estimating PAEE among the elderly.;arxiv
265;Because past activities influence present PAEE, we propose a modeling approach known for its ability to model sequential data, the Recurrent Neural Network (RNN).;arxiv
266;To train the RNN for an elderly population, we used the GOTOV dataset with 34 healthy participants of 60 years and older (mean 65 years old), performing 16 different activities.;arxiv
267;We used accelerometers placed on wrist and ankle, and measurements of energy counts by means of indirect calorimetry.;arxiv
268;After optimization, we propose an architecture consisting of an RNN with 3 GRU layers and a feedforward network combining both accelerometer and participant-level data.;arxiv
269;In this paper, we describe our efforts to go beyond the standard facilities of a GRU-based RNN, with the aim of achieving accuracy surpassing the state of the art.;arxiv
270;These efforts include switching aggregation function from mean to dispersion measures (SD, IQR, ...), combining temporal and static data (person-specific details such as age, weight, BMI) and adding symbolic activity data as predicted by a previously trained ML model.;arxiv
271;The resulting architecture manages to increase its performance by approximatelly 10% while decreasing training input by a factor of 10.;arxiv
272;It can thus be employed to investigate associations of PAEE with vitality parameters related to metabolic and cognitive health and mental well-being.;arxiv
273;Symmetry transformations induce invariances which are frequently described with deep latent variable models.;arxiv
274;In many complex domains, such as the chemical space, invariances can be observed, yet the corresponding symmetry transformation cannot be formulated analytically.;arxiv
275;We propose to learn the symmetry transformation with a model consisting of two latent subspaces, where the first subspace captures the target and the second subspace the remaining invariant information.;arxiv
276;Our approach is based on the deep information bottleneck in combination with a continuous mutual information regulariser.;arxiv
277;Unlike previous methods, we focus on the challenging task of minimising mutual information in continuous domains.;arxiv
278;To this end, we base the calculation of mutual information on correlation matrices in combination with a bijective variable transformation.;arxiv
279;Extensive experiments demonstrate that our model outperforms state-of-the-art methods on artificial and molecular datasets.;arxiv
280;Gaussian processes have become a promising tool for various safety-critical settings, since the posterior variance can be used to directly estimate the model error and quantify risk.;arxiv
281;However, state-of-the-art techniques for safety-critical settings hinge on the assumption that the kernel hyperparameters are known, which does not apply in general.;arxiv
282;To mitigate this, we introduce robust Gaussian process uniform error bounds in settings with unknown hyperparameters.;arxiv
283;Our approach computes a confidence region in the space of hyperparameters, which enables us to obtain a probabilistic upper bound for the model error of a Gaussian process with arbitrary hyperparameters.;arxiv
284;Instead, we are able to derive bounds from data in an intuitive fashion.;arxiv
285;We additionally employ the proposed technique to derive performance guarantees for a class of learning-based control problems.;arxiv
286;Experiments show that the bound performs significantly better than vanilla and fully Bayesian Gaussian processes.;arxiv
287;We present the first learning-based visual odometry (VO) model, which generalizes to multiple datasets and real-world scenarios and outperforms geometry-based methods in challenging scenes.;arxiv
288;We achieve this by leveraging the SLAM dataset TartanAir, which provides a large amount of diverse synthetic data in challenging environments.;arxiv
289;Furthermore, to make our VO model generalize across datasets, we propose an up-to-scale loss function and incorporate the camera intrinsic parameters into the model.;arxiv
290;Experiments show that a single model, TartanVO, trained only on synthetic data, without any finetuning, can be generalized to real-world datasets such as KITTI and EuRoC, demonstrating significant advantages over the geometry-based methods on challenging trajectories.;arxiv
291;We consider online sequential decision problems where an agent must balance exploration and exploitation.;arxiv
292;We provide a new analysis showing that any algorithm producing policies in the optimistic set enjoys $\tilde O(\sqrt{AT})$ Bayesian regret for a problem with $A$ actions after $T$ rounds.;arxiv
293;We extend the regret analysis for optimistic policies to bilinear saddle-point problems which include zero-sum matrix games and constrained bandits as special cases.;arxiv
294;In this case we show that Thompson sampling can produce policies outside of the optimistic set and suffer linear regret in some instances.;arxiv
295;The procedure works for any posteriors, \ie, it does not require the posterior to have any special properties, such as log-concavity, unimodality, or smoothness.;arxiv
296;The variational view of the problem has many useful properties, including the ability to tune the exploration-exploitation tradeoff, add regularization, incorporate constraints, and linearly parameterize the policy.;arxiv
297;Current deep learning architectures show remarkable performance when trained in large-scale, controlled datasets.;arxiv
298;However, the predictive ability of these architectures significantly decreases when learning new classes incrementally.;arxiv
299;This is due to their inclination to forget the knowledge acquired from previously seen data, a phenomenon termed catastrophic-forgetting.;arxiv
300;On the other hand, Self-Organizing Maps (SOMs) can model the input space utilizing constrained k-means and thus maintain past knowledge.;arxiv
301;Here, we propose a novel algorithm inspired by biological neurons, termed Dendritic-Self-Organizing Map (DendSOM).;arxiv
302;DendSOM consists of a single layer of SOMs, which extract patterns from specific regions of the input space accompanied by a set of hit matrices, one per SOM, which estimate the association between units and labels.;arxiv
303;The best-matching unit of an input pattern is selected using the maximum cosine similarity rule, while the point-wise mutual information is employed for class inference.;arxiv
304;DendSOM performs unsupervised feature extraction as it does not use labels for targeted updating of the weights.;arxiv
305;It outperforms classical SOMs and several state-of-the-art continual learning algorithms on benchmark datasets, such as the Split-MNIST and Split-CIFAR-10.;arxiv
306;We propose that the incorporation of neuronal properties in SOMs may help remedy catastrophic forgetting.;arxiv
307;This work studies the denoising of piecewise smooth graph signals that exhibit inhomogeneous levels of smoothness over a graph, where the value at each node can be vector-valued.;arxiv
308;We extend the graph trend filtering framework to denoising vector-valued graph signals with a family of non-convex regularizers, which exhibit superior recovery performance over existing convex regularizers.;arxiv
309;Using an oracle inequality, we establish the statistical error rates of first-order stationary points of the proposed non-convex method for generic graphs.;arxiv
310;Furthermore, we present an ADMM-based algorithm to solve the proposed method and establish its convergence.;arxiv
311;Numerical experiments are conducted on both synthetic and real-world data for denoising, support recovery, event detection, and semi-supervised classification.;arxiv
312;In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation.;arxiv
313;The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis.;arxiv
314;It is efficiently trained by optimizing a variant of variational bound on the data likelihood.;arxiv
315;DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation.;arxiv
316;We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster.;arxiv
317;In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.;arxiv
318;Due to the ease of modern data collection, applied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome.;arxiv
319;Generalized linear models (GLMs) offer a particularly interpretable framework for such an analysis.;arxiv
320;Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in parameter dimension, and so are limited to settings with at most tens of thousand parameters.;arxiv
321;We propose to reduce time and memory costs with a low-rank approximation of the data in an approach we call LR-GLM.;arxiv
322;When used with the Laplace approximation or Markov chain Monte Carlo, LR-GLM provides a full Bayesian posterior approximation and admits running times reduced by a full factor of the parameter dimension.;arxiv
323;We rigorously establish the quality of our approximation and show how the choice of rank allows a tunable computational-statistical trade-off.;arxiv
324;Experiments support our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets.;arxiv
325;Fairness in machine learning (ML), the process to understand and correct algorithmic bias, has gained increasing attention with numerous literature being carried out, commonly assume the underlying data is independent and identically distributed (IID).;arxiv
326;On the other hand, graphs are a ubiquitous data structure to capture connections among individual units and is non-IID by nature.;arxiv
327;It is therefore of great importance to bridge the traditional fairness literature designed on IID data and ubiquitous non-IID graph representations to tackle bias in ML systems.;arxiv
328;In this survey, we review such recent advance in fairness amidst non-IID graph data and identify datasets and evaluation metrics available for future research.;arxiv
329;We also point out the limitations of existing work as well as promising future directions.;arxiv
330;When designing variational autoencoders (VAEs) or other types of latent space models, the dimensionality of the latent space is typically defined upfront.;arxiv
331;In this process, it is possible that the number of dimensions is under- or overprovisioned for the application at hand.;arxiv
332;In case the dimensionality is not predefined, this parameter is usually determined using time- and resource-consuming cross-validation.;arxiv
333;For these reasons we have developed a technique to shrink the latent space dimensionality of VAEs automatically and on-the-fly during training using Generalized ELBO with Constrained Optimization (GECO) and the $L_0$-Augment-REINFORCE-Merge ($L_0$-ARM) gradient estimator.;arxiv
334;The GECO optimizer ensures that we are not violating a predefined upper bound on the reconstruction error.;arxiv
335;This paper presents the algorithmic details of our method along with experimental results on five different datasets.;arxiv
336;We find that our training procedure is stable and that the latent space can be pruned effectively without violating the GECO constraints.;arxiv
337;This article describes Jigsaw, a convolutional neural network (CNN) used in geosciences and based on Inception but tailored for geoscientific analyses.;arxiv
338;Introduces JigsawHSI (based on Jigsaw) and uses it on the land-use land-cover (LULC) classification problem with the Indian Pines, Pavia University and Salinas hyperspectral image data sets.;arxiv
339;The network is compared against HybridSN, a spectral-spatial 3D-CNN followed by 2D-CNN that achieves state-of-the-art results on the datasets.;arxiv
340;This short article proves that JigsawHSI is able to meet or exceed HybridSN's performance in all three cases.;arxiv
341;Additionally, the use of jigsaw in geosciences is highlighted, while the code and toolkit are made available.;arxiv
342;We investigate optimal posteriors for recently introduced \cite{begin2016pac} chi-squared divergence based PAC-Bayesian bounds in terms of nature of their distribution, scalability of computations, and test set performance.;arxiv
343;For a finite classifier set, we deduce bounds for three distance functions: KL-divergence, linear and squared distances.;arxiv
344;Optimal posterior weights are proportional to deviations of empirical risks, usually with subset support.;arxiv
345;For uniform prior, it is sufficient to search among posteriors on classifier subsets ordered by these risks.;arxiv
346;We show the bound minimization for linear distance as a convex program and obtain a closed-form expression for its optimal posterior.;arxiv
347;Whereas that for squared distance is a quasi-convex program under a specific condition, and the one for KL-divergence is non-convex optimization (a difference of convex functions).;arxiv
348;To compute such optimal posteriors, we derive fast converging fixed point (FP) equations.;arxiv
349;We apply these approaches to a finite set of SVM regularization parameter values to yield stochastic SVMs with tight bounds.;arxiv
350;Chi-squared divergence based posteriors have weaker bounds and worse test errors, hinting at an underlying regularization by KL-divergence based posteriors.;arxiv
351;Our study highlights the impact of divergence function on the performance of PAC-Bayesian classifiers.;arxiv
352;We compare our stochastic classifiers with cross-validation based deterministic classifier.;arxiv
353;The latter has better test errors, but ours is more sample robust, has quantifiable generalization guarantees, and is computationally much faster.;arxiv
354;Due to the insufficient measurements in the distribution system state estimation (DSSE), full observability and redundant measurements are difficult to achieve without using the pseudo measurements.;arxiv
355;The matrix completion state estimation (MCSE) combines the matrix completion and power system model to estimate voltage by exploring the low-rank characteristics of the matrix.;arxiv
356;This paper proposes a robust matrix completion state estimation (RMCSE) to estimate the voltage in a distribution system under a low-observability condition.;arxiv
357;Tradition state estimation weighted least squares (WLS) method requires full observability to calculate the states and needs redundant measurements to proceed a bad data detection.;arxiv
358;The proposed method improves the robustness of the MCSE to bad data by minimizing the rank of the matrix and measurements residual with different weights.;arxiv
359;It can estimate the system state in a low-observability system and has robust estimates without the bad data detection process in the face of multiple bad data.;arxiv
360;The method is numerically evaluated on the IEEE 33-node radial distribution system.;arxiv
361;The estimation performance and robustness of RMCSE are compared with the WLS with the largest normalized residual bad data identification (WLS-LNR), and the MCSE.;arxiv
362;This paper proposes a representational model for image pairs such as consecutive video frames that are related by local pixel displacements, in the hope that the model may shed light on motion perception in primary visual cortex (V1).;arxiv
363;When the image frame undergoes changes due to local pixel displacements, the vectors are multiplied by the matrices that represent the local displacements.;arxiv
364;Thus the vector representation is equivariant as it varies according to the local displacements.;arxiv
365;Our experiments show that our model can learn Gabor-like filter pairs of quadrature phases.;arxiv
366;The profiles of the learned filters match those of simple cells in Macaque V1.;arxiv
367;Moreover, we demonstrate that the model can learn to infer local motions in either a supervised or unsupervised manner.;arxiv
368;With such a simple model, we achieve competitive results on optical flow estimation.;arxiv
369;Energy-based modeling is a promising approach to unsupervised learning, which yields many downstream applications from a single model.;arxiv
370;"The main difficulty in learning energy-based models with the ""contrastive approaches"" is the generation of samples from the current energy function at each iteration.";arxiv
371;Many advances have been made to accomplish this subroutine cheaply.;arxiv
372;Nevertheless, all such sampling paradigms run MCMC targeting the current model, which requires infinitely long chains to generate samples from the true energy distribution and is problematic in practice.;arxiv
373;This paper proposes an alternative approach to getting these samples and avoiding crude MCMC sampling from the current model.;arxiv
374;We accomplish this by viewing the evolution of the modeling distribution as (i) the evolution of the energy function, and (ii) the evolution of the samples from this distribution along some vector field.;arxiv
375;We subsequently derive this time-dependent vector field such that the particles following this field are approximately distributed as the current density model.;arxiv
376;Thereby we match the evolution of the particles with the evolution of the energy function prescribed by the learning procedure.;arxiv
377;Importantly, unlike Monte Carlo sampling, our method targets to match the current distribution in a finite time.;arxiv
378;Finally, we demonstrate its effectiveness empirically compared to MCMC-based learning methods.;arxiv
379;Optimal decision-making in social settings is often based on forecasts from time series (TS) data.;arxiv
380;Recently, several approaches using deep neural networks (DNNs) such as recurrent neural networks (RNNs) have been introduced for TS forecasting and have shown promising results.;arxiv
381;However, the applicability of these approaches is being questioned for TS settings where there is a lack of quality training data and where the TS to forecast exhibit complex behaviors.;arxiv
382;Examples of such settings include financial TS forecasting, where producing accurate and consistent long-term forecasts is notoriously difficult.;arxiv
383;In this work, we investigate whether DNN-based models can be used to forecast these TS conjointly by learning a joint representation of the series instead of computing the forecast from the raw time-series representations.;arxiv
384;To this end, we make use of the dynamic factor graph (DFG) to build a multivariate autoregressive model.;arxiv
385;We investigate a common limitation of RNNs that rely on the DFG framework and propose a novel variable-length attention-based mechanism (ACTM) to address it.;arxiv
386;With ACTM, it is possible to vary the autoregressive order of a TS model over time and model a larger set of probability distributions than with previous approaches.;arxiv
387;Using this mechanism, we propose a self-supervised DNN architecture for multivariate TS forecasting that learns and takes advantage of the relationships between them.;arxiv
388;We test our model on two datasets covering 19 years of investment fund activities.;arxiv
389;Our experimental results show that the proposed approach significantly outperforms typical DNN-based and statistical models at forecasting the 21-day price trajectory.;arxiv
390;We point out how improving forecasting accuracy and knowing which forecaster to use can improve the excess return of autonomous trading strategies.;arxiv
391;The majority of online reviews consist of plain-text feedback together with a single numeric score.;arxiv
392;For example, a user's impression of an audiobook presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products.;arxiv
393;In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product.;arxiv
394;By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we use our model to uncover which parts of a review discuss which of the rated aspects.;arxiv
395;Second, we use our model to summarize reviews, which for us means finding the sentences that best explain a user's rating.;arxiv
396;Finally, since aspect ratings are optional in many of the datasets we consider, we use our model to recover those ratings that are missing from a user's evaluation.;arxiv
397;Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce.;arxiv
398;The Gauss-Manin connection of a family of hypersurfaces governs the change of the period matrix along the family.;arxiv
399;This connection can be complicated even when the equations defining the family look simple.;arxiv
400;When this is the case, it is computationally expensive to compute the period matrices of varieties in the family via homotopy continuation.;arxiv
401;We train neural networks that can quickly and reliably guess the complexity of the Gauss-Manin connection of a pencil of hypersurfaces.;arxiv
402;Idioms pose problems to almost all Machine Translation systems.;arxiv
403;This type of language is very frequent in day-to-day language use and cannot be simply ignored.;arxiv
404;The recent interest in memory augmented models in the field of Language Modelling has aided the systems to achieve good results by bridging long-distance dependencies.;arxiv
405;In this paper we explore the use of such techniques into a Neural Machine Translation system to help in translation of idiomatic language.;arxiv
406;The recognition network in deep latent variable models such as variational autoencoders (VAEs) relies on amortized inference for efficient posterior approximation that can scale up to large datasets.;arxiv
407;However, this technique has also been demonstrated to select suboptimal variational parameters, often resulting in considerable additional error called the amortization gap.;arxiv
408;To close the amortization gap and improve the training of the generative model, recent works have introduced an additional refinement step that applies stochastic variational inference (SVI) to improve upon the variational parameters returned by the amortized inference model.;arxiv
409;In this paper, we propose the Buffered Stochastic Variational Inference (BSVI), a new refinement procedure that makes use of SVI's sequence of intermediate variational proposal distributions and their corresponding importance weights to construct a new generalized importance-weighted lower bound.;arxiv
410;We demonstrate empirically that training the variational autoencoders with BSVI consistently out-performs SVI, yielding an improved training procedure for VAEs.;arxiv
411;This paper proposes an enhancement of convolutional neural networks for object detection in resource-constrained robotics through a geometric input transformation called Visual Mesh.;arxiv
412;It uses object geometry to create a graph in vision space, reducing computational complexity by normalizing the pixel and feature density of objects.;arxiv
413;The experiments compare the Visual Mesh with several other fast convolutional neural networks.;arxiv
414;The results demonstrate execution times sixteen times quicker than the fastest competitor tested, while achieving outstanding accuracy.;arxiv
415;In this paper, we present a novel two-stage metric learning algorithm.;arxiv
416;We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points.;arxiv
417;Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold.;arxiv
418;This induces in the input data space a new family of distance metric with unique properties.;arxiv
419;Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite.;arxiv
420;Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation.;arxiv
421;We evaluate its performance on a number of datasets.;arxiv
422;It outperforms significantly other metric learning methods and SVM.;arxiv
423;In the recent years money laundering schemes have grown in complexity and speed of realization, affecting financial institutions and millions of customers globally.;arxiv
424;Existing topologies and models for AML analysis and information sharing are subject to major limitations, such as compliance with regulatory constraints, extended infrastructure to run high-computation algorithms, data quality and span, proving cumbersome and costly to execute, federate, and interpret.;arxiv
425;This paper proposes a new topology for exploring multi-banking customer social relations in AML context -- customer-to-customer, customer-to-transaction, and transaction-to-transaction -- using a 3D modeling topological algebra formulated through Poincar\'e embeddings.;arxiv
426;Generative adversarial networks (GANs) are one powerful type of deep learning models that have been successfully utilized in numerous fields.;arxiv
427;They belong to a broader family called generative methods, which generate new data with a probabilistic model by learning sample distribution from real examples.;arxiv
428;In the clinical context, GANs have shown enhanced capabilities in capturing spatially complex, nonlinear, and potentially subtle disease effects compared to traditional generative methods.;arxiv
429;This review appraises the existing literature on the applications of GANs in imaging studies of various neurological conditions, including Alzheimer's disease, brain tumors, brain aging, and multiple sclerosis.;arxiv
430;We provide an intuitive explanation of various GAN methods for each application and further discuss the main challenges, open questions, and promising future directions of leveraging GANs in neuroimaging.;arxiv
431;We aim to bridge the gap between advanced deep learning methods and neurology research by highlighting how GANs can be leveraged to support clinical decision making and contribute to a better understanding of the structural and functional patterns of brain diseases.;arxiv
432;The existing publications demonstrate that the limit order book data is useful in predicting short-term volatility in stock markets.;arxiv
433;Since stocks are not independent, changes on one stock can also impact other related stocks.;arxiv
434;In this paper, we are interested in forecasting short-term realized volatility in a multivariate approach based on limit order book data and relational data.;arxiv
435;To achieve this goal, we introduce Graph Transformer Network for Volatility Forecasting.;arxiv
436;The model allows to combine limit order book features and an unlimited number of temporal and cross-sectional relations from different sources.;arxiv
437;Through experiments based on about 500 stocks from S&P 500 index, we find a better performance for our model than for other benchmarks.;arxiv
438;Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles.;arxiv
439;Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance.;arxiv
440;We conjecture that ineffective exploration in large overactuated action spaces is a key problem.;arxiv
441;This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems.;arxiv
442;We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction.;arxiv
443;By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.;arxiv
444;Pruning unimportant parameters can allow deep neural networks (DNNs) to reduce their heavy computation and memory requirements.;arxiv
445;A saliency metric estimates which parameters can be safely pruned with little impact on the classification performance of the DNN.;arxiv
446;Many saliency metrics have been proposed, each within the context of a wider pruning algorithm.;arxiv
447;The result is that it is difficult to separate the effectiveness of the saliency metric from the wider pruning algorithm that surrounds it.;arxiv
448;Similar-looking saliency metrics can yield very different results because of apparently minor design choices.;arxiv
449;We propose a taxonomy of saliency metrics based on four mostly-orthogonal principal components.;arxiv
450;We show that a broad range of metrics from the pruning literature can be grouped according to these components.;arxiv
451;Our taxonomy not only serves as a guide to prior work, but allows us to construct new saliency metrics by exploring novel combinations of our taxonomic components.;arxiv
452;We perform an in-depth experimental investigation of more than 300 saliency metrics.;arxiv
453;Our results provide decisive answers to open research questions, and demonstrate the importance of reduction and scaling when pruning groups of weights.;arxiv
454;We find that some of our constructed metrics can outperform the best existing state-of-the-art metrics for convolutional neural network channel pruning.;arxiv
455;The problem of solving partial differential equations (PDEs) can be formulated into a least-squares minimization problem, where neural networks are used to parametrize PDE solutions.;arxiv
456;A global minimizer corresponds to a neural network that solves the given PDE.;arxiv
457;In this paper, we show that the gradient descent method can identify a global minimizer of the least-squares optimization for solving second-order linear PDEs with two-layer neural networks under the assumption of over-parametrization.;arxiv
458;We also analyze the generalization error of the least-squares optimization for second-order linear PDEs and two-layer neural networks, when the right-hand-side function of the PDE is in a Barron-type space and the least-squares optimization is regularized with a Barron-type norm, without the over-parametrization assumption.;arxiv
459;Estimating predictive uncertainty is crucial for many computer vision tasks, from image classification to autonomous driving systems.;arxiv
460;Hamiltonian Monte Carlo (HMC) is an sampling method for performing Bayesian inference.;arxiv
461;On the other hand, Dropout regularization has been proposed as an approximate model averaging technique that tends to improve generalization in large scale models such as deep neural networks.;arxiv
462;Although, HMC provides convergence guarantees for most standard Bayesian models, it does not handle discrete parameters arising from Dropout regularization.;arxiv
463;In this paper, we present a robust methodology for improving predictive uncertainty in classification problems, based on Dropout and Hamiltonian Monte Carlo.;arxiv
464;Even though Dropout induces a non-smooth energy function with no such convergence guarantees, the resulting discretization of the Hamiltonian proves empirical success.;arxiv
465;The proposed method allows to effectively estimate the predictive accuracy and to provide better generalization for difficult test examples.;arxiv
466;Urban ride-hailing demand prediction is a crucial but challenging task for intelligent transportation system construction.;arxiv
467;Predictable ride-hailing demand can facilitate more reasonable vehicle scheduling and online car-hailing platform dispatch.;arxiv
468;Conventional deep learning methods with no external structured data can be accomplished via hybrid models of CNNs and RNNs by meshing plentiful pixel-level labeled data, but spatial data sparsity and limited learning capabilities on temporal long-term dependencies are still two striking bottlenecks.;arxiv
469;To address these limitations, we propose a new virtual graph modeling method to focus on significant demand regions and a novel Deep Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) to strengthen learning capabilities of spatial dynamics and temporal long-term dependencies.;arxiv
470;Specifically, DMVST-VGNN integrates the structures of 1D Convolutional Neural Network, Multi Graph Attention Neural Network and Transformer layer, which correspond to short-term temporal dynamics view, spatial dynamics view and long-term temporal dynamics view respectively.;arxiv
471;In this paper, experiments are conducted on two large-scale New York City datasets in fine-grained prediction scenes.;arxiv
472;And the experimental results demonstrate effectiveness and superiority of DMVST-VGNN framework in significant citywide ride-hailing demand prediction.;arxiv
473;High-quality education is one of the keys to achieving a more sustainable world.;arxiv
474;The recent COVID-19 epidemic has triggered the outbreak of online education, which has enabled both students and teachers to learn and teach at home.;arxiv
475;Meanwhile, it is now possible to record and research a large amount of learning data using online learning platforms in order to offer better intelligent educational services.;arxiv
476;Knowledge Tracing (KT), which aims to monitor students' evolving knowledge state, is a fundamental and crucial task to support these intelligent services.;arxiv
477;Therefore, an increasing amount of research attention has been paid to this emerging area and considerable progress has been made.;arxiv
478;In this survey, we propose a new taxonomy of existing basic KT models from a technical perspective and provide a comprehensive overview of these models in a systematic manner.;arxiv
479;In addition, many variants of KT models have been proposed to capture more complete learning process.;arxiv
480;We then review these variants involved in three phases of the learning process: before, during, and after the student learning, respectively.;arxiv
481;Moreover, we present several typical applications of KT in different educational scenarios.;arxiv
482;Finally, we provide some potential directions for future research in this fast-growing field.;arxiv
483;Formation mechanisms are fundamental to the study of complex networks, but learning them from observations is challenging.;arxiv
484;In real-world domains, one often has access only to the final constructed graph, instead of the full construction process, and observed graphs exhibit complex structural properties.;arxiv
485;In this work, we propose GraphOpt, an end-to-end framework that jointly learns an implicit model of graph structure formation and discovers an underlying optimization mechanism in the form of a latent objective function.;arxiv
486;The learned objective can serve as an explanation for the observed graph properties, thereby lending itself to transfer across different graphs within a domain.;arxiv
487;GraphOpt poses link formation in graphs as a sequential decision-making process and solves it using maximum entropy inverse reinforcement learning algorithm.;arxiv
488;Further, it employs a novel continuous latent action space that aids scalability.;arxiv
489;Empirically, we demonstrate that GraphOpt discovers a latent objective transferable across graphs with different characteristics.;arxiv
490;GraphOpt also learns a robust stochastic policy that achieves competitive link prediction performance without being explicitly trained on this task and further enables construction of graphs with properties similar to those of the observed graph.;arxiv
491;Despite their recent success on image denoising, the need for deep and complex architectures still hinders the practical usage of CNNs.;arxiv
492;Older but computationally more efficient methods such as BM3D remain a popular choice, especially in resource-constrained scenarios.;arxiv
493;In this study, we aim to find out whether compact neural networks can learn to produce competitive results as compared to BM3D for AWGN image denoising.;arxiv
494;To this end, we configure networks with only two hidden layers and employ different neuron models and layer widths for comparing the performance with BM3D across different AWGN noise levels.;arxiv
495;Our results conclusively show that the recently proposed self-organized variant of operational neural networks based on a generative neuron model (Self-ONNs) is not only a better choice as compared to CNNs, but also provide competitive results as compared to BM3D and even significantly surpass it for high noise levels.;arxiv
496;For an autonomous agent, executing a poor policy may be costly or even dangerous.;arxiv
497;For such agents, it is desirable to determine confidence interval lower bounds on the performance of any given policy without executing said policy.;arxiv
498;Current methods for exact high confidence off-policy evaluation that use importance sampling require a substantial amount of data to achieve a tight lower bound.;arxiv
499;Existing model-based methods only address the problem in discrete state spaces.;arxiv
500;Since exact bounds are intractable for many domains we trade off strict guarantees of safety for more data-efficient approximate bounds.;arxiv
501;In this context, we propose two bootstrapping off-policy evaluation methods which use learned MDP transition models in order to estimate lower confidence bounds on policy performance with limited data in both continuous and discrete state spaces.;arxiv
502;Since direct use of a model may introduce bias, we derive a theoretical upper bound on model bias for when the model transition function is estimated with i.i.d. trajectories.;arxiv
503;This bound broadens our understanding of the conditions under which model-based methods have high bias.;arxiv
504;Finally, we empirically evaluate our proposed methods and analyze the settings in which different bootstrapping off-policy confidence interval methods succeed and fail.;arxiv
505;Delineating the associations between images and a vector of covariates is of central interest in medical imaging studies.;arxiv
506;To tackle this problem of image response regression, we propose a novel nonparametric approach in the framework of spatially varying coefficient models, where the spatially varying functions are estimated through deep neural networks.;arxiv
507;Compared to existing solutions, the proposed method explicitly accounts for spatial smoothness and subject heterogeneity, has straightforward interpretations, and is highly flexible and accurate in capturing complex association patterns.;arxiv
508;A key idea in our approach is to treat the image voxels as the effective samples, which not only alleviates the limited sample size issue that haunts the majority of medical imaging studies, but also leads to more robust and reproducible results.;arxiv
509;Focusing on a broad family of piecewise smooth functions, we establish the estimation and selection consistency, and derive the asymptotic error bounds.;arxiv
510;We demonstrate the efficacy of the method through intensive simulations, and further illustrate its advantages with analyses of two functional magnetic resonance imaging datasets.;arxiv
511;Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains.;arxiv
512;Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information.;arxiv
513;The models should not expose private information in these datasets.;arxiv
514;Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy.;arxiv
515;Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.;arxiv
516;In 2021 the Johns Hopkins University Applied Physics Laboratory held an internal challenge to develop artificially intelligent (AI) agents that could excel at the collaborative card game Hanabi.;arxiv
517;Agents were evaluated on their ability to play with human players whom the agents had never previously encountered.;arxiv
518;This study details the development of the agent that won the challenge by achieving a human-play average score of 16.5, outperforming the current state-of-the-art for human-bot Hanabi scores.;arxiv
519;The winning agent's development consisted of observing and accurately modeling the author's decision making in Hanabi, then training with a behavioral clone of the author.;arxiv
520;Notably, the agent discovered a human-complementary play style by first mimicking human decision making, then exploring variations to the human-like strategy that led to higher simulated human-bot scores.;arxiv
521;This work examines in detail the design and implementation of this human compatible Hanabi teammate, as well as the existence and implications of human-complementary strategies and how they may be explored for more successful applications of AI in human machine teams.;arxiv
522;We propose Bayesian extensions of two nonparametric regression methods which are kernel and mutual $k$-nearest neighbor regression methods.;arxiv
523;Derived based on Gaussian process models for regression, the extensions provide distributions for target value estimates and the framework to select the hyperparameters.;arxiv
524;It is shown that both the proposed methods asymptotically converge to kernel and mutual $k$-nearest neighbor regression methods, respectively.;arxiv
525;The simulation results show that the proposed methods can select proper hyperparameters and are better than or comparable to the former methods for an artificial data set and a real world data set.;arxiv
526;Deep neural networks (DNNs) exhibit great success on many tasks with the help of large-scale well annotated datasets.;arxiv
527;Training on these noisy labeled datasets may adversely deteriorate their generalization performance.;arxiv
528;Existing methods either rely on complex training stage division or bring too much computation for marginal performance improvement.;arxiv
529;In this paper, we propose a Temporal Calibrated Regularization (TCR), in which we utilize the original labels and the predictions in the previous epoch together to make DNN inherit the simple pattern it has learned with little overhead.;arxiv
530;We conduct extensive experiments on various neural network architectures and datasets, and find that it consistently enhances the robustness of DNNs to label noise.;arxiv
531;We introduce the binacox, a prognostic method to deal with the problem of detecting multiple cut-points per features in a multivariate setting where a large number of continuous features are available.;arxiv
532;The method is based on the Cox model and combines one-hot encoding with the binarsity penalty, which uses total-variation regularization together with an extra linear constraint, and enables feature selection.;arxiv
533;Original nonasymptotic oracle inequalities for prediction (in terms of Kullback-Leibler divergence) and estimation with a fast rate of convergence are established.;arxiv
534;The statistical performance of the method is examined in an extensive Monte Carlo simulation study, and then illustrated on three publicly available genetic cancer datasets.;arxiv
535;On these high-dimensional datasets, our proposed method significantly outperforms state-of-the-art survival models regarding risk prediction in terms of the C-index, with a computing time orders of magnitude faster.;arxiv
536;In addition, it provides powerful interpretability from a clinical perspective by automatically pinpointing significant cut-points in relevant variables.;arxiv
537;"Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage ""fair"" outcomes.";arxiv
538;Less attention has been paid, however, to the ethical foundations which underlie such efforts.;arxiv
539;Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter.;arxiv
540;Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness.;arxiv
541;Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future.;arxiv
542;In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism.;arxiv
543;We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.;arxiv
544;Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems.;arxiv
545;To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data.;arxiv
546;However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution.;arxiv
547;This makes the feedback data for long-tail items extremely sparse.;arxiv
548;Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations.;arxiv
549;The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features.;arxiv
550;Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization.;arxiv
551;Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.;arxiv
552;We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively.;arxiv
553;Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques.;arxiv
554;We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic.;arxiv
555;Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.;arxiv
556;Both efficient neural networks and hardware accelerators are being explored to speed up DNN inference on edge devices.;arxiv
557;For example, MobileNet uses depthwise separable convolution to achieve much lower latency, while systolic arrays provide much higher performance per watt.;arxiv
558;Interestingly however, the combination of these two ideas is inefficient: The computational patterns of depth-wise separable convolution are not systolic and lack data reuse to saturate the systolic array's constrained dataflow.;arxiv
559;In this paper, we propose FuSeConv (Fully-Separable Convolution) as a drop-in replacement for depth-wise separable convolution.;arxiv
560;FuSeConv generalizes the decomposition of convolutions fully to separable 1D convolutions along spatial and depth dimensions.;arxiv
561;The resultant computation is systolic and efficiently utilizes the systolic array with a slightly modified dataflow.;arxiv
562;With FuSeConv, we achieve a significant speed-up of 3x-7x with the MobileNet family of networks on a systolic array of size 64x64, with comparable accuracy on the ImageNet dataset.;arxiv
563;The high speed-up motivates exploration of hardware-aware Neural Operator Search (NOS) in complement to ongoing efforts on Neural Architecture Search (NAS).;arxiv
564;Many off-policy prediction learning algorithms have been proposed in the past decade, but it remains unclear which algorithms learn faster than others.;arxiv
565;We empirically compare 11 off-policy prediction learning algorithms with linear function approximation on two small tasks: the Rooms task, and the High Variance Rooms task.;arxiv
566;The tasks are designed such that learning fast in them is challenging.;arxiv
567;In the Rooms task, the product of importance sampling ratios can be as large as $2^{14}$ and can sometimes be two.;arxiv
568;To control the high variance caused by the product of the importance sampling ratios, step size should be set small, which in turn slows down learning.;arxiv
569;The High Variance Rooms task is more extreme in that the product of the ratios can become as large as $2^{14}\times 25$.;arxiv
570;This paper builds upon the empirical study of off-policy prediction learning algorithms by Ghiassian and Sutton (2021).;arxiv
571;We consider the same set of algorithms as theirs and employ the same experimental methodology.;arxiv
572;We found that the algorithms' performance is highly affected by the variance induced by the importance sampling ratios.;arxiv
573;We observed that Emphatic TD($\lambda$) tends to have lower asymptotic error than other algorithms, but might learn more slowly in some cases.;arxiv
574;We suggest algorithms for practitioners based on their problem of interest, and suggest approaches that can be applied to specific algorithms that might result in substantially improved algorithms.;arxiv
575;Tensor networks are factorisations of high rank tensors into networks of lower rank tensors and have primarily been used to analyse quantum many-body problems.;arxiv
576;Tensor networks have seen a recent surge of interest in relation to supervised learning tasks with a focus on image classification.;arxiv
577;In this work, we improve upon the matrix product state (MPS) tensor networks that can operate on one-dimensional vectors to be useful for working with 2D and 3D medical images.;arxiv
578;We treat small image regions as orderless, squeeze their spatial information into feature dimensions and then perform MPS operations on these locally orderless regions.;arxiv
579;These local representations are then aggregated in a hierarchical manner to retain global structure.;arxiv
580;The proposed locally orderless tensor network (LoTeNet) is compared with relevant methods on three datasets.;arxiv
581;The architecture of LoTeNet is fixed in all experiments and we show it requires lesser computational resources to attain performance on par or superior to the compared methods.;arxiv
582;Modern solutions to the single image super-resolution (SISR) problem using deep neural networks aim not only at better performance accuracy but also at a lighter and computationally efficient model.;arxiv
583;To that end, recently, neural architecture search (NAS) approaches have shown some tremendous potential.;arxiv
584;Following the same underlying, in this paper, we suggest a novel trilevel NAS method that provides a better balance between different efficiency metrics and performance to solve SISR.;arxiv
585;Unlike available NAS, our search is more complete, and therefore it leads to an efficient, optimized, and compressed architecture.;arxiv
586;To make the search on trilevel spaces differentiable and efficient, we exploit a new sparsestmax technique that is excellent at generating sparse distributions of individual neural architecture candidates so that they can be better disentangled for the final selection from the enlarged search space.;arxiv
587;We further introduce the sorting technique to the sparsestmax relaxation for better network-level compression.;arxiv
588;The proposed NAS optimization additionally facilitates simultaneous search and training in a single phase, reducing search time and train time.;arxiv
589;Comprehensive evaluations on the benchmark datasets show our method's clear superiority over the state-of-the-art NAS in terms of a good trade-off between model size, performance, and efficiency.;arxiv
590;Speech sounds of spoken language are obtained by varying configuration of the articulators surrounding the vocal tract.;arxiv
591;They contain abundant information that can be utilized to better understand the underlying mechanism of human speech production.;arxiv
592;We propose a novel deep neural network-based learning framework that understands acoustic information in the variable-length sequence of vocal tract shaping during speech production, captured by real-time magnetic resonance imaging (rtMRI), and translate it into text.;arxiv
593;The proposed framework comprises of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end.;arxiv
594;On the USC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better compared to the existing models.;arxiv
595;To the best of our knowledge, this is the first study that demonstrates the recognition of entire spoken sentence based on an individual's articulatory motions captured by rtMRI video.;arxiv
596;Results suggest that each sub-regions distortion is affected by both emotion and gender.;arxiv
597;Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios.;arxiv
598;Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages.;arxiv
599;To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations.;arxiv
600;We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages.;arxiv
601;This paper introduces, for the first time to our knowledge, physics-informed neural networks to accurately estimate the AC-OPF result and delivers rigorous guarantees about their performance.;arxiv
602;Power system operators, along with several other actors, are increasingly using Optimal Power Flow (OPF) algorithms for a wide number of applications, including planning and real-time operations.;arxiv
603;However, in its original form, the AC Optimal Power Flow problem is often challenging to solve as it is non-linear and non-convex.;arxiv
604;Besides the large number of approximations and relaxations, recent efforts have also been focusing on Machine Learning approaches, especially neural networks.;arxiv
605;So far, however, these approaches have only partially considered the wide number of physical models available during training.;arxiv
606;And, more importantly, they have offered no guarantees about potential constraint violations of their output.;arxiv
607;Our approach (i) introduces the AC power flow equations inside neural network training and (ii) integrates methods that rigorously determine and reduce the worst-case constraint violations across the entire input domain, while maintaining the optimality of the prediction.;arxiv
608;We demonstrate how physics-informed neural networks achieve higher accuracy and lower constraint violations than standard neural networks, and show how we can further reduce the worst-case violations for all neural networks.;arxiv
609;Distances between data points are widely used in machine learning applications.;arxiv
610;Yet, when corrupted by noise, these distances -- and thus the models based upon them -- may lose their usefulness in high dimensions.;arxiv
611;Indeed, the small marginal effects of the noise may then accumulate quickly, shifting empirical closest and furthest neighbors away from the ground truth.;arxiv
612;In this paper, we exactly characterize such effects in noisy high-dimensional data using an asymptotic probabilistic expression.;arxiv
613;Previously, it has been argued that neighborhood queries become meaningless and unstable when distance concentration occurs, which means that there is a poor relative discrimination between the furthest and closest neighbors in the data.;arxiv
614;However, we conclude that this is not necessarily the case when we decompose the data in a ground truth -- which we aim to recover -- and noise component.;arxiv
615;More specifically, we derive that under particular conditions, empirical neighborhood relations affected by noise are still likely to be truthful even when distance concentration occurs.;arxiv
616;We also include thorough empirical verification of our results, as well as interesting experiments in which our derived 'phase shift' where neighbors become random or not turns out to be identical to the phase shift where common dimensionality reduction methods perform poorly or well for recovering low-dimensional reconstructions of high-dimensional data with dense noise.;arxiv
617;Monocular depth estimation plays a crucial role in 3D recognition and understanding.;arxiv
618;One key limitation of existing approaches lies in their lack of structural information exploitation, which leads to inaccurate spatial layout, discontinuous surface, and ambiguous boundaries.;arxiv
619;In this paper, we tackle this problem in three aspects.;arxiv
620;First, to exploit the spatial relationship of visual features, we propose a structure-aware neural network with spatial attention blocks.;arxiv
621;These blocks guide the network attention to global structures or local details across different feature layers.;arxiv
622;Second, we introduce a global focal relative loss for uniform point pairs to enhance spatial constraint in the prediction, and explicitly increase the penalty on errors in depth-wise discontinuous regions, which helps preserve the sharpness of estimation results.;arxiv
623;Finally, based on analysis of failure cases for prior methods, we collect a new Hard Case (HC) Depth dataset of challenging scenes, such as special lighting conditions, dynamic objects, and tilted camera angles.;arxiv
624;The new dataset is leveraged by an informed learning curriculum that mixes training examples incrementally to handle diverse data distributions.;arxiv
625;Experimental results show that our method outperforms state-of-the-art approaches by a large margin in terms of both prediction accuracy on NYUDv2 dataset and generalization performance on unseen datasets.;arxiv
626;A law practitioner has to go through a lot of long legal case proceedings.;arxiv
627;To understand the motivation behind the actions of different parties/individuals in a legal case, it is essential that the parts of the document that express an intent corresponding to the case be clearly understood.;arxiv
628;In this paper, we introduce a dataset of 93 legal documents, belonging to the case categories of either Murder, Land Dispute, Robbery, or Corruption, where phrases expressing intent same as the category of the document are annotated.;arxiv
629;Also, we annotate fine-grained intents for each such phrase to enable a deeper understanding of the case for a reader.;arxiv
630;Finally, we analyze the performance of several transformer-based models in automating the process of extracting intent phrases (both at a coarse and a fine-grained level), and classifying a document into one of the possible 4 categories, and observe that, our dataset is challenging, especially in the case of fine-grained intent classification.;arxiv
631;We study the underdamped Langevin diffusion when the log of the target distribution is smooth and strongly concave.;arxiv
632;We present a MCMC algorithm based on its discretization and show that it achieves $\varepsilon$ error (in 2-Wasserstein distance) in $\mathcal{O}(\sqrt{d}/\varepsilon)$ steps.;arxiv
633;This is a significant improvement over the best known rate for overdamped Langevin MCMC, which is $\mathcal{O}(d/\varepsilon^2)$ steps under the same smoothness/concavity assumptions.;arxiv
634;The underdamped Langevin MCMC scheme can be viewed as a version of Hamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped Langevin MCMC methods in a number of application areas.;arxiv
635;We provide quantitative rates that support this empirical wisdom.;arxiv
636;A Markov chain update scheme using a machine-learned flow-based generative model is proposed for Monte Carlo sampling in lattice field theories.;arxiv
637;The generative model may be optimized (trained) to produce samples from a distribution approximating the desired Boltzmann distribution determined by the lattice action of the theory being studied.;arxiv
638;Training the model systematically improves autocorrelation times in the Markov chain, even in regions of parameter space where standard Markov chain Monte Carlo algorithms exhibit critical slowing down in producing decorrelated updates.;arxiv
639;Moreover, the model may be trained without existing samples from the desired distribution.;arxiv
640;The algorithm is compared with HMC and local Metropolis sampling for $\phi^4$ theory in two dimensions.;arxiv
641;In multi-label classification tasks, each problem instance is associated with multiple classes simultaneously.;arxiv
642;In such settings, the correlation between labels contains valuable information that can be used to obtain more accurate classification models.;arxiv
643;The correlation between labels can be exploited at different levels such as capturing the pair-wise correlation or exploiting the higher-order correlations.;arxiv
644;Even though the high-order approach is more capable of modeling the correlation, it is computationally more demanding and has scalability issues.;arxiv
645;This paper aims at exploiting the high-order label correlation within subsets of labels using a supervised learning classifier system (UCS).;arxiv
646;For this purpose, the label powerset (LP) strategy is employed and a prediction aggregation within the set of the relevant labels to an unseen instance is utilized to increase the prediction capability of the LP method in the presence of unseen labelsets.;arxiv
647;Exact match ratio and Hamming loss measures are considered to evaluate the rule performance and the expected fitness value of a classifier is investigated for both metrics.;arxiv
648;Also, a computational complexity analysis is provided for the proposed algorithm.;arxiv
649;The experimental results of the proposed method are compared with other well-known LP-based methods on multiple benchmark datasets and confirm the competitive performance of this method.;arxiv
650;We study a variant of the multi-armed bandit problem where side information in the form of bounds on the mean of each arm is provided.;arxiv
651;We develop the novel non-optimistic Global Under-Explore (GLUE) algorithm which uses the provided mean bounds (across all the arms) to infer pseudo-variances for each arm, which in turn decide the rate of exploration for the arms.;arxiv
652;We analyze the regret of GLUE and prove regret upper bounds that are never worse than that of the standard UCB algorithm.;arxiv
653;Furthermore, we show that GLUE improves upon regret guarantees that exists in literature for structured bandit problems (both theoretically and empirically).;arxiv
654;Finally, we study the practical setting of learning adaptive interventions using prior data that has been confounded by unrecorded variables that affect rewards.;arxiv
655;We show that mean bounds can be inferred naturally from such logs and can thus be used to improve the learning process.;arxiv
656;We validate our findings through semi-synthetic experiments on data derived from real data sets.;arxiv
657;In an ever-increasing interest for Machine Learning (ML) and a favorable data development context, we here propose an original methodology for data-based prediction of two-dimensional physical fields.;arxiv
658;Polynomial Chaos Expansion (PCE), widely used in the Uncertainty Quantification community (UQ), has long been employed as a robust representation for probabilistic input-to-output mapping.;arxiv
659;It has been recently tested in a pure ML context, and shown to be as powerful as classical ML techniques for point-wise prediction.;arxiv
660;Some advantages are inherent to the method, such as its explicitness and adaptability to small training sets, in addition to the associated probabilistic framework.;arxiv
661;Simultaneously, Dimensionality Reduction (DR) techniques are increasingly used for pattern recognition and data compression and have gained interest due to improved data quality.;arxiv
662;In this study, the interest of Proper Orthogonal Decomposition (POD) for the construction of a statistical predictive model is demonstrated.;arxiv
663;Both POD and PCE have amply proved their worth in their respective frameworks.;arxiv
664;The goal of the present paper was to combine them for a field-measurement-based forecasting.;arxiv
665;The described steps are also useful to analyze the data.;arxiv
666;Some challenging issues encountered when using multidimensional field measurements are addressed, for example when dealing with few data.;arxiv
667;The POD-PCE coupling methodology is presented, with particular focus on input data characteristics and training-set choice.;arxiv
668;A simple methodology for evaluating the importance of each physical parameter is proposed for the PCE model and extended to the POD-PCE coupling.;arxiv
669;This is necessary for understanding how those tasks, operations, and activities can be improvised and made better suited for the users so that they reduce the mental workload on the individual and the operators can use them with ease and less difficulty.;arxiv
670;However, a particular task can be gauged by a user as simple while for others it may be difficult.;arxiv
671;Understanding the complexity of a particular task can only be done on user level and we propose to do this by understanding the mental workload (MWL) generated on an operator while performing a task which requires processing a lot of information to get the task done.;arxiv
672;In this work, we have proposed an experimental setup which replicates modern day workload on doing regular day job tasks.;arxiv
673;We propose an approach to automatically evaluate the task complexity perceived by an individual by using electroencephalogram (EEG) data of a user during operation.;arxiv
674;Few crucial steps that are addressed in this work include extraction and optimization of different features and selection of relevant features for dimensionality reduction and using supervised machine learning techniques.;arxiv
675;In addition to this, performance results of the classifiers are compared using all features and also using only the selected features.;arxiv
676;From the results, it can be inferred that machine learning algorithms perform better as compared to traditional approaches for mental workload estimation.;arxiv
677;Low-bit width neural networks have been extensively explored for deployment on edge devices to reduce computational resources.;arxiv
678;Existing approaches have focused on gradient-based optimization in a two-stage train-and-compress setting or as a combined optimization where gradients are quantized during training.;arxiv
679;Such schemes require high-performance hardware during the training phase and usually store an equivalent number of full-precision weights apart from the quantized weights.;arxiv
680;In this paper, we explore methods of direct combinatorial optimization in the problem of risk minimization with binary weights, which can be made equivalent to a non-monotone submodular maximization under certain conditions.;arxiv
681;We employ an approximation algorithm for the cases with single and multilayer neural networks.;arxiv
682;For linear models, it has $\mathcal{O}(nd)$ time complexity where $n$ is the sample size and $d$ is the data dimension.;arxiv
683;We show that a combination of greedy coordinate descent and this novel approach can attain competitive accuracy on binary classification tasks.;arxiv
684;Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging).;arxiv
685;Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity.;arxiv
686;The very number of parameters makes models hard to understand.;arxiv
687;This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes.;arxiv
688;Each explainer is a technique for exploration of a black box model.;arxiv
689;Presented approaches are model-agnostic, what means that they extract useful information from any predictive method despite its internal structure.;arxiv
690;Each explainer is linked with a specific aspect of a model.;arxiv
691;Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable.;arxiv
692;Every explainer presented in this paper works for a single model or for a collection of models.;arxiv
693;In the latter case, models can be compared against each other.;arxiv
694;Such comparison helps to find strengths and weaknesses of different approaches and gives additional possibilities for model validation.;arxiv
695;Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.;arxiv
696;The current implementation supports the most popular frameworks for classification and regression.;arxiv
697;Voice conversion (VC) is a task that transforms the source speaker's timbre, accent, and tones in audio into another one's while preserving the linguistic content.;arxiv
698;It is still a challenging work, especially in a one-shot setting.;arxiv
699;Auto-encoder-based VC methods disentangle the speaker and the content in input speech without given the speaker's identity, so these methods can further generalize to unseen speakers.;arxiv
700;The disentangle capability is achieved by vector quantization (VQ), adversarial training, or instance normalization (IN).;arxiv
701;However, the imperfect disentanglement may harm the quality of output speech.;arxiv
702;In this work, to further improve audio quality, we use the U-Net architecture within an auto-encoder-based VC system.;arxiv
703;We find that to leverage the U-Net architecture, a strong information bottleneck is necessary.;arxiv
704;The VQ-based method, which quantizes the latent vectors, can serve the purpose.;arxiv
705;The objective and the subjective evaluations show that the proposed method performs well in both audio naturalness and speaker similarity.;arxiv
706;Bayesian Neural Networks (BNN) have recently emerged in the Deep Learning world for dealing with uncertainty estimation in classification tasks, and are used in many application domains such as astrophysics, autonomous driving...;arxiv
707;BNN assume a prior over the weights of a neural network instead of point estimates, enabling in this way the estimation of both aleatoric and epistemic uncertainty of the model prediction.;arxiv
708;Moreover, a particular type of BNN, namely MC Dropout, assumes a Bernoulli distribution on the weights by using Dropout.;arxiv
709;Several attempts to optimize the dropout rate exist, e.g. using a variational approach.;arxiv
710;"In this paper, we present a new method called ""Dropout Regulation"" (DR), which consists of automatically adjusting the dropout rate during training using a controller as used in automation.";arxiv
711;DR allows for a precise estimation of the uncertainty which is comparable to the state-of-the-art while remaining simple to implement.;arxiv
712;Machine learning techniques have been increasingly used in astronomical applications and have proven to successfully classify objects in image data with high accuracy.;arxiv
713;The current work uses archival data from the Faint Images of the Radio Sky at Twenty Centimeters (FIRST) to classify radio galaxies into four classes: Fanaroff-Riley Class I (FRI), Fanaroff-Riley Class II (FRII), Bent-Tailed (BENT), and Compact (COMPT).;arxiv
714;The model presented in this work is based on Convolutional Neural Networks (CNNs).;arxiv
715;The proposed architecture comprises three parallel blocks of convolutional layers combined and processed for final classification by two feed-forward layers.;arxiv
716;Our model classified selected classes of radio galaxy sources on an independent testing subset with an average of 96\% for precision, recall, and F1 score.;arxiv
717;The best selected augmentation techniques were rotations, horizontal or vertical flips, and increase of brightness.;arxiv
718;Shifts, zoom and decrease of brightness worsened the performance of the model.;arxiv
719;The current results show that model developed in this work is able to identify different morphological classes of radio galaxies with a high efficiency and performance;arxiv
720;Open book question answering is a type of natural language based QA (NLQA) where questions are expected to be answered with respect to a given set of open book facts, and common knowledge about a topic.;arxiv
721;Recently a challenge involving such QA, OpenBookQA, has been proposed.;arxiv
722;Unlike most other NLQA tasks that focus on linguistic understanding, OpenBookQA requires deeper reasoning involving linguistic understanding as well as reasoning with common knowledge.;arxiv
723;In this paper we address QA with respect to the OpenBookQA dataset and combine state of the art language models with abductive information retrieval (IR), information gain based re-ranking, passage selection and weighted scoring to achieve 72.0% accuracy, an 11.6% improvement over the current state of the art.;arxiv
724;Skew-Gaussian processes (SkewGPs) extend the multivariate Unified Skew-Normal distributions over finite dimensional vectors to distribution over functions.;arxiv
725;SkewGPs are more general and flexible than Gaussian processes, as SkewGPs may also represent asymmetric distributions.;arxiv
726;In a recent contribution we showed that SkewGP and probit likelihood are conjugate, which allows us to compute the exact posterior for non-parametric binary classification and preference learning.;arxiv
727;In this paper, we generalize previous results and we prove that SkewGP is conjugate with both the normal and affine probit likelihood, and more in general, with their product.;arxiv
728;We show empirically that the proposed framework based on SkewGP provides better performance than Gaussian processes in active learning and Bayesian (constrained) optimization.;arxiv
729;These two tasks are fundamental for design of experiments and in Data Science.;arxiv
730;We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction.;arxiv
731;Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task.;arxiv
732;While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers.;arxiv
733;The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique.;arxiv
734;Experimental results show an average of 82% F-measure on four different product review domains.;arxiv
735;We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis.;arxiv
736;We establish a connection to the previously investigated matrix-space models for compositionality, and show they are special cases of the multiplicative recurrent net.;arxiv
737;Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus.;arxiv
738;Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.;arxiv
739;Data privacy is a central problem for embodied agents that can perceive the environment, communicate with humans, and act in the real world.;arxiv
740;In this work, we introduce privacy-preserving embodied agent learning for the task of Vision-and-Language Navigation (VLN), where an embodied agent navigates house environments by following natural language instructions.;arxiv
741;We view each house environment as a local client, which shares nothing other than local updates with the cloud server and other clients, and propose a novel federated vision-and-language navigation (FedVLN) framework to protect data privacy during both training and pre-exploration.;arxiv
742;Particularly, we propose a decentralized training strategy to limit the data of each client to its local model training and a federated pre-exploration method to do partial model aggregation to improve model generalizability to unseen environments.;arxiv
743;Extensive results on R2R and RxR datasets show that under our FedVLN framework, decentralized VLN models achieve comparable results with centralized training while protecting seen environment privacy, and federated pre-exploration significantly outperforms centralized pre-exploration while preserving unseen environment privacy.;arxiv
744;Gaussian variational approximation is a popular methodology to approximate posterior distributions in Bayesian inference especially in high dimensional and large data settings.;arxiv
745;To control the computational cost while being able to capture the correlations among the variables, the low rank plus diagonal structure was introduced in the previous literature for the Gaussian covariance matrix.;arxiv
746;For a specific Bayesian learning task, the uniqueness of the solution is usually ensured by imposing stringent constraints on the parameterized covariance matrix, which could break down during the optimization process.;arxiv
747;In this paper, we consider two special covariance structures by applying the Stiefel manifold and Grassmann manifold constraints, to address the optimization difficulty in such factorization architectures.;arxiv
748;To speed up the updating process with minimum hyperparameter-tuning efforts, we design two new schemes of Riemannian stochastic gradient descent methods and compare them with other existing methods of optimizing on manifolds.;arxiv
749;In addition to fixing the identification issue, results from both simulation and empirical experiments prove the ability of the proposed methods of obtaining competitive accuracy and comparable converge speed in both high-dimensional and large-scale learning tasks.;arxiv
750;Abc-boost is a new line of boosting algorithms for multi-class classification, by utilizing the commonly used sum-to-zero constraint.;arxiv
751;To implement abc-boost, a base class must be identified at each boosting step.;arxiv
752;Prior studies used a very expensive procedure based on exhaustive search for determining the base class at each boosting step.;arxiv
753;Good testing performances of abc-boost (implemented as abc-mart and abc-logitboost) on a variety of datasets were reported.;arxiv
754;For large datasets, however, the exhaustive search strategy adopted in prior abc-boost algorithms can be too prohibitive.;arxiv
755;To overcome this serious limitation, this paper suggests a heuristic by introducing Gaps when computing the base class during training.;arxiv
756;We test this idea on large datasets (Covertype and Poker) as well as datasets of moderate sizes.;arxiv
757;On the large datasets, even with G=100 (or larger), there is essentially no loss of test accuracy.;arxiv
758;On the moderate datasets, no obvious loss of test accuracy is observed when G<= 20~50.;arxiv
759;Therefore, aided by this heuristic, it is promising that abc-boost will be a practical tool for accurate multi-class classification.;arxiv
760;Nowadays, in many scientific and industrial fields there is an increasing need for estimating treatment effects and answering causal questions.;arxiv
761;The key for addressing these problems is the wealth of observational data and the processes for leveraging this data.;arxiv
762;In this work, we propose a new model for predicting the potential outcomes and the propensity score, which is based on a neural network architecture.;arxiv
763;The proposed model exploits the covariates as well as the outcomes of neighboring instances in training data.;arxiv
764;Numerical experiments illustrate that the proposed model reports better treatment effect estimation performance compared to state-of-the-art models.;arxiv
765;Pauli spin blockade (PSB) can be employed as a great resource for spin qubit initialisation and readout even at elevated temperatures but it can be difficult to identify.;arxiv
766;We present a machine learning algorithm capable of automatically identifying PSB using charge transport measurements.;arxiv
767;The scarcity of PSB data is circumvented by training the algorithm with simulated data and by using cross-device validation.;arxiv
768;We demonstrate our approach on a silicon field-effect transistor device and report an accuracy of 96% on different test devices, giving evidence that the approach is robust to device variability.;arxiv
769;The approach is expected to be employable across all types of quantum dot devices.;arxiv
770;In this paper, we focus on the fairness issues regarding unsupervised outlier detection.;arxiv
771;Traditional algorithms, without a specific design for algorithmic fairness, could implicitly encode and propagate statistical bias in data and raise societal concerns.;arxiv
772;To correct such unfairness and deliver a fair set of potential outlier candidates, we propose Deep Clustering based Fair Outlier Detection (DCFOD) that learns a good representation for utility maximization while enforcing the learnable representation to be subgroup-invariant on the sensitive attribute.;arxiv
773;Considering the coupled and reciprocal nature between clustering and outlier detection, we leverage deep clustering to discover the intrinsic cluster structure and out-of-structure instances.;arxiv
774;Meanwhile, an adversarial training erases the sensitive pattern for instances for fairness adaptation.;arxiv
775;Technically, we propose an instance-level weighted representation learning strategy to enhance the joint deep clustering and outlier detection, where the dynamic weight module re-emphasizes contributions of likely-inliers while mitigating the negative impact from outliers.;arxiv
776;Demonstrated by experiments on eight datasets comparing to 17 outlier detection algorithms, our DCFOD method consistently achieves superior performance on both the outlier detection validity and two types of fairness notions in outlier detection.;arxiv
777;This letter illustrates our preliminary works in deep nerual network (DNN) for wireless communication scenario identification in wireless multi-path fading channels.;arxiv
778;In this letter, six kinds of channel scenarios referring to COST 207 channel model have been performed.;arxiv
779;Noisy labels are inevitable yet problematic in machine learning society.;arxiv
780;It ruins the generalization of a classifier by making the classifier over-fitted to noisy labels.;arxiv
781;Existing methods on noisy label have focused on modifying the classifier during the training procedure.;arxiv
782;First, these methods are not applicable to a pre-trained classifier without further access to training.;arxiv
783;Second, it is not easy to train a classifier and regularize all negative effects from noisy labels, simultaneously.;arxiv
784;We suggest a new branch of method, Noisy Prediction Calibration (NPC) in learning with noisy labels.;arxiv
785;Through the introduction and estimation of a new type of transition matrix via generative model, NPC corrects the noisy prediction from the pre-trained classifier to the true label as a post-processing scheme.;arxiv
786;We prove that NPC theoretically aligns with the transition matrix based methods.;arxiv
787;Yet, NPC empirically provides more accurate pathway to estimate true label, even without involvement in classifier learning.;arxiv
788;Also, NPC is applicable to any classifier trained with noisy label methods, if training instances and its predictions are available.;arxiv
789;Our method, NPC, boosts the classification performances of all baseline models on both synthetic and real-world datasets.;arxiv
790;In this article, we study shape fitting problems, $\epsilon$-coresets, and total sensitivity.;arxiv
791;We focus on the $(j,k)$-projective clustering problems, including $k$-median/$k$-means, $k$-line clustering, $j$-subspace approximation, and the integer $(j,k)$-projective clustering problem.;arxiv
792;We derive upper bounds of total sensitivities for these problems, and obtain $\epsilon$-coresets using these upper bounds.;arxiv
793;Using a dimension-reduction type argument, we are able to greatly simplify earlier results on total sensitivity for the $k$-median/$k$-means clustering problems, and obtain positively-weighted $\epsilon$-coresets for several variants of the $(j,k)$-projective clustering problem.;arxiv
794;We also extend an earlier result on $\epsilon$-coresets for the integer $(j,k)$-projective clustering problem in fixed dimension to the case of high dimension.;arxiv
795;This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs.;arxiv
796;However, BERT-based APIs have exhibited a series of security and privacy vulnerabilities.;arxiv
797;For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model.;arxiv
798;However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied.;arxiv
799;On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown.;arxiv
800;In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries.;arxiv
801;We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs.;arxiv
802;Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERT-based APIs.;arxiv
803;Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.;arxiv
804;Imitation learning enables high-fidelity, vision-based learning of policies within rich, photorealistic environments.;arxiv
805;However, such techniques often rely on traditional discrete-time neural models and face difficulties in generalizing to domain shifts by failing to account for the causal relationships between the agent and the environment.;arxiv
806;In this paper, we propose a theoretical and experimental framework for learning causal representations using continuous-time neural networks, specifically over their discrete-time counterparts.;arxiv
807;We evaluate our method in the context of visual-control learning of drones over a series of complex tasks, ranging from short- and long-term navigation, to chasing static and dynamic objects through photorealistic environments.;arxiv
808;Our results demonstrate that causal continuous-time deep models can perform robust navigation tasks, where advanced recurrent models fail.;arxiv
809;These models learn complex causal control representations directly from raw visual inputs and scale to solve a variety of tasks using imitation learning.;arxiv
810;One of the major objectives of Artificial Intelligence is to design learning algorithms that are executed on a general purposes computational machines such as human brain.;arxiv
811;Neural Turing Machine (NTM) is a step towards realizing such a computational machine.;arxiv
812;The attempt is made here to run a systematic review on Neural Turing Machine.;arxiv
813;First, the mind-map and taxonomy of machine learning, neural networks, and Turing machine are introduced.;arxiv
814;Finally, the paper discusses on issues and ends up with several future works.;arxiv
815;The ability to transfer adversarial attacks from one model (the surrogate) to another model (the victim) has been an issue of concern within the machine learning (ML) community.;arxiv
816;The ability to successfully evade unseen models represents an uncomfortable level of ease toward implementing attacks.;arxiv
817;In this work we note that as studied, current transfer attack research has an unrealistic advantage for the attacker: the attacker has the exact same training data as the victim.;arxiv
818;We present the first study of transferring adversarial attacks focusing on the data available to attacker and victim under imperfect settings without querying the victim, where there is some variable level of overlap in the exact data used or in the classes learned by each model.;arxiv
819;This threat model is relevant to applications in medicine, malware, and others.;arxiv
820;Under this new threat model attack success rate is not correlated with data or class overlap in the way one would expect, and varies with dataset.;arxiv
821;This makes it difficult for attacker and defender to reason about each other and contributes to the broader study of model robustness and security.;arxiv
822;We remedy this by developing a masked version of Projected Gradient Descent that simulates class disparity, which enables the attacker to reliably estimate a lower-bound on their attack's success.;arxiv
823;With the advent of powerful, low-cost IoT systems, processing data closer to where the data originates, known as edge computing, has become an increasingly viable option.;arxiv
824;In addition to lowering the cost of networking infrastructures, edge computing reduces edge-cloud delay, which is essential for mission-critical applications.;arxiv
825;In this paper, we show the feasibility and study the performance of image classification using IoT devices.;arxiv
826;Specifically, we explore the relationships between various factors of image classification algorithms that may affect energy consumption such as dataset size, image resolution, algorithm type, algorithm phase, and device hardware.;arxiv
827;Our experiments show a strong, positive linear relationship between three predictor variables, namely model complexity, image resolution, and dataset size, with respect to energy consumption.;arxiv
828;In addition, in order to provide a means of predicting the energy consumption of an edge device performing image classification, we investigate the usage of three machine learning algorithms using the data generated from our experiments.;arxiv
829;The performance as well as the trade offs for using linear regression, Gaussian process, and random forests are discussed and validated.;arxiv
830;Our results indicate that the random forest model outperforms the two former algorithms, with an R-squared value of 0.95 and 0.79 for two different validation datasets.;arxiv
831;Knitting is an effective technique for producing complex three-dimensional surfaces owing to the inherent flexibility of interlooped yarns and recent advances in manufacturing providing better control of local stitch patterns.;arxiv
832;Fully yarn-level modelling of large-scale knitted membranes is not feasible.;arxiv
833;Therefore, we use a two-scale homogenisation approach and model the membrane as a Kirchhoff-Love shell on the macroscale and as Euler-Bernoulli rods on the microscale.;arxiv
834;The governing equations for both the shell and the rod are discretised with cubic B-spline basis functions.;arxiv
835;For homogenisation we consider only the in-plane response of the membrane.;arxiv
836;The solution of the nonlinear microscale problem requires a significant amount of time due to the large deformations and the enforcement of contact constraints, rendering conventional online computational homogenisation approaches infeasible.;arxiv
837;To sidestep this problem, we use a pre-trained statistical Gaussian Process Regression (GPR) model to map the macroscale deformations to macroscale stresses.;arxiv
838;During the offline learning phase, the GPR model is trained by solving the microscale problem for a sufficiently rich set of deformation states obtained by either uniform or Sobol sampling.;arxiv
839;The trained GPR model encodes the nonlinearities and anisotropies present in the microscale and serves as a material model for the membrane response of the macroscale shell.;arxiv
840;The bending response can be chosen in dependence of the mesh size to penalise the fine out-of-plane wrinkling of the membrane.;arxiv
841;After verifying and validating the different components of the proposed approach, we introduce several examples involving membranes subjected to tension and shear to demonstrate its versatility and good performance.;arxiv
842;There are partially separable data types that make classification tasks very hard.;arxiv
843;In other words, only parts of the data are informative meaning that looking at the rest of the data would not give any distinguishable hint for classification.;arxiv
844;In this situation, the typical assumption of having the whole labeled data as an informative unit set for classification does not work.;arxiv
845;Consequently, typical classification methods with the mentioned assumption fail in such a situation.;arxiv
846;In this study, we propose a framework for the classification of partially separable data types that are not classifiable using typical methods.;arxiv
847;An algorithm based on the framework is proposed that tries to detect separable subgroups of the data using an iterative clustering approach.;arxiv
848;Then the detected subgroups are used in the classification process.;arxiv
849;The proposed approach was tested on a real dataset for autism screening and showed its capability by distinguishing children with autism from normal ones, while the other methods failed to do so.;arxiv
850;In many games humans on average do not achieve maximal payoff and the behaviour of individual players remains inhomogeneous even after playing many rounds.;arxiv
851;For instance, in repeated prisoner dilemma games humans do not always optimize their mean reward and frequently exhibit broad distributions of cooperativity.;arxiv
852;The reasons for these failures of maximization are not known.;arxiv
853;Here we show that the dynamics resulting from the tendency to shift choice probabilities towards previously rewarding choices in closed loop interaction with the strategy of the opponent can not only explain systematic deviations from 'rationality', but also reproduce the diversity of choice behaviours.;arxiv
854;As a representative example we investigate the dynamics of choice probabilities in prisoner dilemma games with opponents using strategies with different degrees of extortion and generosity.;arxiv
855;We find that already a simple model for human learning can account for a surprisingly wide range of human decision behaviours.;arxiv
856;It reproduces suppression of cooperation against extortionists and increasing cooperation when playing with generous opponents, explains the broad distributions of individual choices in ensembles of players, and predicts the evolution of individual subjects' cooperation rates over the course of the games.;arxiv
857;We conclude that important aspects of human decision behaviours are rooted in elementary learning mechanisms realised in the brain.;arxiv
858;Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment.;arxiv
859;These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters.;arxiv
860;In general, there is a trade-off between generality and performance when algorithms use such biases.;arxiv
861;Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms.;arxiv
862;In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents.;arxiv
863;We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature.;arxiv
864;In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better.;arxiv
865;We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system.;arxiv
866;As hypothesized, the system with adaptive components performed better on many of the new tasks.;arxiv
867;In this paper, we tackle the problem of handling narrowband and wideband speech by building a single acoustic model (AM), also called mixed bandwidth AM.;arxiv
868;In the proposed approach, an auxiliary input feature is used to provide the bandwidth information to the model, and bandwidth embeddings are jointly learned as part of acoustic model training.;arxiv
869;Experimental evaluations show that using bandwidth embeddings helps the model to handle the variability of the narrow and wideband speech, and makes it possible to train a mixed-bandwidth AM.;arxiv
870;Furthermore, we propose to use parallel convolutional layers to handle the mismatch between the narrow and wideband speech better, where separate convolution layers are used for each type of input speech signal.;arxiv
871;Our best system achieves 13% relative improvement on narrowband speech, while not degrading on wideband speech.;arxiv
872;We consider an online learning to rank setting in which, at each round, an oblivious adversary generates a list of $m$ documents, pertaining to a query, and the learner produces scores to rank the documents.;arxiv
873;The adversary then generates a relevance vector and the learner updates its ranker according to the feedback received.;arxiv
874;We develop efficient algorithms for well known losses in the pointwise, pairwise and listwise families.;arxiv
875;We also prove that no online algorithm can have sublinear regret, with top-1 feedback, for any loss that is calibrated with respect to NDCG.;arxiv
876;We apply our algorithms on benchmark datasets demonstrating efficient online learning of a ranking function from highly restricted feedback.;arxiv
877;In order to contrast the explosion in size of state-of-the-art machine learning models that can be attributed to the empirical advantages of over-parametrization, and due to the necessity of deploying fast, sustainable, and private on-device models on resource-constrained devices, the community has focused on techniques such as pruning, quantization, and distillation as central strategies for model compression.;arxiv
878;Towards the goal of facilitating the adoption of a common interface for neural network pruning in PyTorch, this contribution describes the recent addition of the PyTorch torch.nn.utils.prune module, which provides shared, open source pruning functionalities to lower the technical implementation barrier to reducing model size and capacity before, during, and/or after training.;arxiv
879;We present the module's user interface, elucidate implementation details, illustrate example usage, and suggest ways to extend the contributed functionalities to new pruning methods.;arxiv
880;Analytical performance models are very effective in ensuring the quality of service and cost of service deployment remain desirable under different conditions and workloads.;arxiv
881;While various analytical performance models have been proposed for previous paradigms in cloud computing, serverless computing lacks such models that can provide developers with performance guarantees.;arxiv
882;Besides, most serverless computing platforms still require developers' input to specify the configuration for their deployment that could affect both the performance and cost of their deployment, without providing them with any direct and immediate feedback.;arxiv
883;In previous studies, we built such performance models for steady-state and transient analysis of scale-per-request serverless computing platforms (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) that could give developers immediate feedback about the quality of service and cost of their deployments.;arxiv
884;In this work, we aim to develop analytical performance models for the latest trend in serverless computing platforms that use concurrency value and the rate of requests per second for autoscaling decisions.;arxiv
885;Examples of such serverless computing platforms are Knative and Google Cloud Run (a managed Knative service by Google).;arxiv
886;The proposed performance model can help developers and providers predict the performance and cost of deployments with different configurations which could help them tune the configuration toward the best outcome.;arxiv
887;We validate the applicability and accuracy of the proposed performance model by extensive real-world experimentation on Knative and show that our performance model is able to accurately predict the steady-state characteristics of a given workload with minimal amount of data collection.;arxiv
888;We exploit liver cancer prediction model using machine learning algorithms based on epidemiological data of over 55 thousand peoples from 2014 to the present.;arxiv
889;The best performance is an AUC of 0.71.;arxiv
890;We analyzed model parameters to investigate critical risk factors that contribute the most to prediction.;arxiv
891;Federated learning is a distributed framework according to which a model is trained over a set of devices, while keeping data localized.;arxiv
892;This framework faces several systems-oriented challenges which include (i) communication bottleneck since a large number of devices upload their local updates to a parameter server, and (ii) scalability as the federated network consists of millions of devices.;arxiv
893;Due to these systems challenges as well as issues related to statistical heterogeneity of data and privacy concerns, designing a provably efficient federated learning method is of significant importance yet it remains challenging.;arxiv
894;In this paper, we present FedPAQ, a communication-efficient Federated Learning method with Periodic Averaging and Quantization.;arxiv
895;These features address the communications and scalability challenges in federated learning.;arxiv
896;We also show that FedPAQ achieves near-optimal theoretical guarantees for strongly convex and non-convex loss functions and empirically demonstrate the communication-computation tradeoff provided by our method.;arxiv
897;Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks.;arxiv
898;Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers.;arxiv
899;This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks.;arxiv
900;By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer.;arxiv
901;Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.;arxiv
902;We seek to automate the design of molecules based on specific chemical properties.;arxiv
903;This approach uses substantially fewer model steps per atom than earlier approaches, thus enabling generation of larger molecules, and beats previous state-of-the art baselines by a significant margin.;arxiv
904;Applying reinforcement learning to a combination of a custom context-free grammar with additional masking to enforce non-local constraints is applicable to any optimization of a graph structure under a mixture of local and nonlocal constraints.;arxiv
905;Recurrent Neural Networks (RNNs) have been widely used in processing natural language tasks and achieve huge success.;arxiv
906;Traditional RNNs usually treat each token in a sentence uniformly and equally.;arxiv
907;However, this may miss the rich semantic structure information of a sentence, which is useful for understanding natural languages.;arxiv
908;Since semantic structures such as word dependence patterns are not parameterized, it is a challenge to capture and leverage structure information.;arxiv
909;In this paper, we propose an improved variant of RNN, Multi-Channel RNN (MC-RNN), to dynamically capture and leverage local semantic structure information.;arxiv
910;Concretely, MC-RNN contains multiple channels, each of which represents a local dependence pattern at a time.;arxiv
911;An attention mechanism is introduced to combine these patterns at each step, according to the semantic information.;arxiv
912;Then we parameterize structure information by adaptively selecting the most appropriate connection structures among channels.;arxiv
913;In this way, diverse local structures and dependence patterns in sentences can be well captured by MC-RNN.;arxiv
914;To verify the effectiveness of MC-RNN, we conduct extensive experiments on typical natural language processing tasks, including neural machine translation, abstractive summarization, and language modeling.;arxiv
915;Experimental results on these tasks all show significant improvements of MC-RNN over current top systems.;arxiv
916;These systems typically consist of Spoken Language understanding component which, in turn, consists of two tasks - Intent Classification (IC) and Slot Labeling (SL).;arxiv
917;Generally, these two tasks are modeled together jointly to achieve best performance.;arxiv
918;However, this joint modeling adds to model obfuscation.;arxiv
919;In this work, we first design framework for a modularization of joint IC-SL task to enhance architecture transparency.;arxiv
920;Then, we explore a number of self-attention, convolutional, and recurrent models, contributing a large-scale analysis of modeling paradigms for IC+SL across two datasets.;arxiv
921;Finally, using this framework, we propose a class of 'label-recurrent' models that otherwise non-recurrent, with a 10-dimensional representation of the label history, and show that our proposed systems are easy to interpret, highly accurate (achieving over 30% error reduction in SL over the state-of-the-art on the Snips dataset), as well as fast, at 2x the inference and 2/3 to 1/2 the training time of comparable recurrent models, thus giving an edge in critical real-world systems.;arxiv
922;Protein secondary structure is crucial to creating an information bridge between the primary and tertiary (3D) structures.;arxiv
923;Precise prediction of eight-state protein secondary structure (PSS) has significantly utilized in the structural and functional analysis of proteins in bioinformatics.;arxiv
924;Deep learning techniques have been recently applied in this research area and raised the eight-state (Q8) protein secondary structure prediction accuracy remarkably.;arxiv
925;Nevertheless, from a theoretical standpoint, there are still lots of rooms for improvement, specifically in the eight-state PSS prediction.;arxiv
926;In this study, we have presented a new deep convolutional neural network (DCNN), namely PS8-Net, to enhance the accuracy of eight-class PSS prediction.;arxiv
927;The input of this architecture is a carefully constructed feature matrix from the proteins sequence features and profile features.;arxiv
928;We introduce a new PS8 module in the network, which is applied with skip connection to extracting the long-term inter-dependencies from higher layers, obtaining local contexts in earlier layers, and achieving global information during secondary structure prediction.;arxiv
929;Our proposed PS8-Net achieves 76.89%, 71.94%, 76.86%, and 75.26% Q8 accuracy respectively on benchmark CullPdb6133, CB513, CASP10, and CASP11 datasets.;arxiv
930;This architecture enables the efficient processing of local and global interdependencies between amino acids to make an accurate prediction of each class.;arxiv
931;To the best of our knowledge, PS8-Net experiment results demonstrate that it outperforms all the state-of-the-art methods on the aforementioned benchmark datasets.;arxiv
932;Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability.;arxiv
933;Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability.;arxiv
934;In this work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models.;arxiv
935;We demonstrate that our models find interesting patterns in the data.;arxiv
936;Lastly, we show that we improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs.;arxiv
937;In convex optimization, the problem of finding near-stationary points has not been adequately studied yet, unlike other optimality measures such as the function value.;arxiv
938;Even in the deterministic case, the optimal method (OGM-G, due to Kim and Fessler (2021)) has just been discovered recently.;arxiv
939;In this work, we conduct a systematic study of algorithmic techniques for finding near-stationary points of convex finite-sums.;arxiv
940;We put an emphasis on the simplicity and practicality of the new schemes, which could facilitate future work.;arxiv
941;In knowledge distillation, since a single, omnipotent teacher network cannot solve all problems, multiple teacher-based knowledge distillations have been studied recently.;arxiv
942;However, sometimes their improvements are not as good as expected because some immature teachers may transfer the false knowledge to the student.;arxiv
943;In this paper, to overcome this limitation and take the efficacy of the multiple networks, we divide the multiple networks into teacher and student groups, respectively.;arxiv
944;That is, the student group is a set of immature networks that require learning the teacher's knowledge, while the teacher group consists of the selected networks that have performed well.;arxiv
945;Furthermore, according to our online role change strategy, the top-ranked networks in the student group are able to promote to the teacher group at every iteration and vice versa.;arxiv
946;After training the teacher group using the error images of the student group to refine the teacher group's knowledge, we transfer the collective knowledge from the teacher group to the student group successfully.;arxiv
947;We verify the superiority of the proposed method on CIFAR-10 and CIFAR-100, which achieves high performance.;arxiv
948;We further show the generality of our method with various backbone architectures such as resent, wrn, vgg, mobilenet, and shufflenet.;arxiv
949;Recent works have shown that modern machine learning techniques can provide an alternative approach to the long-standing joint source-channel coding (JSCC) problem.;arxiv
950;Very promising initial results, superior to popular digital schemes that utilize separate source and channel codes, have been demonstrated for wireless image and video transmission using deep neural networks (DNNs).;arxiv
951;This can prevent the application of these codes in scenarios where the hardware or protocol can only admit certain sets of channel inputs, prescribed by a digital constellation.;arxiv
952;Herein, we propose DeepJSCC-Q, an end-to-end optimized JSCC solution for wireless image transmission using a finite channel input alphabet.;arxiv
953;We show that DeepJSCC-Q can achieve similar performance to prior works that allow any complex valued channel input, especially when high modulation orders are available, and that the performance asymptotically approaches that of unconstrained channel input as the modulation order increases.;arxiv
954;Importantly, DeepJSCC-Q preserves the graceful degradation of image quality in unpredictable channel conditions, a desirable property for deployment in mobile systems with rapidly changing channel conditions.;arxiv
955;Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups.;arxiv
956;For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups.;arxiv
957;These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups.;arxiv
958;Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity.;arxiv
959;We present a fast, scalable, data-driven approach for solving linear relaxations of 0-1 integer linear programs using a graph neural network.;arxiv
960;Our solver is based on the Lagrange decomposition based algorithm FastDOG (Abbas et al. (2022)).;arxiv
961;We make the algorithm differentiable and perform backpropagation through the dual update scheme for end-to-end training of its algorithmic parameters.;arxiv
962;This allows to preserve the algorithm's theoretical properties including feasibility and guaranteed non-decrease in the lower bound.;arxiv
963;Since FastDOG can get stuck in suboptimal fixed points, we provide additional freedom to our graph neural network to predict non-parametric update steps for escaping such points while maintaining dual feasibility.;arxiv
964;For training of the graph neural network we use an unsupervised loss and perform experiments on large-scale real world datasets.;arxiv
965;We train on smaller problems and test on larger ones showing strong generalization performance with a graph neural network comprising only around 10k parameters.;arxiv
966;Our solver achieves significantly faster performance and better dual objectives than its non-learned version.;arxiv
967;In comparison to commercial solvers our learned solver achieves close to optimal objective values of LP relaxations and is faster by up to an order of magnitude on very large problems from structured prediction and on selected combinatorial optimization problems.;arxiv
968;Variance reduction is a simple and effective technique that accelerates convex (or non-convex) stochastic optimization.;arxiv
969;Among existing variance reduction methods, SVRG and SAGA adopt unbiased gradient estimators and are the most popular variance reduction methods in recent years.;arxiv
970;Although various accelerated variants of SVRG (e.g., Katyusha and Acc-Prox-SVRG) have been proposed, the direct acceleration of SAGA still remains unknown.;arxiv
971;In this paper, we propose a directly accelerated variant of SAGA using a novel Sampled Negative Momentum (SSNM), which achieves the best known oracle complexity for strongly convex problems (with known strong convexity parameter).;arxiv
972;Consequently, our work fills the void of directly accelerated SAGA.;arxiv
973;In this paper we present a SOA (Service Oriented Architecture)-based platform, enabling the retrieval and analysis of big datasets stemming from social networking (SN) sites and Internet of Things (IoT) devices, collected by smart city applications and socially-aware data aggregation services.;arxiv
974;A large set of city applications in the areas of Participating Urbanism, Augmented Reality and Sound-Mapping throughout participating cities is being applied, resulting into produced sets of millions of user-generated events and online SN reports fed into the RADICAL platform.;arxiv
975;Moreover, we study the application of data analytics such as sentiment analysis to the combined IoT and SN data saved into an SQL database, further investigating algorithmic and configurations to minimize delays in dataset processing and results retrieval.;arxiv
976;These bounds are tight either when U is independent of T or when U is independent of Y given T (when there is no unobserved confounding).;arxiv
977;We discuss possible calibration strategies for this bound to get interval estimates for treatment effects, and experimentally validate the bound using synthetic and semi-synthetic datasets.;arxiv
978;Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space.;arxiv
979;In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently.;arxiv
980;We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex.;arxiv
981;Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm.;arxiv
982;When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings.;arxiv
983;We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance.;arxiv
984;Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.;arxiv
985;Artificial intelligence (AI) based approaches are beginning to impact several domains of human life, science and technology.;arxiv
986;Polymer informatics is one such domain where AI and machine learning (ML) tools are being used in the efficient development, design and discovery of polymers.;arxiv
987;Surrogate models are trained on available polymer data for instant property prediction, allowing screening of promising polymer candidates with specific target property requirements.;arxiv
988;Questions regarding synthesizability, and potential (retro)synthesis steps to create a target polymer, are being explored using statistical means.;arxiv
989;Data-driven strategies to tackle unique challenges resulting from the extraordinary chemical and physical diversity of polymers at small and large scales are being explored.;arxiv
990;Other major hurdles for polymer informatics are the lack of widespread availability of curated and organized data, and approaches to create machine-readable representations that capture not just the structure of complex polymeric situations but also synthesis and processing conditions.;arxiv
991;Methods to solve inverse problems, wherein polymer recommendations are made using advanced AI algorithms that meet application targets, are being investigated.;arxiv
992;As various parts of the burgeoning polymer informatics ecosystem mature and become integrated, efficiency improvements, accelerated discoveries and increased productivity can result.;arxiv
993;Here, we review emergent components of this polymer informatics ecosystem and discuss imminent challenges and opportunities.;arxiv
994;Prompt-Tuning is a new paradigm for finetuning pre-trained language models in a parameter-efficient way.;arxiv
995;Here, we explore the use of HyperNetworks to generate hyper-prompts: we propose HyperPrompt, a novel architecture for prompt-based task-conditioning of self-attention in Transformers.;arxiv
996;The hyper-prompts are end-to-end learnable via generation by a HyperNetwork.;arxiv
997;HyperPrompt allows the network to learn task-specific feature maps where the hyper-prompts serve as task global memories for the queries to attend to, at the same time enabling flexible information sharing among tasks.;arxiv
998;We show that HyperPrompt is competitive against strong multi-task learning baselines with as few as $0.14\%$ of additional task-conditioning parameters, achieving great parameter and computational efficiency.;arxiv
999;Through extensive empirical experiments, we demonstrate that HyperPrompt can achieve superior performances over strong T5 multi-task learning baselines and parameter-efficient adapter variants including Prompt-Tuning and HyperFormer++ on Natural Language Understanding benchmarks of GLUE and SuperGLUE across many model sizes.;arxiv
1000;We first introduce a structural generative model that is suitable to express and discover such concepts.;arxiv
1001;We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output.;arxiv
1002;Our method also allows easy integration of user's prior knowledge to induce high interpretability of concepts.;arxiv
1003;Using multiple datasets, we demonstrate that our method can discover useful binary concepts for explanation.;arxiv
1004;Ridge or more formally $\ell_2$ regularization shows up in many areas of statistics and machine learning.;arxiv
1005;It is one of those essential devices that any good data scientist needs to master for their craft.;arxiv
1006;In this brief ridge fest I have collected together some of the magic and beauty of ridge that my colleagues and I have encountered over the past 40 years in applied statistics.;arxiv
1007;One of the problems with automated audio captioning (AAC) is the indeterminacy in word selection corresponding to the audio event/scene.;arxiv
1008;Since one acoustic event/scene can be described with several words, it results in a combinatorial explosion of possible captions and difficulty in training.;arxiv
1009;To solve this problem, we propose a Transformer-based audio-captioning model with keyword estimation called TRACKE.;arxiv
1010;TRACKE estimates keywords, which comprise a word set corresponding to audio events/scenes in the input audio, and generates the caption while referring to the estimated keywords to reduce word-selection indeterminacy.;arxiv
1011;Experimental results on a public AAC dataset indicate that TRACKE achieved state-of-the-art performance and successfully estimated both the caption and its keywords.;arxiv
1012;We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce an end-to-end trainable system that integrates reasoning and perception.;arxiv
1013;PSL represents first-order logic in terms of a convex graphical model -- Hinge Loss Markov random fields (HL-MRFs).;arxiv
1014;PSL stands out among probabilistic logic frameworks due to its tractability having been applied to systems of more than 1 billion ground rules.;arxiv
1015;The key to our approach is to represent predicates in first-order logic using deep neural networks and then to approximately back-propagate through the HL-MRF and thus train every aspect of the first-order system being represented.;arxiv
1016;We believe that this approach represents an interesting direction for the integration of deep learning and reasoning techniques with applications to knowledge base learning, multi-task learning, and explainability.;arxiv
1017;We evaluate DeepPSL on a zero shot learning problem in image classification.;arxiv
1018;State of the art results demonstrate the utility and flexibility of our approach.;arxiv
1019;For most existing federated learning algorithms, each round consists of minimizing a loss function at each client to learn an optimal model at the client, followed by aggregating these client models at the server.;arxiv
1020;Point estimation of the model parameters at the clients does not take into account the uncertainty in the models estimated at each client.;arxiv
1021;In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the client models for more accurate and robust predictions.;arxiv
1022;Uncertainty also provides useful information for other important tasks, such as active learning and out-of-distribution (OOD) detection.;arxiv
1023;We present a framework for Bayesian federated learning where each client infers the posterior predictive distribution using its training data and present various ways to aggregate these client-specific predictive distributions at the server.;arxiv
1024;Since communicating and aggregating predictive distributions can be challenging and expensive, our approach is based on distilling each client's predictive distribution into a single deep neural network.;arxiv
1025;This enables us to leverage advances in standard federated learning to Bayesian federated learning as well.;arxiv
1026;Unlike some recent works that have tried to estimate model uncertainty of each client, our work also does not make any restrictive assumptions, such as the form of the client's posterior distribution.;arxiv
1027;We evaluate our approach on classification in federated setting, as well as active learning and OOD detection in federated settings, on which our approach outperforms various existing federated learning baselines.;arxiv
1028;Classifiers trained using conventional empirical risk minimization or maximum likelihood methods are known to suffer dramatic performance degradations when tested over examples adversarially selected based on knowledge of the classifier's decision rule.;arxiv
1029;Due to the prominence of Artificial Neural Networks (ANNs) as classifiers, their sensitivity to adversarial examples, as well as robust training schemes, have been recently the subject of intense investigation.;arxiv
1030;In this paper, for the first time, the sensitivity of spiking neural networks (SNNs), or third-generation neural networks, to adversarial examples is studied.;arxiv
1031;The study considers rate and time encoding, as well as rate and first-to-spike decoding.;arxiv
1032;Furthermore, a robust training mechanism is proposed that is demonstrated to enhance the performance of SNNs under white-box attacks.;arxiv
1033;This paper proposes a novel U-Net variant using stacked dilated convolutions for medical image segmentation (SDU-Net).;arxiv
1034;SDU-Net adopts the architecture of vanilla U-Net with modifications in the encoder and decoder operations (an operation indicates all the processing for feature maps of the same resolution).;arxiv
1035;Unlike vanilla U-Net which incorporates two standard convolutions in each encoder/decoder operation, SDU-Net uses one standard convolution followed by multiple dilated convolutions and concatenates all dilated convolution outputs as input to the next operation.;arxiv
1036;Experiments showed that SDU-Net outperformed vanilla U-Net, attention U-Net (AttU-Net), and recurrent residual U-Net (R2U-Net) in all four tested segmentation tasks while using parameters around 40% of vanilla U-Net's, 17% of AttU-Net's, and 15% of R2U-Net's.;arxiv
1037;Distributed gradient descent (DGD) is an efficient way of implementing gradient descent (GD), especially for large data sets, by dividing the computation tasks into smaller subtasks and assigning to different computing servers (CSs) to be executed in parallel.;arxiv
1038;In standard parallel execution, per-iteration waiting time is limited by the execution time of the straggling servers.;arxiv
1039;Coded DGD techniques have been introduced recently, which can tolerate straggling servers via assigning redundant computation tasks to the CSs.;arxiv
1040;In most of the existing DGD schemes, either with coded computation or coded communication, the non-straggling CSs transmit one message per iteration once they complete all their assigned computation tasks.;arxiv
1041;However, although the straggling servers cannot complete all their assigned tasks, they are often able to complete a certain portion of them.;arxiv
1042;In this paper, we allow multiple transmissions from each CS at each iteration in order to make sure a maximum number of completed computations can be reported to the aggregating server (AS), including the straggling servers.;arxiv
1043;We numerically show that the average completion time per iteration can be reduced significantly by slightly increasing the communication load per server.;arxiv
1044;Recognition tasks, such as object recognition and keypoint estimation, have seen widespread adoption in recent years.;arxiv
1045;Most state-of-the-art methods for these tasks use deep networks that are computationally expensive and have huge memory footprints.;arxiv
1046;This makes it exceedingly difficult to deploy these systems on low power embedded devices.;arxiv
1047;Hence, the importance of decreasing the storage requirements and the amount of computation in such models is paramount.;arxiv
1048;The recently proposed Lottery Ticket Hypothesis (LTH) states that deep neural networks trained on large datasets contain smaller subnetworks that achieve on par performance as the dense networks.;arxiv
1049;In this work, we perform the first empirical study investigating LTH for model pruning in the context of object detection, instance segmentation, and keypoint estimation.;arxiv
1050;Our studies reveal that lottery tickets obtained from ImageNet pretraining do not transfer well to the downstream tasks.;arxiv
1051;We provide guidance on how to find lottery tickets with up to 80% overall sparsity on different sub-tasks without incurring any drop in the performance.;arxiv
1052;Finally, we analyse the behavior of trained tickets with respect to various task attributes such as object size, frequency, and difficulty of detection.;arxiv
1053;We provide quantitative bounds measuring the $L^2$ difference in function space between the trajectory of a finite-width network trained on finitely many samples from the idealized kernel dynamics of infinite width and infinite data.;arxiv
1054;An implication of the bounds is that the network is biased to learn the top eigenfunctions of the Neural Tangent Kernel not just on the training set but over the entire input space.;arxiv
1055;This bias depends on the model architecture and input distribution alone and thus does not depend on the target function which does not need to be in the RKHS of the kernel.;arxiv
1056;The result is valid for deep architectures with fully connected, convolutional, and residual layers.;arxiv
1057;Furthermore the width does not need to grow polynomially with the number of samples in order to obtain high probability bounds up to a stopping time.;arxiv
1058;The proof exploits the low-effective-rank property of the Fisher Information Matrix at initialization, which implies a low effective dimension of the model (far smaller than the number of parameters).;arxiv
1059;We conclude that local capacity control from the low effective rank of the Fisher Information Matrix is still underexplored theoretically.;arxiv
1060;The worldwide spread of pneumonia caused by a novel coronavirus poses an unprecedented challenge to the world's medical resources and prevention and control measures.;arxiv
1061;Covid-19 attacks not only the lungs, making it difficult to breathe and life-threatening, but also the heart, kidneys, brain and other vital organs of the body, with possible sequela.;arxiv
1062;At present, the detection of COVID-19 needs to be realized by the reverse transcription-polymerase Chain Reaction (RT-PCR).;arxiv
1063;However, many countries are in the outbreak period of the epidemic, and the medical resources are very limited.;arxiv
1064;They cannot provide sufficient numbers of gene sequence detection, and many patients may not be isolated and treated in time.;arxiv
1065;Given this situation, we researched the analytical and diagnostic capabilities of deep learning on chest radiographs and proposed Cascade-SEMEnet which is cascaded with SEME-ResNet50 and SEME-DenseNet169.;arxiv
1066;The two cascade networks of Cascade - SEMEnet both adopt large input sizes and SE-Structure and use MoEx and histogram equalization to enhance the data.;arxiv
1067;We first used SEME-ResNet50 to screen chest X-ray and diagnosed three classes: normal, bacterial, and viral pneumonia.;arxiv
1068;Then we used SEME-DenseNet169 for fine-grained classification of viral pneumonia and determined if it is caused by COVID-19.;arxiv
1069;To exclude the influence of non-pathological features on the network, we preprocessed the data with U-Net during the training of SEME-DenseNet169.;arxiv
1070;The results showed that our network achieved an accuracy of 85.6\% in determining the type of pneumonia infection and 97.1\% in the fine-grained classification of COVID-19.;arxiv
1071;We used Grad-CAM to visualize the judgment based on the model and help doctors understand the chest radiograph while verifying the effectivene.;arxiv
1072;Model discovery aims at autonomously discovering differential equations underlying a dataset.;arxiv
1073;Approaches based on Physics Informed Neural Networks (PINNs) have shown great promise, but a fully-differentiable model which explicitly learns the equation has remained elusive.;arxiv
1074;In this paper we propose such an approach by integrating neural network-based surrogates with Sparse Bayesian Learning (SBL).;arxiv
1075;This combination yields a robust model discovery algorithm, which we showcase on various datasets.;arxiv
1076;We then identify a connection with multitask learning, and build on it to construct a Physics Informed Normalizing Flow (PINF).;arxiv
1077;We present a proof-of-concept using a PINF to directly learn a density model from single particle data.;arxiv
1078;Our work expands PINNs to various types of neural network architectures, and connects neural network-based surrogates to the rich field of Bayesian parameter inference.;arxiv
1079;This work investigates the use of deep convolutional neural networks (CNN) to automatically perform measurements of fetal body parts, including head circumference, biparietal diameter, abdominal circumference and femur length, and to estimate gestational age and fetal weight using fetal ultrasound videos.;arxiv
1080;We developed a novel multi-task CNN-based spatio-temporal fetal US feature extraction and standard plane detection algorithm (called FUVAI) and evaluated the method on 50 freehand fetal US video scans.;arxiv
1081;We compared FUVAI fetal biometric measurements with measurements made by five experienced sonographers at two time points separated by at least two weeks.;arxiv
1082;We found that automated fetal biometric measurements obtained by FUVAI were comparable to the measurements performed by experienced sonographers The observed differences in measurement values were within the range of inter- and intra-observer variability.;arxiv
1083;Moreover, analysis has shown that these differences were not statistically significant when comparing any individual medical expert to our model.;arxiv
1084;We argue that FUVAI has the potential to assist sonographers who perform fetal biometric measurements in clinical settings by providing them with suggestions regarding the best measuring frames, along with automated measurements.;arxiv
1085;Moreover, FUVAI is able perform these tasks in just a few seconds, which is a huge difference compared to the average of six minutes taken by sonographers.;arxiv
1086;This is significant, given the shortage of medical experts capable of interpreting fetal ultrasound images in numerous countries.;arxiv
1087;Federated Learning allows training of data stored in distributed devices without the need for centralizing training data, thereby maintaining data privacy.;arxiv
1088;Addressing the ability to handle data heterogeneity (non-identical and independent distribution or non-IID) is a key enabler for the wider deployment of Federated Learning.;arxiv
1089;In this paper, we propose a novel Divide-and-Conquer training methodology that enables the use of the popular FedAvg aggregation algorithm by overcoming the acknowledged FedAvg limitations in non-IID environments.;arxiv
1090;We propose a novel use of Cosine-distance based Weight Divergence metric to determine the exact point where a Deep Learning network can be divided into class agnostic initial layers and class-specific deep layers for performing a Divide and Conquer training.;arxiv
1091;Also, we show that this methodology leads to compute and bandwidth optimizations under certain documented conditions.;arxiv
1092;Gradient Boosting Decision Tree (GBDT) are popular machine learning algorithms with implementations such as LightGBM and in popular machine learning toolkits like Scikit-Learn.;arxiv
1093;Many implementations can only produce trees in an offline manner and in a greedy manner.;arxiv
1094;We explore ways to convert existing GBDT implementations to known neural network architectures with minimal performance loss in order to allow decision splits to be updated in an online manner and provide extensions to allow splits points to be altered as a neural architecture search problem.;arxiv
1095;We provide learning bounds for our neural network.;arxiv
1096;Deep neural networks (DNNs) are one of the most highlighted methods in machine learning.;arxiv
1097;However, as DNNs are black-box models, they lack explanatory power for their predictions.;arxiv
1098;Recently, neural additive models (NAMs) have been proposed to provide this power while maintaining high prediction performance.;arxiv
1099;In this paper, we propose a novel NAM approach for multivariate nowcasting (NC) problems, which comprise an important focus area of machine learning.;arxiv
1100;For the multivariate time-series data used in NC problems, explanations should be considered for every input value to the variables at distinguishable time steps.;arxiv
1101;By employing generalized additive models, the proposed NAM-NC successfully explains each input value's importance for multiple variables and time steps.;arxiv
1102;Experimental results involving a toy example and two real-world datasets show that the NAM-NC predicts multivariate time-series data as accurately as state-of-the-art neural networks, while also providing the explanatory importance of each input value.;arxiv
1103;We also examine parameter-sharing networks using NAM-NC to decrease their complexity, and NAM-MC's hard-tied feature net extracted explanations with good performance.;arxiv
1104;We develop a new method to detect anomalies within time series, which is essential in many application domains, reaching from self-driving cars, finance, and marketing to medical diagnosis and epidemiology.;arxiv
1105;The method is based on self-supervised deep learning that has played a key role in facilitating deep anomaly detection on images, where powerful image transformations are available.;arxiv
1106;However, such transformations are widely unavailable for time series.;arxiv
1107;Addressing this, we develop Local Neural Transformations(LNT), a method learning local transformations of time series from data.;arxiv
1108;The method produces an anomaly score for each time step and thus can be used to detect anomalies within time series.;arxiv
1109;We prove in a theoretical analysis that our novel training objective is more suitable for transformation learning than previous deep Anomaly detection(AD) methods.;arxiv
1110;Our experiments demonstrate that LNT can find anomalies in speech segments from the LibriSpeech data set and better detect interruptions to cyber-physical systems than previous work.;arxiv
1111;Visualization of the learned transformations gives insight into the type of transformations that LNT learns.;arxiv
1112;The operation and management of intelligent transportation systems (ITS), such as traffic monitoring, relies on real-time data aggregation of vehicular traffic information, including vehicular types (e.g., cars, trucks, and buses), in the critical roads and highways.;arxiv
1113;While traditional approaches based on vehicular-embedded GPS sensors or camera networks would either invade drivers' privacy or require high deployment cost, this paper introduces a low-cost method, namely SenseMag, to recognize the vehicular type using a pair of non-invasive magnetic sensors deployed on the straight road section.;arxiv
1114;SenseMag filters out noises and segments received magnetic signals by the exact time points that the vehicle arrives or departs from every sensor node.;arxiv
1115;Further, SenseMag adopts a hierarchical recognition model to first estimate the speed/velocity, then identify the length of vehicle using the predicted speed, sampling cycles, and the distance between the sensor nodes.;arxiv
1116;With the vehicle length identified and the temporal/spectral features extracted from the magnetic signals, SenseMag classify the types of vehicles accordingly.;arxiv
1117;Some semi-automated learning techniques have been adopted for the design of filters, features, and the choice of hyper-parameters.;arxiv
1118;To be specific, our field experiment results validate that SenseMag is with at least $90\%$ vehicle type classification accuracy and less than 5\% vehicle length classification error.;arxiv
1119;As training deep learning models on large dataset takes a lot of time and resources, it is desired to construct a small synthetic dataset with which we can train deep learning models sufficiently.;arxiv
1120;There are recent works that have explored solutions on condensing image datasets through complex bi-level optimization.;arxiv
1121;For instance, dataset condensation (DC) matches network gradients w.r.t.;arxiv
1122;large-real data and small-synthetic data, where the network weights are optimized for multiple steps at each outer iteration.;arxiv
1123;To bridge the gap, we investigate efficient dataset condensation tailored for graph datasets where we model the discrete graph structure as a probabilistic model.;arxiv
1124;We further propose a one-step gradient matching scheme, which performs gradient matching for only one single step without training the network weights.;arxiv
1125;Our theoretical analysis shows this strategy can generate synthetic graphs that lead to lower classification loss on real graphs.;arxiv
1126;Extensive experiments on various graph datasets demonstrate the effectiveness and efficiency of the proposed method.;arxiv
1127;In particular, we are able to reduce the dataset size by 90% while approximating up to 98% of the original performance and our method is significantly faster than multi-step gradient matching (e.g. 15x in CIFAR10 for synthesizing 500 graphs).;arxiv
1128;"Despite the increasing visibility of fine-grained recognition in our field, ""fine-grained'' has thus far lacked a precise definition.";arxiv
1129;In this work, building upon clustering theory, we pursue a framework for measuring dataset granularity.;arxiv
1130;We argue that dataset granularity should depend not only on the data samples and their labels, but also on the distance function we choose.;arxiv
1131;We propose an axiomatic framework to capture desired properties for a dataset granularity measure and provide examples of measures that satisfy these properties.;arxiv
1132;We assess each measure via experiments on datasets with hierarchical labels of varying granularity.;arxiv
1133;When measuring granularity in commonly used datasets with our measure, we find that certain datasets that are widely considered fine-grained in fact contain subsets of considerable size that are substantially more coarse-grained than datasets generally regarded as coarse-grained.;arxiv
1134;We also investigate the interplay between dataset granularity with a variety of factors and find that fine-grained datasets are more difficult to learn from, more difficult to transfer to, more difficult to perform few-shot learning with, and more vulnerable to adversarial attacks.;arxiv
1135;This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights.;arxiv
1136;We prove upper and lower bounds on the approximation error of these networks for smooth function classes.;arxiv
1137;The lower bound is derived through the Rademacher complexity of neural networks, which may be of independent interest.;arxiv
1138;We apply these approximation bounds to analyze the convergence of regression using norm constrained neural networks and distribution estimation by GANs.;arxiv
1139;In particular, we obtain convergence rates for over-parameterized neural networks.;arxiv
1140;It is also shown that GANs can achieve optimal rate of learning probability distributions, when the discriminator is a properly chosen norm constrained neural network.;arxiv
1141;Feature Learning aims to extract relevant information contained in data sets in an automated fashion.;arxiv
1142;It is driving force behind the current deep learning trend, a set of methods that have had widespread empirical success.;arxiv
1143;What is lacking is a theoretical understanding of different feature learning schemes.;arxiv
1144;This work provides a theoretical framework for feature learning and then characterizes when features can be learnt in an unsupervised fashion.;arxiv
1145;We also provide means to judge the quality of features via rate-distortion theory and its generalizations.;arxiv
1146;Deep learning models (DLMs) are state-of-the-art techniques in speech recognition.;arxiv
1147;However, training good DLMs can be time consuming especially for production-size models and corpora.;arxiv
1148;Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them.;arxiv
1149;Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD.;arxiv
1150;ASGD can be used as a substitute in some cases.;arxiv
1151;Design optimization techniques are often used at the beginning of the design process to explore the space of possible designs.;arxiv
1152;In these domains illumination algorithms, such as MAP-Elites, are promising alternatives to classic optimization algorithms because they produce diverse, high-quality solutions in a single run, instead of only a single near-optimal solution.;arxiv
1153;Unfortunately, these algorithms currently require a large number of function evaluations, limiting their applicability.;arxiv
1154;In this article we introduce a new illumination algorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate modeling techniques to create a map of the design space according to user-defined features while minimizing the number of fitness evaluations.;arxiv
1155;On a 2-dimensional airfoil optimization problem SAIL produces hundreds of diverse but high-performing designs with several orders of magnitude fewer evaluations than MAP-Elites or CMA-ES.;arxiv
1156;We demonstrate that SAIL is also capable of producing maps of high-performing designs in realistic 3-dimensional aerodynamic tasks with an accurate flow simulation.;arxiv
1157;Data-efficient design exploration with SAIL can help designers understand what is possible, beyond what is optimal, by considering more than pure objective-based optimization.;arxiv
1158;Network data is increasingly being used in quantitative, data-driven public policy research.;arxiv
1159;These are typically very rich datasets that contain complex correlations and inter-dependencies.;arxiv
1160;This richness both promises to be quite useful for policy research, while at the same time posing a challenge for the useful extraction of information from these datasets - a challenge which calls for new data analysis methods.;arxiv
1161;In this report, we formulate a research agenda of key methodological problems whose solutions would enable new advances across many areas of policy research.;arxiv
1162;We then review recent advances in applying deep learning to network data, and show how these methods may be used to address many of the methodological problems we identified.;arxiv
1163;We particularly emphasize deep generative methods, which can be used to generate realistic synthetic networks useful for microsimulation and agent-based models capable of informing key public policy questions.;arxiv
1164;We extend these recent advances by developing a new generative framework which applies to large social contact networks commonly used in epidemiological modeling.;arxiv
1165;For context, we also compare and contrast these recent neural network-based approaches with the more traditional Exponential Random Graph Models.;arxiv
1166;Lastly, we discuss some open problems where more progress is needed.;arxiv
1167;With deep learning gaining attention from the research community for prediction and control of real physical systems, learning important representations is becoming now more than ever mandatory.;arxiv
1168;It is of extreme importance that deep learning representations are coherent with physics.;arxiv
1169;When learning from discrete data this can be guaranteed by including some sort of prior into the learning, however, not all discretization priors preserve important structures from the physics.;arxiv
1170;In this paper, we introduce Symplectic Momentum Neural Networks (SyMo) as models from a discrete formulation of mechanics for non-separable mechanical systems.;arxiv
1171;The combination of such formulation leads SyMos to be constrained towards preserving important geometric structures such as momentum and a symplectic form and learn from limited data.;arxiv
1172;Furthermore, it allows to learn dynamics only from the poses as training data.;arxiv
1173;We extend SyMos to include variational integrators within the learning framework by developing an implicit root-find layer which leads to End-to-End Symplectic Momentum Neural Networks (E2E-SyMo).;arxiv
1174;Through experimental results, using the pendulum and cartpole, we show that such combination not only allows these models to learn from limited data but also provides the models with the capability of preserving the symplectic form and show better long-term behaviour.;arxiv
1175;We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger.;arxiv
1176;We also show that these gaps can be observed experimentally: Increasing the depth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball.;arxiv
1177;Neural networks represent a field of research that can quickly achieve very good results in the field of medical image segmentation using a GPU.;arxiv
1178;A possible way to achieve good results without GPUs are random forests.;arxiv
1179;For this purpose, two random forest approaches were compared with a state-of-the-art deep convolutional neural network.;arxiv
1180;To make the comparison the PhC-C2DH-U373 and the retinal imaging datasets were used.;arxiv
1181;The evaluation showed that the deep convolutional neutral network achieved the best results.;arxiv
1182;However, one of the random forest approaches also achieved a similar high performance.;arxiv
1183;Our results indicate that random forest approaches are a good alternative to deep convolutional neural networks and, thus, allow the usage of medical image segmentation without a GPU.;arxiv
1184;Microarray technology is a process that allows thousands of genes simultaneously monitor to various experimental conditions.;arxiv
1185;It is used to identify the co-expressed genes in specific cells or tissues that are actively used to make proteins, This method is used to analysis the gene expression, an important task in bioinformatics research.;arxiv
1186;Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes, biologically relevant groupings of genes and samples.;arxiv
1187;In this paper we analysed K-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group the microarray data sets on the basic of ISODATA.;arxiv
1188;AGMFI is to generate initial values for merge and Spilt factor, maximum merge times instead of selecting efficient values as in ISODATA.;arxiv
1189;The initial seeds for each cluster were normally chosen either sequentially or randomly.;arxiv
1190;The quality of the final clusters was found to be influenced by these initial seeds.;arxiv
1191;For the real life problems, the suitable number of clusters cannot be predicted.;arxiv
1192;To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters.;arxiv
1193;This paper is an initial endeavor to bridge the gap between powerful Deep Reinforcement Learning methodologies and the problem of exploration/coverage of unknown terrains.;arxiv
1194;Within this scope, MarsExplorer, an openai-gym compatible environment tailored to exploration/coverage of unknown areas, is presented.;arxiv
1195;MarsExplorer translates the original robotics problem into a Reinforcement Learning setup that various off-the-shelf algorithms can tackle.;arxiv
1196;Any learned policy can be straightforwardly applied to a robotic platform without an elaborate simulation model of the robot's dynamics to apply a different learning/adaptation phase.;arxiv
1197;One of its core features is the controllable multi-dimensional procedural generation of terrains, which is the key for producing policies with strong generalization capabilities.;arxiv
1198;Four different state-of-the-art RL algorithms (A3C, PPO, Rainbow, and SAC) are trained on the MarsExplorer environment, and a proper evaluation of their results compared to the average human-level performance is reported.;arxiv
1199;In the follow-up experimental analysis, the effect of the multi-dimensional difficulty setting on the learning capabilities of the best-performing algorithm (PPO) is analyzed.;arxiv
1200;A milestone result is the generation of an exploration policy that follows the Hilbert curve without providing this information to the environment or rewarding directly or indirectly Hilbert-curve-like trajectories.;arxiv
1201;The experimental analysis is concluded by evaluating PPO learned policy algorithm side-by-side with frontier-based exploration strategies.;arxiv
1202;A study on the performance curves revealed that PPO-based policy was capable of performing adaptive-to-the-unknown-terrain sweeping without leaving expensive-to-revisit areas uncovered, underlying the capability of RL-based methodologies to tackle exploration tasks efficiently.;arxiv
1203;The source code can be found at: https://github.com/dimikout3/MarsExplorer.;arxiv
1204;Synthesizing optimal controllers for dynamical systems often involves solving optimization problems with hard real-time constraints.;arxiv
1205;These constraints determine the class of numerical methods that can be applied: computationally expensive but accurate numerical routines are replaced by fast and inaccurate methods, trading inference time for solution accuracy.;arxiv
1206;This paper provides techniques to improve the quality of optimized control policies given a fixed computational budget.;arxiv
1207;We achieve the above via a hypersolvers approach, which hybridizes a differential equation solver and a neural network.;arxiv
1208;The performance is evaluated in direct and receding-horizon optimal control tasks in both low and high dimensions, where the proposed approach shows consistent Pareto improvements in solution accuracy and control performance.;arxiv
1209;Canonical correlation analysis (CCA) is a powerful technique for discovering whether or not hidden sources are commonly present in two (or more) datasets.;arxiv
1210;Its well-appreciated merits include dimensionality reduction, clustering, classification, feature selection, and data fusion.;arxiv
1211;The standard CCA however, does not exploit the geometry of the common sources, which may be available from the given data or can be deduced from (cross-) correlations.;arxiv
1212;In this paper, this extra information provided by the common sources generating the data is encoded in a graph, and is invoked as a graph regularizer.;arxiv
1213;This leads to a novel graph-regularized CCA approach, that is termed graph (g) CCA.;arxiv
1214;The novel gCCA accounts for the graph-induced knowledge of common sources, while minimizing the distance between the wanted canonical variables.;arxiv
1215;Tailored for diverse practical settings where the number of data is smaller than the data vector dimensions, the dual formulation of gCCA is also developed.;arxiv
1216;One such setting includes kernels that are incorporated to account for nonlinear data dependencies.;arxiv
1217;The resultant graph-kernel (gk) CCA is also obtained in closed form.;arxiv
1218;Finally, corroborating image classification tests over several real datasets are presented to showcase the merits of the novel linear, dual, and kernel approaches relative to competing alternatives.;arxiv
1219;Sparse subspace clustering methods with sparsity induced by $\ell^{0}$-norm, such as $\ell^{0}$-Sparse Subspace Clustering ($\ell^{0}$-SSC)~\citep{YangFJYH16-L0SSC-ijcv}, are demonstrated to be more effective than its $\ell^{1}$ counterpart such as Sparse Subspace Clustering (SSC)~\citep{ElhamifarV13}.;arxiv
1220;However, the theoretical analysis of $\ell^{0}$-SSC is restricted to clean data that lie exactly in subspaces.;arxiv
1221;Real data often suffer from noise and they may lie close to subspaces.;arxiv
1222;In this paper, we show that an optimal solution to the optimization problem of noisy $\ell^{0}$-SSC achieves subspace detection property (SDP), a key element with which data from different subspaces are separated, under deterministic and semi-random model.;arxiv
1223;Our results provide theoretical guarantee on the correctness of noisy $\ell^{0}$-SSC in terms of SDP on noisy data for the first time, which reveals the advantage of noisy $\ell^{0}$-SSC in terms of much less restrictive condition on subspace affinity.;arxiv
1224;In order to improve the efficiency of noisy $\ell^{0}$-SSC, we propose Noisy-DR-$\ell^{0}$-SSC which provably recovers the subspaces on dimensionality reduced data.;arxiv
1225;Noisy-DR-$\ell^{0}$-SSC first projects the data onto a lower dimensional space by random projection, then performs noisy $\ell^{0}$-SSC on the projected data for improved efficiency.;arxiv
1226;This paper considers joint analysis of multiple functionally related structures in classification tasks.;arxiv
1227;In particular, our method developed is driven by how functionally correlated brain structures vary together between autism and control groups.;arxiv
1228;We find that the resulting joint structure is effective, robust, and interpretable in recognizing the underlying patterns of the joint variation of multi-block non-Euclidean data.;arxiv
1229;We verified the method in classifying the structural shape data collected from cases that developed and did not develop into Autistic Spectrum Disorder (ASD).;arxiv
1230;In recent years cybersecurity has become a major concern in adaptation of smart applications.;arxiv
1231;Specially, in smart homes where a large number of IoT devices are used having a secure and trusted mechanisms can provide peace of mind for users.;arxiv
1232;Accurate detection of cyber attacks is crucial, however precise identification of the type of attacks plays a huge role if devising the countermeasure for protecting the system.;arxiv
1233;Artificial Neural Networks (ANN) have provided promising results for detecting any security attacks for smart applications.;arxiv
1234;However, due to complex nature of the model used for this technique it is not easy for normal users to trust ANN based security solutions.;arxiv
1235;Also, selection of right hyperparameters for ANN architecture plays a crucial role in the accurate detection of security attacks, especially when it come to identifying the subcategories of attacks.;arxiv
1236;In this paper, we propose a model that considers both the issues of explainability of ANN model and the hyperparameter selection for this approach to be easily trusted and adapted by users of smart home applications.;arxiv
1237;Also, our approach considers a subset of the dataset for optimal selection of hyperparamters to reduce the overhead of the process of ANN architecture design.;arxiv
1238;Distinctively this paper focuses on configuration, performance and evaluation of ANN architecture for identification of five categorical attacks and nine subcategorical attacks.;arxiv
1239;Using a very recent IoT dataset our approach showed high performance for intrusion detection with 99.9%, 99.7%, and 97.7% accuracy for Binary, Category, and Subcategory level classification of attacks.;arxiv
1240;Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance.;arxiv
1241;In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling.;arxiv
1242;Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images.;arxiv
1243;For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction.;arxiv
1244;The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image.;arxiv
1245;Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class.;arxiv
1246;Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success.;arxiv
1247;Understanding the operation of biological and artificial networks remains a difficult and important challenge.;arxiv
1248;To identify general principles, researchers are increasingly interested in surveying large collections of networks that are trained on, or biologically adapted to, similar tasks.;arxiv
1249;A standardized set of analysis tools is now needed to identify how network-level covariates -- such as architecture, anatomical brain region, and model organism -- impact neural representations (hidden layer activations).;arxiv
1250;Here, we provide a rigorous foundation for these analyses by defining a broad family of metric spaces that quantify representational dissimilarity.;arxiv
1251;Using this framework we modify existing representational similarity measures based on canonical correlation analysis to satisfy the triangle inequality, formulate a novel metric that respects the inductive biases in convolutional layers, and identify approximate Euclidean embeddings that enable network representations to be incorporated into essentially any off-the-shelf machine learning method.;arxiv
1252;We demonstrate these methods on large-scale datasets from biology (Allen Institute Brain Observatory) and deep learning (NAS-Bench-101).;arxiv
1253;In doing so, we identify relationships between neural representations that are interpretable in terms of anatomical features and model performance.;arxiv
1254;We propose a novel and effective purification based adversarial defense method against pre-processor blind white- and black-box attacks.;arxiv
1255;Our method is computationally efficient and trained only with self-supervised learning on general images, without requiring any adversarial training or retraining of the classification model.;arxiv
1256;We first show an empirical analysis on the adversarial noise, defined to be the residual between an original image and its adversarial example, has almost zero mean, symmetric distribution.;arxiv
1257;Based on this observation, we propose a very simple iterative Gaussian Smoothing (GS) which can effectively smooth out adversarial noise and achieve substantially high robust accuracy.;arxiv
1258;To further improve it, we propose Neural Contextual Iterative Smoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised manner to reconstruct the discriminative features of the original image that is also smoothed out by GS.;arxiv
1259;From our extensive experiments on the large-scale ImageNet using four classification models, we show that our method achieves both competitive standard accuracy and state-of-the-art robust accuracy against most strong purifier-blind white- and black-box attacks.;arxiv
1260;Also, we propose a new benchmark for evaluating a purification method based on commercial image classification APIs, such as AWS, Azure, Clarifai and Google.;arxiv
1261;We generate adversarial examples by ensemble transfer-based black-box attack, which can induce complete misclassification of APIs, and demonstrate that our method can be used to increase adversarial robustness of APIs.;arxiv
1262;Correspondence-based shape models are key to various medical imaging applications that rely on a statistical analysis of anatomies.;arxiv
1263;Such shape models are expected to represent consistent anatomical features across the population for population-specific shape statistics.;arxiv
1264;Early approaches for correspondence placement rely on nearest neighbor search for simpler anatomies.;arxiv
1265;Coordinate transformations for shape correspondence hold promise to address the increasing anatomical complexities.;arxiv
1266;Nonetheless, due to the inherent shape-level geometric complexity and population-level shape variation, the coordinate-wise correspondence often does not translate to the anatomical correspondence.;arxiv
1267;An alternative, group-wise approach for correspondence placement explicitly models the trade-off between geometric description and the population's statistical compactness.;arxiv
1268;However, these models achieve limited success in resolving nonlinear shape correspondence.;arxiv
1269;Recent works have addressed this limitation by adopting an application-specific notion of correspondence through lifting positional data to a higher dimensional feature space.;arxiv
1270;However, they heavily rely on manual expertise to create domain-specific features and consistent landmarks.;arxiv
1271;This paper proposes an automated feature learning approach, using deep convolutional neural networks to extract correspondence-friendly features from shape ensembles.;arxiv
1272;Further, an unsupervised domain adaptation scheme is introduced to augment the pretrained geometric features with new anatomies.;arxiv
1273;Results on anatomical datasets of human scapula, femur, and pelvis bones demonstrate that features learned in supervised fashion show improved performance for correspondence estimation compared to the manual features.;arxiv
1274;Further, unsupervised learning is demonstrated to learn complex anatomy features using the supervised domain adaptation from features learned on simpler anatomy.;arxiv
1275;Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine.;arxiv
1276;The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values.;arxiv
1277;Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models.;arxiv
1278;We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time.;arxiv
1279;We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.;arxiv
1280;Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a candidate's speaking proficiency in a language.;arxiv
1281;ASS systems face many challenges like open grammar, variable pronunciations, and unstructured or semi-structured content.;arxiv
1282;Recent deep learning approaches have shown some promise in this domain.;arxiv
1283;However, most of these approaches focus on extracting features from a single audio, making them suffer from the lack of speaker-specific context required to model such a complex task.;arxiv
1284;We propose a novel deep learning technique for non-native ASS, called speaker-conditioned hierarchical modeling.;arxiv
1285;In our technique, we take advantage of the fact that oral proficiency tests rate multiple responses for a candidate.;arxiv
1286;We extract context vectors from these responses and feed them as additional speaker-specific context to our network to score a particular response.;arxiv
1287;We compare our technique with strong baselines and find that such modeling improves the model's average performance by 6.92% (maximum = 12.86%, minimum = 4.51%).;arxiv
1288;We further show both quantitative and qualitative insights into the importance of this additional context in solving the problem of ASS.;arxiv
1289;We describe a layer-by-layer algorithm for training deep convolutional networks, where each step involves gradient updates for a two layer network followed by a simple clustering algorithm.;arxiv
1290;Our algorithm stems from a deep generative model that generates mages level by level, where lower resolution images correspond to latent semantic classes.;arxiv
1291;We analyze the convergence rate of our algorithm assuming that the data is indeed generated according to this model (as well as additional assumptions).;arxiv
1292;While we do not pretend to claim that the assumptions are realistic for natural images, we do believe that they capture some true properties of real data.;arxiv
1293;Furthermore, we show that our algorithm actually works in practice (on the CIFAR dataset), achieving results in the same ballpark as that of vanilla convolutional neural networks that are being trained by stochastic gradient descent.;arxiv
1294;Finally, our proof techniques may be of independent interest.;arxiv
1295;Calculating hydrocarbon components solubility of natural gases is known as one of the important issues for operational works in petroleum and chemical engineering.;arxiv
1296;In this work, a novel solubility estimation tool has been proposed for hydrocarbon gases including methane, ethane, propane, and butane in aqueous electrolyte solutions based on extreme learning machine (ELM) algorithm.;arxiv
1297;Comparing the ELM outputs with a comprehensive real databank which has 1175 solubility points concluded to R-squared values of 0.985 and 0.987 for training and testing phases respectively.;arxiv
1298;Furthermore, the visual comparison of estimated and actual hydrocarbon solubility led to confirm the ability of the proposed solubility model.;arxiv
1299;Additionally, sensitivity analysis has been employed on the input variables of the model to identify their impacts on hydrocarbon solubility.;arxiv
1300;Such a comprehensive and reliable study can help engineers and scientists to successfully determine the important thermodynamic properties which are key factors in optimizing and designing different industrial units such as refineries and petrochemical plants.;arxiv
1301;Actively sampled data can have very different characteristics than passively sampled data.;arxiv
1302;Therefore, it's promising to investigate using different inference procedures during AL than are used during passive learning (PL).;arxiv
1303;This general idea is explored in detail for the focused case of AL with cost-weighted SVMs for imbalanced data, a situation that arises for many HLT tasks.;arxiv
1304;The key idea behind the proposed InitPA method for addressing imbalance is to base cost models during AL on an estimate of overall corpus imbalance computed via a small unbiased sample rather than the imbalance in the labeled training data, which is the leading method used during PL.;arxiv
1305;Weight sharing based and predictor based methods are two major types of fast neural architecture search methods.;arxiv
1306;In this paper, we propose to jointly use weight sharing and predictor in a unified framework.;arxiv
1307;First, we construct a SuperNet in a weight-sharing way and probabilisticly sample architectures from the SuperNet.;arxiv
1308;To increase the correctness of the evaluation of architectures, besides direct evaluation using the inherited weights, we further apply a few-shot predictor to assess the architecture on the other hand.;arxiv
1309;The final evaluation of the architecture is the combination of direct evaluation, the prediction from the predictor and the cost of the architecture.;arxiv
1310;We regard the evaluation as a reward and apply a self-critical policy gradient approach to update the architecture probabilities.;arxiv
1311;To further reduce the side effects of weight sharing, we propose a weakly weight sharing method by introducing another HyperNet.;arxiv
1312;We conduct experiments on datasets including CIFAR-10, CIFAR-100 and ImageNet under NATS-Bench, DARTS and MobileNet search space.;arxiv
1313;The proposed WPNAS method achieves state-of-the-art performance on these datasets.;arxiv
1314;We propose a feed-forward inference method applicable to belief and neural networks.;arxiv
1315;In a belief network, the method estimates an approximate factorized posterior of all hidden units given the input.;arxiv
1316;In neural networks the method propagates uncertainty of the input through all the layers.;arxiv
1317;In neural networks with injected noise, the method analytically takes into account uncertainties resulting from this noise.;arxiv
1318;Such feed-forward analytic propagation is differentiable in parameters and can be trained end-to-end.;arxiv
1319;Compared to standard NN, which can be viewed as propagating only the means, we propagate the mean and variance.;arxiv
1320;The method can be useful in all scenarios that require knowledge of the neuron statistics, e.g. when dealing with uncertain inputs, considering sigmoid activations as probabilities of Bernoulli units, training the models regularized by injected noise (dropout) or estimating activation statistics over the dataset (as needed for normalization methods).;arxiv
1321;In the experiments we show the possible utility of the method in all these tasks as well as its current limitations.;arxiv
1322;The notion of approachability in repeated games with vector payoffs was introduced by Blackwell in the 1950s, along with geometric conditions for approachability and corresponding strategies that rely on computing {\em steering directions} as projections from the current average payoff vector to the (convex) target set.;arxiv
1323;Recently, Abernethy, Batlett and Hazan (2011) proposed a class of approachability algorithms that rely on the no-regret properties of Online Linear Programming for computing a suitable sequence of steering directions.;arxiv
1324;This is first carried out for target sets that are convex cones, and then generalized to any convex set by embedding it in a higher-dimensional convex cone.;arxiv
1325;In this paper we present a more direct formulation that relies on the support function of the set, along with suitable Online Convex Optimization algorithms, which leads to a general class of approachability algorithms.;arxiv
1326;We further show that Blackwell's original algorithm and its convergence follow as a special case.;arxiv
1327;Research on connected vehicles represents a continuously evolving technological domain, fostered by the emerging Internet of Things (IoT) paradigm and the recent advances in intelligent transportation systems.;arxiv
1328;Nowadays, vehicles are platforms capable of generating, receiving and automatically act based on large amount of data.;arxiv
1329;In the context of assisted driving, connected vehicle technology provides real-time information about the surrounding traffic conditions.;arxiv
1330;Such information is expected to improve drivers' quality of life, for example, by adopting decision making strategies according to the current parking availability status.;arxiv
1331;In this context, we propose an online and adaptive scheme for parking availability mapping.;arxiv
1332;Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines.;arxiv
1333;This work introduces a novel physics-informed deep learning framework to discover governing partial differential equations (PDEs) from scarce and noisy data for nonlinear spatiotemporal systems.;arxiv
1334;The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of PDE systems with different levels of data scarcity and noise accounting for different initial/boundary conditions.;arxiv
1335;The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.;arxiv
1336;We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature.;arxiv
1337;"Recently, (Hsu 2018) and (Jiang 2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior ""classical"" algorithms that did not use oracles.";arxiv
1338;"In this paper, we explore the power of a ""heavy edge"" oracle in multiple graph edge streaming models.";arxiv
1339;In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle.;arxiv
1340;In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal.;arxiv
1341;We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs.;arxiv
1342;"Our methodology expands upon prior work on ""classical"" streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle.";arxiv
1343;Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms.;arxiv
1344;Manifold learning techniques have become increasingly valuable as data continues to grow in size.;arxiv
1345;By discovering a lower-dimensional representation (embedding) of the structure of a dataset, manifold learning algorithms can substantially reduce the dimensionality of a dataset while preserving as much information as possible.;arxiv
1346;However, state-of-the-art manifold learning algorithms are opaque in how they perform this transformation.;arxiv
1347;Understanding the way in which the embedding relates to the original high-dimensional space is critical in exploratory data analysis.;arxiv
1348;We previously proposed a Genetic Programming method that performed manifold learning by evolving mappings that are transparent and interpretable.;arxiv
1349;In this paper, we substantially extend our previous work, by introducing a multi-objective approach that automatically balances the competing objectives of manifold quality and dimensionality.;arxiv
1350;Our proposed approach is competitive with a range of baseline and state-of-the-art manifold learning methods, while also providing a range (front) of solutions that give different trade-offs between quality and dimensionality.;arxiv
1351;Furthermore, the learned models are shown to often be simple and efficient, utilising only a small number of features in an interpretable manner.;arxiv
1352;Practical brain-machine interfaces have been widely studied to accurately detect human intentions using brain signals in the real world.;arxiv
1353;However, the electroencephalography (EEG) signals are distorted owing to the artifacts such as walking and head movement, so brain signals may be large in amplitude rather than desired EEG signals.;arxiv
1354;Due to these artifacts, detecting accurately human intention in the mobile environment is challenging.;arxiv
1355;In this paper, we proposed the reconstruction framework based on generative adversarial networks using the event-related potentials (ERP) during walking.;arxiv
1356;We used a pre-trained convolutional encoder to represent latent variables and reconstructed ERP through the generative model which shape similar to the opposite of encoder.;arxiv
1357;Finally, the ERP was classified using the discriminative model to demonstrate the validity of our proposed framework.;arxiv
1358;As a result, the reconstructed signals had important components such as N200 and P300 similar to ERP during standing.;arxiv
1359;The accuracy of reconstructed EEG was similar to raw noisy EEG signals during walking.;arxiv
1360;The signal-to-noise ratio of reconstructed EEG was significantly increased as 1.3.;arxiv
1361;The loss of the generative model was 0.6301, which is comparatively low, which means training generative model had high performance.;arxiv
1362;The reconstructed ERP consequentially showed an improvement in classification performance during walking through the effects of noise reduction.;arxiv
1363;The proposed framework could help recognize human intention based on the brain-machine interface even in the mobile environment.;arxiv
1364;Recurrent neural networks (RNNs) are powerful dynamical models for data with complex temporal structure.;arxiv
1365;However, training RNNs has traditionally proved challenging due to exploding or vanishing of gradients.;arxiv
1366;RNN models such as LSTMs and GRUs (and their variants) significantly mitigate these issues associated with training by introducing various types of gating units into the architecture.;arxiv
1367;While these gates empirically improve performance, how the addition of gates influences the dynamics and trainability of GRUs and LSTMs is not well understood.;arxiv
1368;Here, we take the perspective of studying randomly initialized LSTMs and GRUs as dynamical systems, and ask how the salient dynamical properties are shaped by the gates.;arxiv
1369;We leverage tools from random matrix theory and mean-field theory to study the state-to-state Jacobians of GRUs and LSTMs.;arxiv
1370;We show that the update gate in the GRU and the forget gate in the LSTM can lead to an accumulation of slow modes in the dynamics.;arxiv
1371;Moreover, the GRU update gate can poise the system at a marginally stable point.;arxiv
1372;The reset gate in the GRU and the output and input gates in the LSTM control the spectral radius of the Jacobian, and the GRU reset gate also modulates the complexity of the landscape of fixed-points.;arxiv
1373;Furthermore, for the GRU we obtain a phase diagram describing the statistical properties of fixed-points.;arxiv
1374;We also provide a preliminary comparison of training performance to the various dynamical regimes realized by varying hyperparameters.;arxiv
1375;Looking to the future, we have introduced a powerful set of techniques which can be adapted to a broad class of RNNs, to study the influence of various architectural choices on dynamics, and potentially motivate the principled discovery of novel architectures.;arxiv
1376;Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning.;arxiv
1377;In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack.;arxiv
1378;"Recently, a dizzying number of ""X-former"" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency.";arxiv
1379;"With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored ""X-former"" models, providing an organized and comprehensive overview of existing work and models across multiple domains.";arxiv
1380;We study adaptive sensing of Cox point processes, a widely used model from spatial statistics.;arxiv
1381;We introduce three tasks: maximization of captured events, search for the maximum of the intensity function and learning level sets of the intensity function.;arxiv
1382;We model the intensity function as a sample from a truncated Gaussian process, represented in a specially constructed positive basis.;arxiv
1383;In this basis, the positivity constraint on the intensity function has a simple form.;arxiv
1384;We show how an minimal description positive basis can be adapted to the covariance kernel, non-stationarity and make connections to common positive bases from prior works.;arxiv
1385;Our adaptive sensing algorithms use Langevin dynamics and are based on posterior sampling (\textsc{Cox-Thompson}) and top-two posterior sampling (\textsc{Top2}) principles.;arxiv
1386;With latter, the difference between samples serves as a surrogate to the uncertainty.;arxiv
1387;We demonstrate the approach using examples from environmental monitoring and crime rate modeling, and compare it to the classical Bayesian experimental design approach.;arxiv
1388;Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images.;arxiv
1389;However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation.;arxiv
1390;In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort.;arxiv
1391;We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt.;arxiv
1392;Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation.;arxiv
1393;Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation.;arxiv
1394;Extensive results and comparisons demonstrate the effectiveness of our approaches.;arxiv
1395;Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters.;arxiv
1396;Some models able to learn arbitrary spectral filters have emerged recently.;arxiv
1397;However, few works analyze the expressive power of spectral GNNs.;arxiv
1398;This paper studies spectral GNNs' expressive power theoretically.;arxiv
1399;We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality.;arxiv
1400;We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power.;arxiv
1401;Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum.;arxiv
1402;Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions.;arxiv
1403;JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.;arxiv
1404;The linear quadratic regulator (LQR) problem has reemerged as an important theoretical benchmark for reinforcement learning-based control of complex dynamical systems with continuous state and action spaces.;arxiv
1405;In contrast with nearly all recent work in this area, we consider multiplicative noise models, which are increasingly relevant because they explicitly incorporate inherent uncertainty and variation in the system dynamics and thereby improve robustness properties of the controller.;arxiv
1406;Although policy gradient algorithms require optimization of a non-convex cost function, we show that the multiplicative noise LQR cost has a special property called gradient domination, which is exploited to prove global convergence of policy gradient algorithms to the globally optimum control policy with polynomial dependence on problem parameters.;arxiv
1407;Results are provided both in the model-known and model-unknown settings where samples of system trajectories are used to estimate policy gradients.;arxiv
1408;Systems of competing agents can often be modeled as games.;arxiv
1409;Assuming rationality, the most likely outcomes are given by an equilibrium, e.g. a Nash equilibrium.;arxiv
1410;Often only game equilibria are observed, while the players' true cost functions are unknown.;arxiv
1411;This work introduces Nash Fixed Point Networks (N-FPNs), a class of implicit neural networks that learn to predict the equilibria given only the context.;arxiv
1412;The N-FPN design fuses data-driven modeling with provided constraints on the actions available to agents.;arxiv
1413;N-FPNs are compatible with the recently introduced Jacobian-Free Backpropagation technique for training implicit networks, making them significantly faster to train than prior models.;arxiv
1414;N-FPNs can exploit novel constraint decoupling to avoid costly projections.;arxiv
1415;Provided numerical examples show the efficacy of N-FPNs on atomic and non-atomic games (e.g. traffic routing);arxiv
1416;In this paper, we propose an unsupervised data-driven approach to predict real-time locational marginal prices (RTLMPs).;arxiv
1417;The proposed approach is built upon a general data structure for organizing system-wide heterogeneous market data streams into the format of market data images and videos.;arxiv
1418;Leveraging this general data structure, the system-wide RTLMP prediction problem is formulated as a video prediction problem.;arxiv
1419;A video prediction model based on generative adversarial networks (GAN) is proposed to learn the spatio-temporal correlations among historical RTLMPs and predict system-wide RTLMPs for the next hour.;arxiv
1420;An autoregressive moving average (ARMA) calibration method is adopted to improve the prediction accuracy.;arxiv
1421;The proposed RTLMP prediction method takes public market data as inputs, without requiring any confidential information on system topology, model parameters, or market operating details.;arxiv
1422;Case studies using public market data from ISO New England (ISO-NE) and Southwest Power Pool (SPP) demonstrate that the proposed method is able to learn spatio-temporal correlations among RTLMPs and perform accurate RTLMP prediction.;arxiv
1423;Multi-label learning has attracted the attention of the machine learning community.;arxiv
1424;The problem conversion method Binary Relevance converts a familiar single label into a multi-label algorithm.;arxiv
1425;The binary relevance method is widely used because of its simple structure and efficient algorithm.;arxiv
1426;But binary relevance does not consider the links between labels, making it cumbersome to handle some tasks.;arxiv
1427;This paper proposes a multi-label learning algorithm that can also be used for single-label classification.;arxiv
1428;It is based on standard support vector machines and changes the original single decision hyperplane into two parallel decision hyper-planes, which call multi-label parallel support vector machine (MLPSVM).;arxiv
1429;At the end of the article, MLPSVM is compared with other multi-label learning algorithms.;arxiv
1430;The experimental results show that the algorithm performs well on data sets.;arxiv
1431;The choice of the distance function depends mainly on the type of the selected variables.;arxiv
1432;Unfortunately, relatively few options permit to handle mixed type variables, a situation frequently encountered in official statistics.;arxiv
1433;Unfortunately, the unweighted standard setting the contribution of the single variables to the overall Gower's distance is unbalanced because of the different nature of the variables themselves.;arxiv
1434;This article tries to address the main drawbacks that affect the overall unweighted Gower's distance by suggesting some modifications in calculating the distance on the interval and ratio scaled variables.;arxiv
1435;The performance of the proposals is evaluated in simulations mimicking the imputation of missing values through nearest neighbor distance hotdeck method.;arxiv
1436;Object proposal technique with dense anchoring scheme for scene text detection were applied frequently to achieve high recall.;arxiv
1437;It results in the significant improvement in accuracy but waste of computational searching, regression and classification.;arxiv
1438;In this paper, we propose an anchor selection-based region proposal network (AS-RPN) using effective selected anchors instead of dense anchors to extract text proposals.;arxiv
1439;The center, scales, aspect ratios and orientations of anchors are learnable instead of fixing, which leads to high recall and greatly reduced numbers of anchors.;arxiv
1440;By replacing the anchor-based RPN in Faster RCNN, the AS-RPN-based Faster RCNN can achieve comparable performance with previous state-of-the-art text detecting approaches on standard benchmarks, including COCO-Text, ICDAR2013, ICDAR2015 and MSRA-TD500 when using single-scale and single model (ResNet50) testing only.;arxiv
1441;Given multiple source domains, domain generalization aims at learning a universal model that performs well on any unseen but related target domain.;arxiv
1442;In this work, we focus on the domain generalization scenario where domain shifts occur among class-conditional distributions of different domains.;arxiv
1443;Existing approaches are not sufficiently robust when the variation of conditional distributions given the same class is large.;arxiv
1444;In this work, we extend the concept of distributional robust optimization to solve the class-conditional domain generalization problem.;arxiv
1445;Our approach optimizes the worst-case performance of a classifier over class-conditional distributions within a Wasserstein ball centered around the barycenter of the source conditional distributions.;arxiv
1446;We also propose an iterative algorithm for learning the optimal radius of the Wasserstein balls automatically.;arxiv
1447;Experiments show that the proposed framework has better performance on unseen target domain than approaches without domain generalization.;arxiv
1448;Due to information asymmetry, finding optimal policies for Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) is hard with the complexity growing doubly exponentially in the horizon length.;arxiv
1449;The challenge increases greatly in the multi-agent reinforcement learning (MARL) setting where the transition probabilities, observation kernel, and reward function are unknown.;arxiv
1450;Here, we develop a general compression framework with approximate common and private state representations, based on which decentralized policies can be constructed.;arxiv
1451;We derive the optimality gap of executing dynamic programming (DP) with the approximate states in terms of the approximation error parameters and the remaining time steps.;arxiv
1452;When the compression is exact (no error), the resulting DP is equivalent to the one in existing work.;arxiv
1453;Our general framework generalizes a number of methods proposed in the literature.;arxiv
1454;"The results shed light on designing practically useful deep-MARL network structures under the ""centralized learning distributed execution"" scheme.";arxiv
1455;With an abundance of research papers in deep learning, reproducibility or adoption of the existing works becomes a challenge.;arxiv
1456;This is due to the lack of open source implementations provided by the authors.;arxiv
1457;Further, re-implementing research papers in a different library is a daunting task.;arxiv
1458;To address these challenges, we propose a novel extensible approach, DLPaper2Code, to extract and understand deep learning design flow diagrams and tables available in a research paper and convert them to an abstract computational graph.;arxiv
1459;The extracted computational graph is then converted into execution ready source code in both Keras and Caffe, in real-time.;arxiv
1460;An arXiv-like website is created where the automatically generated designs is made publicly available for 5,000 research papers.;arxiv
1461;The generated designs could be rated and edited using an intuitive drag-and-drop UI framework in a crowdsourced manner.;arxiv
1462;To evaluate our approach, we create a simulated dataset with over 216,000 valid design visualizations using a manually defined grammar.;arxiv
1463;Experiments on the simulated dataset show that the proposed framework provide more than $93\%$ accuracy in flow diagram content extraction.;arxiv
1464;Signature is an infinite graded sequence of statistics known to characterize geometric rough paths, which includes the paths with bounded variation.;arxiv
1465;This object has been studied successfully for machine learning with mostly applications in low dimensional cases.;arxiv
1466;In the high dimensional case, it suffers from exponential growth in the number of features in truncated signature transform.;arxiv
1467;We propose a novel neural network based model which borrows the idea from Convolutional Neural Network to address this problem.;arxiv
1468;Our model reduces the number of features efficiently in a data dependent way.;arxiv
1469;Some empirical experiments are provided to support our model.;arxiv
1470;Virtually any model we use in machine learning to make predictions does not perfectly represent reality.;arxiv
1471;So, most of the learning happens under model misspecification.;arxiv
1472;In this work, we present a novel analysis of the generalization performance of Bayesian model averaging under model misspecification and i.i.d. data using a new family of second-order PAC-Bayes bounds.;arxiv
1473;This analysis shows, in simple and intuitive terms, that Bayesian model averaging provides suboptimal generalization performance when the model is misspecified.;arxiv
1474;In consequence, we provide strong theoretical arguments showing that Bayesian methods are not optimal for learning predictive models, unless the model class is perfectly specified.;arxiv
1475;Using novel second-order PAC-Bayes bounds, we derive a new family of Bayesian-like algorithms, which can be implemented as variational and ensemble methods.;arxiv
1476;The output of these algorithms is a new posterior distribution, different from the Bayesian posterior, which induces a posterior predictive distribution with better generalization performance.;arxiv
1477;Experiments with Bayesian neural networks illustrate these findings.;arxiv
1478;Large pre-trained language models have been used to generate code,providing a flexible interface for synthesizing programs from natural language specifications.;arxiv
1479;However, they often violate syntactic and semantic rules of their output language, limiting their practical usability.;arxiv
1480;In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation.;arxiv
1481;First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection.;arxiv
1482;TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features.;arxiv
1483;Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language.;arxiv
1484;CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model.;arxiv
1485;We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs.;arxiv
1486;These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic.;arxiv
1487;We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors.;arxiv
1488;We present a dataset consisting of high-resolution images of 13 micro-PCBs captured in various rotations and perspectives relative to the camera, with each sample labeled for PCB type, rotation category, and perspective categories.;arxiv
1489;We then present the design and results of experimentation on combinations of rotations and perspectives used during training and the resulting impact on test accuracy.;arxiv
1490;We then show when and how well data augmentation techniques are capable of simulating rotations vs. perspectives not present in the training data.;arxiv
1491;We perform all experiments using CNNs with and without homogeneous vector capsules (HVCs) and investigate and show the capsules' ability to better encode the equivariance of the sub-components of the micro-PCBs.;arxiv
1492;The results of our experiments lead us to conclude that training a neural network equipped with HVCs, capable of modeling equivariance among sub-components, coupled with training on a diversity of perspectives, achieves the greatest classification accuracy on micro-PCB data.;arxiv
1493;This paper proposes a differentiable linear quadratic Model Predictive Control (MPC) framework for safe imitation learning.;arxiv
1494;The infinite-horizon cost is enforced using a terminal cost function obtained from the discrete-time algebraic Riccati equation (DARE), so that the learned controller can be proven to be stabilizing in closed-loop.;arxiv
1495;A central contribution is the derivation of the analytical derivative of the solution of the DARE, thereby allowing the use of differentiation-based learning methods.;arxiv
1496;A further contribution is the structure of the MPC optimization problem: an augmented Lagrangian method ensures that the MPC optimization is feasible throughout training whilst enforcing hard constraints on state and input, and a pre-stabilizing controller ensures that the MPC solution and derivatives are accurate at each iteration.;arxiv
1497;The learning capabilities of the framework are demonstrated in a set of numerical studies.;arxiv
1498;Deep neural networks (DNNs) are becoming more prevalent in important safety-critical applications, where reliability in the prediction is paramount.;arxiv
1499;Despite their exceptional prediction capabilities, current DNNs do not have an implicit mechanism to quantify and propagate significant input data uncertainty -- which is common in safety-critical applications.;arxiv
1500;In many cases, this uncertainty is epistemic and can arise from multiple sources, such as lack of knowledge about the data generating process, imprecision, ignorance, and poor understanding of physics phenomena.;arxiv
1501;Recent approaches have focused on quantifying parameter uncertainty, but approaches to end-to-end training of DNNs with epistemic input data uncertainty are more limited and largely problem-specific.;arxiv
1502;In this work, we present a DNN optimized with gradient-based methods capable to quantify input and parameter uncertainty by means of interval analysis, which we call Deep Interval Neural Network (DINN).;arxiv
1503;We perform experiments on an air pollution dataset with sensor uncertainty and show that the DINN can produce accurate bounded estimates from uncertain input data.;arxiv
1504;Quick Shift is a popular mode-seeking and clustering algorithm.;arxiv
1505;We present finite sample statistical consistency guarantees for Quick Shift on mode and cluster recovery under mild distributional assumptions.;arxiv
1506;We then apply our results to construct a consistent modal regression algorithm.;arxiv
1507;GANs are well known for success in the realistic image generation.;arxiv
1508;However, they can be applied in tabular data generation as well.;arxiv
1509;We will review and examine some recent papers about tabular GANs in action.;arxiv
1510;We will generate data to make train distribution bring closer to the test.;arxiv
1511;Then compare model performance trained on the initial train dataset, with trained on the train with GAN generated data, also we train the model by sampling train by adversarial training.;arxiv
1512;We show that using GAN might be an option in case of uneven data distribution between train and test data.;arxiv
1513;Iconography in art is the discipline that studies the visual content of artworks to determine their motifs and themes andto characterize the way these are represented.;arxiv
1514;It is a subject of active research for a variety of purposes, including the interpretation of meaning, the investigation of the origin and diffusion in time and space of representations, and the study of influences across artists and art works.;arxiv
1515;With the proliferation of digital archives of art images, the possibility arises of applying Computer Vision techniques to the analysis of art images at an unprecedented scale, which may support iconography research and education.;arxiv
1516;In this paper we introduce a novel paintings data set for iconography classification and present the quantitativeand qualitative results of applying a Convolutional Neural Network (CNN) classifier to the recognition of the iconography of artworks.;arxiv
1517;The proposed classifier achieves good performances (71.17% Precision, 70.89% Recall, 70.25% F1-Score and 72.73% Average Precision) in the task of identifying saints in Christian religious paintings, a task made difficult by the presence of classes with very similar visual features.;arxiv
1518;Qualitative analysis of the results shows that the CNN focuses on the traditional iconic motifs that characterize the representation of each saint and exploits such hints to attain correct identification.;arxiv
1519;The ultimate goal of our work is to enable the automatic extraction, decomposition, and comparison of iconography elements to support iconographic studies and automatic art work annotation.;arxiv
1520;Distant supervision (DS) is a strong way to expand the datasets for enhancing relation extraction (RE) models but often suffers from high label noise.;arxiv
1521;Current works based on attention, reinforcement learning, or GAN are black-box models so they neither provide meaningful interpretation of sample selection in DS nor stability on different domains.;arxiv
1522;On the contrary, this work proposes a novel model-agnostic instance sampling method for DS by influence function (IF), namely REIF.;arxiv
1523;Our method identifies favorable/unfavorable instances in the bag based on IF, then does dynamic instance sampling.;arxiv
1524;We design a fast influence sampling algorithm that reduces the computational complexity from $\mathcal{O}(mn)$ to $\mathcal{O}(1)$, with analyzing its robustness on the selected sampling function.;arxiv
1525;Experiments show that by simply sampling the favorable instances during training, REIF is able to win over a series of baselines that have complicated architectures.;arxiv
1526;We also demonstrate that REIF can support interpretable instance selection.;arxiv
1527;A powerful and flexible approach to structured prediction consists in embedding the structured objects to be predicted into a feature space of possibly infinite dimension by means of output kernels, and then, solving a regression problem in this output space.;arxiv
1528;A prediction in the original space is computed by solving a pre-image problem.;arxiv
1529;In such an approach, the embedding, linked to the target loss, is defined prior to the learning phase.;arxiv
1530;In this work, we propose to jointly learn a finite approximation of the output embedding and the regression function into the new feature space.;arxiv
1531;For that purpose, we leverage a priori information on the outputs and also unexploited unsupervised output data, which are both often available in structured prediction problems.;arxiv
1532;We prove that the resulting structured predictor is a consistent estimator, and derive an excess risk bound.;arxiv
1533;Moreover, the novel structured prediction tool enjoys a significantly smaller computational complexity than former output kernel methods.;arxiv
1534;The approach empirically tested on various structured prediction problems reveals to be versatile and able to handle large datasets.;arxiv
1535;Traditional neural language models tend to generate generic replies with poor logic and no emotion.;arxiv
1536;In this paper, a syntactically constrained bidirectional-asynchronous approach for emotional conversation generation (E-SCBA) is proposed to address this issue.;arxiv
1537;In our model, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding.;arxiv
1538;It is much different from most existing methods which generate replies from the first word to the last.;arxiv
1539;Through experiments, the results indicate that our approach not only improves the diversity of replies, but gains a boost on both logic and emotion compared with baselines.;arxiv
1540;The over-parameterized models attract much attention in the era of data science and deep learning.;arxiv
1541;It is empirically observed that although these models, e.g. deep neural networks, over-fit the training data, they can still achieve small testing error, and sometimes even {\em outperform} traditional algorithms which are designed to avoid over-fitting.;arxiv
1542;The major goal of this work is to sharply quantify the benefit of data interpolation in the context of nearest neighbors (NN) algorithm.;arxiv
1543;Specifically, we consider a class of interpolated weighting schemes and then carefully characterize their asymptotic performances.;arxiv
1544;Our analysis reveals a U-shaped performance curve with respect to the level of data interpolation, and proves that a mild degree of data interpolation {\em strictly} improves the prediction accuracy and statistical stability over those of the (un-interpolated) optimal $k$NN algorithm.;arxiv
1545;This theoretically justifies (predicts) the existence of the second U-shaped curve in the recently discovered double descent phenomenon.;arxiv
1546;Note that our goal in this study is not to promote the use of interpolated-NN method, but to obtain theoretical insights on data interpolation inspired by the aforementioned phenomenon.;arxiv
1547;Personalized medicine, a paradigm of medicine tailored to a patient's characteristics, is an increasingly attractive field in health care.;arxiv
1548;An important goal of personalized medicine is to identify a subgroup of patients, based on baseline covariates, that benefits more from the targeted treatment than other comparative treatments.;arxiv
1549;Most of the current subgroup identification methods only focus on obtaining a subgroup with an enhanced treatment effect without paying attention to subgroup size.;arxiv
1550;Yet, a clinically meaningful subgroup learning approach should identify the maximum number of patients who can benefit from the better treatment.;arxiv
1551;In this paper, we present an optimal subgroup selection rule (SSR) that maximizes the number of selected patients, and in the meantime, achieves the pre-specified clinically meaningful mean outcome, such as the average treatment effect.;arxiv
1552;We derive two equivalent theoretical forms of the optimal SSR based on the contrast function that describes the treatment-covariates interaction in the outcome.;arxiv
1553;We further propose a ConstrAined PolIcy Tree seArch aLgorithm (CAPITAL) to find the optimal SSR within the interpretable decision tree class.;arxiv
1554;The proposed method is flexible to handle multiple constraints that penalize the inclusion of patients with negative treatment effects, and to address time to event data using the restricted mean survival time as the clinically interesting mean outcome.;arxiv
1555;Extensive simulations, comparison studies, and real data applications are conducted to demonstrate the validity and utility of our method.;arxiv
1556;Representation learning on graphs has been gaining attention due to its wide applicability in predicting missing links, and classifying and recommending nodes.;arxiv
1557;Most embedding methods aim to preserve certain properties of the original graph in the low dimensional space.;arxiv
1558;However, real world graphs have a combination of several properties which are difficult to characterize and capture by a single approach.;arxiv
1559;In this work, we introduce the problem of graph representation ensemble learning and provide a first of its kind framework to aggregate multiple graph embedding methods efficiently.;arxiv
1560;We provide analysis of our framework and analyze -- theoretically and empirically -- the dependence between state-of-the-art embedding methods.;arxiv
1561;We test our models on the node classification task on four real world graphs and show that proposed ensemble approaches can outperform the state-of-the-art methods by up to 8% on macro-F1.;arxiv
1562;We further show that the approach is even more beneficial for underrepresented classes providing an improvement of up to 12%.;arxiv
1563;We develop a general framework for proving rigorous guarantees on the performance of the EM algorithm and a variant known as gradient EM.;arxiv
1564;Our analysis is divided into two parts: a treatment of these algorithms at the population level (in the limit of infinite data), followed by results that apply to updates based on a finite set of samples.;arxiv
1565;First, we characterize the domain of attraction of any global maximizer of the population likelihood.;arxiv
1566;This characterization is based on a novel view of the EM updates as a perturbed form of likelihood ascent, or in parallel, of the gradient EM updates as a perturbed form of standard gradient ascent.;arxiv
1567;Leveraging this characterization, we then provide non-asymptotic guarantees on the EM and gradient EM algorithms when applied to a finite set of samples.;arxiv
1568;We develop consequences of our general theory for three canonical examples of incomplete-data problems: mixture of Gaussians, mixture of regressions, and linear regression with covariates missing completely at random.;arxiv
1569;In each case, our theory guarantees that with a suitable initialization, a relatively small number of EM (or gradient EM) steps will yield (with high probability) an estimate that is within statistical error of the MLE.;arxiv
1570;We provide simulations to confirm this theoretically predicted behavior.;arxiv
1571;Spiking neural networks (SNNs) have gained considerable interest due to their energy-efficient characteristics, yet lack of a scalable training algorithm has restricted their applicability in practical machine learning problems.;arxiv
1572;The deep neural network-to-SNN conversion approach has been widely studied to broaden the applicability of SNNs.;arxiv
1573;Most previous studies, however, have not fully utilized spatio-temporal aspects of SNNs, which has led to inefficiency in terms of number of spikes and inference latency.;arxiv
1574;In this paper, we present T2FSNN, which introduces the concept of time-to-first-spike coding into deep SNNs using the kernel-based dynamic threshold and dendrite to overcome the aforementioned drawback.;arxiv
1575;In addition, we propose gradient-based optimization and early firing methods to further increase the efficiency of the T2FSNN.;arxiv
1576;According to our results, the proposed methods can reduce inference latency and number of spikes to 22% and less than 1%, compared to those of burst coding, which is the state-of-the-art result on the CIFAR-100.;arxiv
1577;Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields.;arxiv
1578;However, no existing work has delved deeper to further investigate the main cause of this phenomenon.;arxiv
1579;In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored.;arxiv
1580;Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph.;arxiv
1581;Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models.;arxiv
1582;Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing.;arxiv
1583;To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse.;arxiv
1584;Extensive experiment results on various data sets illustrate the effect of our fusion method.;arxiv
1585;The challenge consists of four question-answering tasks based on radiology images.;arxiv
1586;The diversity of imaging modalities, organs and disease types combined with a small imbalanced training set made this a highly complex problem.;arxiv
1587;To overcome these difficulties, we implemented a modular pipeline architecture that utilized transfer learning and multi-task learning.;arxiv
1588;Our findings led to the development of a novel model called Supporting Facts Network (SFN).;arxiv
1589;The main idea behind SFN is to cross-utilize information from upstream tasks to improve the accuracy on harder downstream ones.;arxiv
1590;This approach significantly improved the scores achieved in the validation set (18 point improvement in F-1 score).;arxiv
1591;Finally, we submitted four runs to the competition and were ranked seventh.;arxiv
1592;In this paper, we propose a method to distinguish spurious and genuine correlations in text classification.;arxiv
1593;"We treat this as a supervised classification problem, using features derived from treatment effect estimators to distinguish spurious correlations from ""genuine"" ones.";arxiv
1594;Due to the generic nature of these features and their small dimensionality, we find that the approach works well even with limited training examples, and that it is possible to transport the word classifier to new domains.;arxiv
1595;We propose a novel solution for the Register Allocation problem, leveraging multi-agent hierarchical Reinforcement Learning.;arxiv
1596;We formalize the constraints that precisely define the problem for a given instruction-set architecture, while ensuring that the generated code preserves semantic correctness.;arxiv
1597;We also develop a gRPC based framework providing a modular and efficient compiler interface for training and inference.;arxiv
1598;Experimental results match or outperform the LLVM register allocators, targeting Intel x86 and ARM AArch64.;arxiv
1599;Reinforcement learning (RL) has been widely studied for improving sequence-generation models.;arxiv
1600;However, the conventional rewards used for RL training typically cannot capture sufficient semantic information and therefore render model bias.;arxiv
1601;Further, the sparse and delayed rewards make RL exploration inefficient.;arxiv
1602;To alleviate these issues, we propose the concept of nested-Wasserstein distance for distributional semantic matching.;arxiv
1603;To further exploit it, a novel nested-Wasserstein self-imitation learning framework is developed, encouraging the model to exploit historical high-rewarded sequences for enhanced exploration and better semantic matching.;arxiv
1604;Our solution can be understood as approximately executing proximal policy optimization with Wasserstein trust-regions.;arxiv
1605;Experiments on a variety of unconditional and conditional sequence-generation tasks demonstrate the proposed approach consistently leads to improved performance.;arxiv
1606;A deep-learning-based hybrid strategy for short-term load forecasting is presented.;arxiv
1607;The strategy proposes a novel tree-based ensemble method Warm-start Gradient Tree Boosting (WGTB).;arxiv
1608;Current strategies either ensemble submodels of a single type, which fail to take advantage of the statistical strengths of different inference models.;arxiv
1609;Or they simply sum the outputs from completely different inference models, which doesn't maximize the potential of ensemble.;arxiv
1610;Inspired by the bias-variance trade-off, WGTB is proposed and tailored to the great disparity among different inference models on accuracy, volatility and linearity.;arxiv
1611;The complete strategy integrates four different inference models of different capacities.;arxiv
1612;WGTB then ensembles their outputs by a warm-start and a hybrid of bagging and boosting, which lowers bias and variance concurrently.;arxiv
1613;It is validated on two real datasets from State Grid Corporation of China of hourly resolution.;arxiv
1614;The result demonstrates the effectiveness of the proposed strategy that hybridizes the statistical strengths of both low-bias and low-variance inference models.;arxiv
1615;DeepTingle is a text prediction and classification system trained on the collected works of the renowned fantastic gay erotica author Chuck Tingle.;arxiv
1616;"Whereas the writing assistance tools you use everyday (in the form of predictive text, translation, grammar checking and so on) are trained on generic, purportedly ""neutral"" datasets, DeepTingle is trained on a very specific, internally consistent but externally arguably eccentric dataset.";arxiv
1617;This allows us to foreground and confront the norms embedded in data-driven creativity and productivity assistance tools.;arxiv
1618;As such tools effectively function as extensions of our cognition into technology, it is important to identify the norms they embed within themselves and, by extension, us.;arxiv
1619;DeepTingle is realized as a web application based on LSTM networks and the GloVe word embedding, implemented in JavaScript with Keras-JS.;arxiv
1620;Neural networks used for multi-interaction trajectory reconstruction lack the ability to estimate the uncertainty in their outputs, which would be useful to better analyse and understand the systems they model.;arxiv
1621;In this paper we extend the Factorised Neural Relational Inference model to output both a mean and a standard deviation for each component of the phase space vector, which together with an appropriate loss function, can account for uncertainty.;arxiv
1622;A variety of loss functions are investigated including ideas from convexification and a Bayesian treatment of the problem.;arxiv
1623;We show that the physical meaning of the variables is important when considering the uncertainty and demonstrate the existence of pathological local minima that are difficult to avoid during training.;arxiv
1624;Every natural text is written in some style.;arxiv
1625;One cannot form a complete understanding of a text without considering these factors.;arxiv
1626;The factors combine and co-vary in complex ways to form styles.;arxiv
1627;Studying the nature of the co-varying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding.;arxiv
1628;The benchmark contains text in 15 different styles under the proposed four theoretical groupings: figurative, personal, affective, and interpersonal groups.;arxiv
1629;For valid evaluation, we collect an additional diagnostic set by annotating all 15 styles on the same text.;arxiv
1630;Using xSLUE, we propose three interesting cross-style applications in classification, correlation, and generation.;arxiv
1631;First, our proposed cross-style classifier trained with multiple styles together helps improve overall classification performance against individually-trained style classifiers.;arxiv
1632;Second, our study shows that some styles are highly dependent on each other in human-written text.;arxiv
1633;Finally, we find that combinations of some contradictive styles likely generate stylistically less appropriate text.;arxiv
1634;We believe our benchmark and case studies help explore interesting future directions for cross-style research.;arxiv
1635;The preprocessed datasets and code are publicly available.;arxiv
1636;Deep learning has become the mainstream methodological choice for analyzing and interpreting whole-slide digital pathology images (WSIs).;arxiv
1637;It is commonly assumed that tumor regions carry most predictive information.;arxiv
1638;In this paper, we proposed an unsupervised clustering-based multiple-instance learning, and apply our method to develop deep-learning models for prediction of gene mutations using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies (CRC, LUAD, and HNSCC).;arxiv
1639;We showed that unsupervised clustering of image patches could help identify predictive patches, exclude patches lack of predictive information, and therefore improve prediction on gene mutations in all three different cancer types, compared with the WSI based method without selection of image patches and models based on only tumor regions.;arxiv
1640;Additionally, our proposed algorithm outperformed two recently published baseline algorithms leveraging unsupervised clustering to assist model prediction.;arxiv
1641;The unsupervised-clustering-based approach for mutation prediction allows identification of the spatial regions related to mutation of a specific gene via the resolved probability scores, highlighting the heterogeneity of a predicted genotype in the tumor microenvironment.;arxiv
1642;Finally, our study also demonstrated that selection of tumor regions of WSIs is not always the best way to identify patches for prediction of gene mutations, and other tissue types in the tumor micro-environment may provide better prediction ability for gene mutations than tumor tissues.;arxiv
1643;Given a gallery of uncaptioned video sequences, this paper considers the task of retrieving videos based on their relevance to an unseen text query.;arxiv
1644;To compensate for the lack of annotations, we rely instead on a related video gallery composed of video-caption pairs, termed the source gallery, albeit with a domain gap between its videos and those in the target gallery.;arxiv
1645;We thus introduce the problem of Unsupervised Domain Adaptation for Cross-modal Video Retrieval, along with a new benchmark on fine-grained actions.;arxiv
1646;Our approach adapts the embedding space to the target gallery, consistently outperforming source-only as well as marginal and conditional alignment methods.;arxiv
1647;An increasing number of machine learning models have been deployed in domains with high stakes such as finance and healthcare.;arxiv
1648;Despite their superior performances, many models are black boxes in nature which are hard to explain.;arxiv
1649;There are growing efforts for researchers to develop methods to interpret these black-box models.;arxiv
1650;Post hoc explanations based on perturbations, such as LIME, are widely used approaches to interpret a machine learning model after it has been built.;arxiv
1651;This class of methods has been shown to exhibit large instability, posing serious challenges to the effectiveness of the method itself and harming user trust.;arxiv
1652;In this paper, we propose S-LIME, which utilizes a hypothesis testing framework based on central limit theorem for determining the number of perturbation points needed to guarantee stability of the resulting explanation.;arxiv
1653;Experiments on both simulated and real world data sets are provided to demonstrate the effectiveness of our method.;arxiv
1654;"We propose a decentralised ""local2global"" approach to graph representation learning, that one can a-priori use to scale any embedding technique.";arxiv
1655;"Our local2global approach proceeds by first dividing the input graph into overlapping subgraphs (or ""patches"") and training local representations for each patch independently.";arxiv
1656;In a second step, we combine the local representations into a globally consistent representation by estimating the set of rigid motions that best align the local representations using information from the patch overlaps, via group synchronization.;arxiv
1657;A key distinguishing feature of local2global relative to existing work is that patches are trained independently without the need for the often costly parameter synchronisation during distributed training.;arxiv
1658;This allows local2global to scale to large-scale industrial applications, where the input graph may not even fit into memory and may be stored in a distributed manner.;arxiv
1659;Preliminary results on medium-scale data sets (up to $\sim$7K nodes and $\sim$200K edges) are promising, with a graph reconstruction performance for local2global that is comparable to that of globally trained embeddings.;arxiv
1660;A thorough evaluation of local2global on large scale data and applications to downstream tasks, such as node classification and link prediction, constitutes ongoing work.;arxiv
1661;The accuracy of deep neural networks is degraded when the distribution of features in the test environment (target domain) differs from that of the training (source) environment.;arxiv
1662;To mitigate the degradation, test-time adaptation (TTA), where a model adapts to the target domain without access to the source dataset, can be used in the test environment.;arxiv
1663;However, the existing TTA methods lack feature distribution alignment between the source and target domains, which unsupervised domain adaptation mainly addresses, because accessing the source dataset is prohibited in the TTA setting.;arxiv
1664;In this paper, we propose a novel TTA method, named Covariance-Aware Feature alignment (CAFe), which explicitly aligns the source and target feature distributions at test time.;arxiv
1665;To perform alignment without accessing the source data, CAFe uses auxiliary feature statistics (mean and covariance) pre-computed on the source domain, which are lightweight and easily prepared.;arxiv
1666;Further, to improve efficiency and stability, we propose feature grouping, which splits the feature dimensions into groups according to their correlations by using spectral clustering to avoid degeneration of the covariance matrix.;arxiv
1667;We empirically show that CAFe outperforms prior TTA methods on a variety of distribution shifts.;arxiv
1668;In this expository note we describe a surprising phenomenon in overparameterized linear regression, where the dimension exceeds the number of samples: there is a regime where the test risk of the estimator found by gradient descent increases with additional samples.;arxiv
1669;In other words, more data actually hurts the estimator.;arxiv
1670;"This behavior is implicit in a recent line of theoretical works analyzing ""double-descent"" phenomenon in linear models.";arxiv
1671;In this note, we isolate and understand this behavior in an extremely simple setting: linear regression with isotropic Gaussian covariates.;arxiv
1672;In particular, this occurs due to an unconventional type of bias-variance tradeoff in the overparameterized regime: the bias decreases with more samples, but variance increases.;arxiv
1673;Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence.;arxiv
1674;However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations.;arxiv
1675;These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems.;arxiv
1676;In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics.;arxiv
1677;We achieve comparable accuracy on the integration task with our pretrained model while using around $1.5$ orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics.;arxiv
1678;The test accuracy on differential equation tasks is considerably lower comparing with integration as they need higher order recursions that are not present in language translations.;arxiv
1679;We pretrain our model with different pairs of language translations.;arxiv
1680;Our results show language bias in solving symbolic mathematics tasks.;arxiv
1681;Finally, we study the robustness of the fine-tuned model on symbolic math tasks against distribution shift, and our approach generalizes better in distribution shift scenarios for the function integration.;arxiv
1682;The diagnosis process of colorectal cancer mainly focuses on the localization and characterization of abnormal growths in the colon tissue known as polyps.;arxiv
1683;Despite recent advances in deep object localization, the localization of polyps remains challenging due to the similarities between tissues, and the high level of artifacts.;arxiv
1684;Recent studies have shown the negative impact of the presence of artifacts in the polyp detection task, and have started to take them into account within the training process.;arxiv
1685;However, the use of prior knowledge related to the spatial interaction of polyps and artifacts has not yet been considered.;arxiv
1686;In this work, we incorporate artifact knowledge in a post-processing step.;arxiv
1687;Our method models this task as an inductive graph representation learning problem, and is composed of training and inference steps.;arxiv
1688;Detected bounding boxes around polyps and artifacts are considered as nodes connected by a defined criterion.;arxiv
1689;The training step generates a node classifier with ground truth bounding boxes.;arxiv
1690;In inference, we use this classifier to analyze a second graph, generated from artifact and polyp predictions given by region proposal networks.;arxiv
1691;We evaluate how the choices in the connectivity and artifacts affect the performance of our method and show that it has the potential to reduce the false positives in the results of a region proposal network.;arxiv
1692;This leads to erroneous user preference estimates, namely, overestimation of over-presented content while violating the right to be presented of each alternative, contrary of which we define as a fair system.;arxiv
1693;We consider two models that explicitly incorporate, or ignore the systematic and limited exposure to alternatives.;arxiv
1694;By simulations, we demonstrate that ignoring the systematic presentations overestimates promoted options and underestimates censored alternatives.;arxiv
1695;Simply conditioning on the limited exposure is a remedy for these biases.;arxiv
1696;We investigate the problem of truth discovery based on opinions from multiple agents who may be unreliable or biased.;arxiv
1697;We consider the case where agents' reliabilities or biases are correlated if they belong to the same community, which defines a group of agents with similar opinions regarding a particular event.;arxiv
1698;We incorporate knowledge of the agents' social network in our truth discovery framework and develop Laplace variational inference methods to estimate agents' reliabilities, communities, and the event states.;arxiv
1699;We also develop a stochastic variational inference method to scale our model to large social networks.;arxiv
1700;Simulations and experiments on real data suggest that when observations are sparse, our proposed methods perform better than several other inference methods, including majority voting, TruthFinder, AccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier Combination (BCC) method, and the Community BCC method.;arxiv
1701;Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and popular variants of MCMC for problems when an MCMC state consists of an unknown number of components.;arxiv
1702;It is well known that state-of-the-art methods for split-merge MCMC do not scale well.;arxiv
1703;Strategies for rapid mixing requires smart and informative proposals to reduce the rejection rate.;arxiv
1704;However, all known smart proposals involve expensive operations to suggest informative transitions.;arxiv
1705;As a result, the cost of each iteration is prohibitive for massive scale datasets.;arxiv
1706;It is further known that uninformative but computationally efficient proposals, such as random split-merge, leads to extremely slow convergence.;arxiv
1707;This tradeoff between mixing time and per update cost seems hard to get around.;arxiv
1708;In this paper, we show a sweet spot.;arxiv
1709;We leverage some unique properties of weighted MinHash, which is a popular LSH, to design a novel class of split-merge proposals which are significantly more informative than random sampling but at the same time efficient to compute.;arxiv
1710;Overall, we obtain a superior tradeoff between convergence and per update cost.;arxiv
1711;As a direct consequence, our proposals are around 6X faster than the state-of-the-art sampling methods on two large real datasets KDDCUP and PubMed with several millions of entities and thousands of clusters.;arxiv
1712;We present a CLSRIL-23, a self supervised learning based audio pre-trained model which learns cross lingual speech representations from raw audio across 23 Indic languages.;arxiv
1713;It is built on top of wav2vec 2.0 which is solved by training a contrastive task over masked latent speech representations and jointly learns the quantization of latents shared across all languages.;arxiv
1714;We compare the language wise loss during pretraining to compare effects of monolingual and multilingual pretraining.;arxiv
1715;Performance on some downstream fine-tuning tasks for speech recognition is also compared and our experiments show that multilingual pretraining outperforms monolingual training, in terms of learning speech representations which encodes phonetic similarity of languages and also in terms of performance on down stream tasks.;arxiv
1716;A decrease of 5% is observed in WER and 9.5% in CER when a multilingual pretrained model is used for finetuning in Hindi.;arxiv
1717;All the code models are also open sourced.;arxiv
1718;CLSRIL-23 is a model trained on $23$ languages and almost 10,000 hours of audio data to facilitate research in speech recognition for Indic languages.;arxiv
1719;We hope that new state of the art systems will be created using the self supervised approach, especially for low resources Indic languages.;arxiv
1720;Label space expansion for multi-label classification (MLC) is a methodology that encodes the original label vectors to higher dimensional codes before training and decodes the predicted codes back to the label vectors during testing.;arxiv
1721;The methodology has been demonstrated to improve the performance of MLC algorithms when coupled with off-the-shelf error-correcting codes for encoding and decoding.;arxiv
1722;Nevertheless, such a coding scheme can be complicated to implement, and cannot easily satisfy a common application need of cost-sensitive MLC---adapting to different evaluation criteria of interest.;arxiv
1723;In this work, we show that a simpler coding scheme based on the concept of a reference pair of label vectors achieves cost-sensitivity more naturally.;arxiv
1724;In particular, our proposed cost-sensitive reference pair encoding (CSRPE) algorithm contains cluster-based encoding, weight-based training and voting-based decoding steps, all utilizing the cost information.;arxiv
1725;Furthermore, we leverage the cost information embedded in the code space of CSRPE to propose a novel active learning algorithm for cost-sensitive MLC.;arxiv
1726;Extensive experimental results verify that CSRPE performs better than state-of-the-art algorithms across different MLC criteria.;arxiv
1727;The results also demonstrate that the CSRPE-backed active learning algorithm is superior to existing algorithms for active MLC, and further justify the usefulness of CSRPE.;arxiv
1728;Sparse R-CNN is a recent strong object detection baseline by set prediction on sparse, learnable proposal boxes and proposal features.;arxiv
1729;In this work, we propose to improve Sparse R-CNN with two dynamic designs.;arxiv
1730;First, Sparse R-CNN adopts a one-to-one label assignment scheme, where the Hungarian algorithm is applied to match only one positive sample for each ground truth.;arxiv
1731;Such one-to-one assignment may not be optimal for the matching between the learned proposal boxes and ground truths.;arxiv
1732;To address this problem, we propose dynamic label assignment (DLA) based on the optimal transport algorithm to assign increasing positive samples in the iterative training stages of Sparse R-CNN.;arxiv
1733;We constrain the matching to be gradually looser in the sequential stages as the later stage produces the refined proposals with improved precision.;arxiv
1734;Second, the learned proposal boxes and features remain fixed for different images in the inference process of Sparse R-CNN.;arxiv
1735;Motivated by dynamic convolution, we propose dynamic proposal generation (DPG) to assemble multiple proposal experts dynamically for providing better initial proposal boxes and features for the consecutive training stages.;arxiv
1736;DPG thereby can derive sample-dependent proposal boxes and features for inference.;arxiv
1737;Experiments demonstrate that our method, named Dynamic Sparse R-CNN, can boost the strong Sparse R-CNN baseline with different backbones for object detection.;arxiv
1738;Particularly, Dynamic Sparse R-CNN reaches the state-of-the-art 47.2% AP on the COCO 2017 validation set, surpassing Sparse R-CNN by 2.2% AP with the same ResNet-50 backbone.;arxiv
1739;End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding.;arxiv
1740;However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs.;arxiv
1741;Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs.;arxiv
1742;In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning.;arxiv
1743;Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance.;arxiv
1744;Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.;arxiv
1745;We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations.;arxiv
1746;The representations change significantly when translation and part-of-speech decoders are added.;arxiv
1747;The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled.;arxiv
1748;We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure.;arxiv
1749;Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way.;arxiv
1750;Hyperbolic-spaces are better suited to represent data with underlying hierarchical relationships, e.g., tree-like data.;arxiv
1751;However, it is often necessary to incorporate, through alignment, different but related representations meaningfully.;arxiv
1752;This aligning is an important class of machine learning problems, with applications as ontology matching and cross-lingual alignment.;arxiv
1753;Optimal transport (OT)-based approaches are a natural choice to tackle the alignment problem as they aim to find a transformation of the source dataset to match a target dataset, subject to some distribution constraints.;arxiv
1754;This work proposes a novel approach based on OT of embeddings on the Poincar\'e model of hyperbolic spaces.;arxiv
1755;"Our method relies on the gyrobarycenter mapping on M\""obius gyrovector spaces.";arxiv
1756;As a result of this formalism, we derive extensions to some existing Euclidean methods of OT-based domain adaptation to their hyperbolic counterparts.;arxiv
1757;Empirically, we show that both Euclidean and hyperbolic methods have similar performances in the context of retrieval.;arxiv
1758;Traditionally, designers of machine learning models have relied predominantly either on Structure or Context.;arxiv
1759;We propose a new model, which jointly learns on Context and Structure of source code.;arxiv
1760;Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model.;arxiv
1761;We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages.;arxiv
1762;Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.;arxiv
1763;Meta-learning aims at optimizing the hyperparameters of a model class or training algorithm from the observation of data from a number of related tasks.;arxiv
1764;The statistical properties of the task environment thus dictate the similarity of the tasks.;arxiv
1765;The goal of the meta-learner is to ensure that the hyperparameters obtain a small loss when applied for training of a new task sampled from the task environment.;arxiv
1766;The difference between the resulting average loss, known as meta-population loss, and the corresponding empirical loss measured on the available data from related tasks, known as meta-generalization gap, is a measure of the generalization capability of the meta-learner.;arxiv
1767;In this paper, we present novel information-theoretic bounds on the average absolute value of the meta-generalization gap.;arxiv
1768;Task similarity is gauged via the Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences.;arxiv
1769;We illustrate the proposed bounds on the example of ridge regression with meta-learned bias.;arxiv
1770;Person Re-Identification (Re-ID) is of great importance to the many video surveillance systems.;arxiv
1771;Learning discriminative features for Re-ID remains a challenge due to the large variations in the image space, e.g., continuously changing human poses, illuminations and point of views.;arxiv
1772;In this paper, we propose HAVANA, a novel extensible, light-weight HierArchical and VAriation-Normalized Autoencoder that learns features robust to intra-class variations.;arxiv
1773;In contrast to existing generative approaches that prune the variations with heavy extra supervised signals, HAVANA suppresses the intra-class variations with a Variation-Normalized Autoencoder trained with no additional supervision.;arxiv
1774;We also introduce a novel Jensen-Shannon triplet loss for contrastive distribution learning in Re-ID.;arxiv
1775;In addition, we present Hierarchical Variation Distiller, a hierarchical VAE to factorize the latent representation and explicitly model the variations.;arxiv
1776;To the best of our knowledge, HAVANA is the first VAE-based framework for person ReID.;arxiv
1777;The efficiency of deep machine learning for automatic delineation of tumor areas has been demonstrated for intraoperative neuronavigation using active IR-mapping with the use of the cold test.;arxiv
1778;The proposed approach employs a matrix IR-imager to remotely register the space-time distribution of surface temperature pattern, which is determined by the dynamics of local cerebral blood flow.;arxiv
1779;The advantages of this technique are non-invasiveness, zero risks for the health of patients and medical staff, low implementation and operational costs, ease and speed of use.;arxiv
1780;Traditional IR-diagnostic technique has a crucial limitation - it involves a diagnostician who determines the boundaries of tumor areas, which gives rise to considerable uncertainty, which can lead to diagnosis errors that are difficult to control.;arxiv
1781;The current study demonstrates that implementing deep learning algorithms allows to eliminate the explained drawback.;arxiv
1782;Transfer learning is a promising approach to creating predictive models that incorporate simulation and experimental data into a common framework.;arxiv
1783;In this technique, a neural network is first trained on a large database of simulations, then partially retrained on sparse sets of experimental data to adjust predictions to be more consistent with reality.;arxiv
1784;Previously, this technique has been used to create predictive models of Omega and NIF inertial confinement fusion (ICF) experiments that are more accurate than simulations alone.;arxiv
1785;In this work, we conduct a transfer learning driven hypothetical ICF campaign in which the goal is to maximize experimental neutron yield via Bayesian optimization.;arxiv
1786;The transfer learning model achieves yields within 5% of the maximum achievable yield in a modest-sized design space in fewer than 20 experiments.;arxiv
1787;Furthermore, we demonstrate that this method is more efficient at optimizing designs than traditional model calibration techniques commonly employed in ICF design.;arxiv
1788;Such an approach to ICF design could enable robust optimization of experimental performance under uncertainty.;arxiv
1789;Deep learning (DL) is the state-of-the-art methodology in various medical image segmentation tasks.;arxiv
1790;However, it requires relatively large amounts of manually labeled training data, which may be infeasible to generate in some applications.;arxiv
1791;In addition, DL methods have relatively poor generalizability to out-of-sample data.;arxiv
1792;Multi-atlas segmentation (MAS), on the other hand, has promising performance using limited amounts of training data and good generalizability.;arxiv
1793;A hybrid method that integrates the high accuracy of DL and good generalizability of MAS is highly desired and could play an important role in segmentation problems where manually labeled data is hard to generate.;arxiv
1794;Most of the prior work focuses on improving single components of MAS using DL rather than directly optimizing the final segmentation accuracy via an end-to-end pipeline.;arxiv
1795;Only one study explored this idea in binary segmentation of 2D images, but it remains unknown whether it generalizes well to multi-class 3D segmentation problems.;arxiv
1796;In this study, we propose a 3D end-to-end hybrid pipeline, named deep label fusion (DLF), that takes advantage of the strengths of MAS and DL.;arxiv
1797;Experimental results demonstrate that DLF yields significant improvements over conventional label fusion methods and U-Net, a direct DL approach, in the context of segmenting medial temporal lobe subregions using 3T T1-weighted and T2-weighted MRI.;arxiv
1798;Further, when applied to an unseen similar dataset acquired in 7T, DLF maintains its superior performance, which demonstrates its good generalizability.;arxiv
1799;The Jacobian matrix (or the gradient for single-output networks) is directly related to many important properties of neural networks, such as the function landscape, stationary points, (local) Lipschitz constants and robustness to adversarial attacks.;arxiv
1800;In this paper, we propose a recursive algorithm, RecurJac, to compute both upper and lower bounds for each element in the Jacobian matrix of a neural network with respect to network's input, and the network can contain a wide range of activation functions.;arxiv
1801;As a byproduct, we can efficiently obtain a (local) Lipschitz constant, which plays a crucial role in neural network robustness verification, as well as the training stability of GANs.;arxiv
1802;Experiments show that (local) Lipschitz constants produced by our method is of better quality than previous approaches, thus providing better robustness verification results.;arxiv
1803;Our algorithm has polynomial time complexity, and its computation time is reasonable even for relatively large networks.;arxiv
1804;Additionally, we use our bounds of Jacobian matrix to characterize the landscape of the neural network, for example, to determine whether there exist stationary points in a local neighborhood.;arxiv
1805;Graph classification is a widely studied problem and has broad applications.;arxiv
1806;In many real-world problems, the number of labeled graphs available for training classification models is limited, which renders these models prone to overfitting.;arxiv
1807;To address this problem, we propose two approaches based on contrastive self-supervised learning (CSSL) to alleviate overfitting.;arxiv
1808;In the first approach, we use CSSL to pretrain graph encoders on widely-available unlabeled graphs without relying on human-provided labels, then finetune the pretrained encoders on labeled graphs.;arxiv
1809;In the second approach, we develop a regularizer based on CSSL, and solve the supervised classification task and the unsupervised CSSL task simultaneously.;arxiv
1810;To perform CSSL on graphs, given a collection of original graphs, we perform data augmentation to create augmented graphs out of the original graphs.;arxiv
1811;An augmented graph is created by consecutively applying a sequence of graph alteration operations.;arxiv
1812;A contrastive loss is defined to learn graph encoders by judging whether two augmented graphs are from the same original graph.;arxiv
1813;Experiments on various graph classification datasets demonstrate the effectiveness of our proposed methods.;arxiv
1814;Knowledge tracing (KT) models, e.g., the deep knowledge tracing (DKT) model, track an individual learner's acquisition of skills over time by examining the learner's performance on questions related to those skills.;arxiv
1815;A practical limitation in most existing KT models is that all questions nested under a particular skill are treated as equivalent observations of a learner's ability, which is an inaccurate assumption in real-world educational scenarios.;arxiv
1816;To overcome this limitation we introduce qDKT, a variant of DKT that models every learner's success probability on individual questions over time.;arxiv
1817;First, qDKT incorporates graph Laplacian regularization to smooth predictions under each skill, which is particularly useful when the number of questions in the dataset is big.;arxiv
1818;Second, qDKT uses an initialization scheme inspired by the fastText algorithm, which has found success in a variety of language modeling tasks.;arxiv
1819;Our experiments on several real-world datasets show that qDKT achieves state-of-art performance on predicting learner outcomes.;arxiv
1820;Because of this, qDKT can serve as a simple, yet tough-to-beat, baseline for new question-centric KT models.;arxiv
1821;Mobile and embedded applications require neural networks-based pattern recognition systems to perform well under a tight computational budget.;arxiv
1822;In contrast to commonly used synchronous, frame-based vision systems and CNNs, asynchronous, spiking neural networks driven by event-based visual input respond with low latency to sparse, salient features in the input, leading to high efficiency at run-time.;arxiv
1823;The discrete nature of the event-based data streams makes direct training of asynchronous neural networks challenging.;arxiv
1824;This paper studies asynchronous spiking neural networks, obtained by conversion from a conventional CNN trained on frame-based data.;arxiv
1825;As an example, we consider a CNN trained to steer a robot to follow a moving target.;arxiv
1826;We identify possible pitfalls of the conversion and demonstrate how the proposed solutions bring the classification accuracy of the asynchronous network to only 3\% below the performance of the original synchronous CNN, while requiring 12x fewer computations.;arxiv
1827;While being applied to a simple task, this work is an important step towards low-power, fast, and embedded neural networks-based vision solutions for robotic applications.;arxiv
1828;In this paper, we systematically investigate the above assumption in several NLP tasks.;arxiv
1829;We show, both theoretically and experimentally, that some popular designs of the sample-level loss $G$ may be inconsistent with the true population-level metric $F$ of the task, so that models trained to optimize the former can be substantially sub-optimal to the latter, a phenomenon we call it, Simpson's bias, due to its deep connections with the classic paradox known as Simpson's reversal paradox in statistics and social sciences.;arxiv
1830;Protein engineering seeks to identify protein sequences with optimized properties.;arxiv
1831;When guided by machine learning, protein sequence generation methods can draw on prior knowledge and experimental efforts to improve this process.;arxiv
1832;In this review, we highlight recent applications of machine learning to generate protein sequences, focusing on the emerging field of deep generative methods.;arxiv
1833;A new message-passing (MP) method is considered for the matrix completion problem associated with recommender systems.;arxiv
1834;We attack the problem using a (generative) factor graph model that is related to a probabilistic low-rank matrix factorization.;arxiv
1835;Based on the model, we propose a new algorithm, termed IMP, for the recovery of a data matrix from incomplete observations.;arxiv
1836;The algorithm is based on a clustering followed by inference via MP (IMP).;arxiv
1837;The algorithm is compared with a number of other matrix completion algorithms on real collaborative filtering (e.g., Netflix) data matrices.;arxiv
1838;Our results show that, while many methods perform similarly with a large number of revealed entries, the IMP algorithm outperforms all others when the fraction of observed entries is small.;arxiv
1839;This is helpful because it reduces the well-known cold-start problem associated with collaborative filtering (CF) systems in practice.;arxiv
1840;In this paper, we consider the problem of large scale multi agent reinforcement learning.;arxiv
1841;Firstly, we studied the representation problem of the pairwise value function to reduce the complexity of the interactions among agents.;arxiv
1842;Secondly, we adopt a l2-norm trick to ensure the trivial term of the approximated value function is bounded.;arxiv
1843;Thirdly, experimental results on battle game demonstrate the effectiveness of the proposed approach.;arxiv
1844;Recent advances in deep learning have achieved promising performance for medical image analysis, while in most cases ground-truth annotations from human experts are necessary to train the deep model.;arxiv
1845;In practice, such annotations are expensive to collect and can be scarce for medical imaging applications.;arxiv
1846;Therefore, there is significant interest in learning representations from unlabelled raw data.;arxiv
1847;In this paper, we propose a self-supervised learning approach to learn meaningful and transferable representations from medical imaging video without any type of human annotation.;arxiv
1848;We assume that in order to learn such a representation, the model should identify anatomical structures from the unlabelled data.;arxiv
1849;Therefore we force the model to address anatomy-aware tasks with free supervision from the data itself.;arxiv
1850;Specifically, the model is designed to correct the order of a reshuffled video clip and at the same time predict the geometric transformation applied to the video clip.;arxiv
1851;Experiments on fetal ultrasound video show that the proposed approach can effectively learn meaningful and strong representations, which transfer well to downstream tasks like standard plane detection and saliency prediction.;arxiv
1852;The automatic digitizing of paper maps is a significant and challenging task for both academia and industry.;arxiv
1853;As an important procedure of map digitizing, the semantic segmentation section mainly relies on manual visual interpretation with low efficiency.;arxiv
1854;In this study, we select urban planning maps as a representative sample and investigate the feasibility of utilizing U-shape fully convolutional based architecture to perform end-to-end map semantic segmentation.;arxiv
1855;The experimental results obtained from the test area in Shibuya district, Tokyo, demonstrate that our proposed method could achieve a very high Jaccard similarity coefficient of 93.63% and an overall accuracy of 99.36%.;arxiv
1856;For implementation on GPGPU and cuDNN, the required processing time for the whole Shibuya district can be less than three minutes.;arxiv
1857;The results indicate the proposed method can serve as a viable tool for urban planning map semantic segmentation task with high accuracy and efficiency.;arxiv
1858;This paper mainly discusses the generation of personalized fonts as the problem of image style transfer.;arxiv
1859;The main purpose of this paper is to design a network framework that can extract and recombine the content and style of the characters.;arxiv
1860;These attempts can be used to synthesize the entire set of fonts with only a small amount of characters.;arxiv
1861;The paper combines various depth networks such as Convolutional Neural Network, Multi-layer Perceptron and Residual Network to find the optimal model to extract the features of the fonts character.;arxiv
1862;The result shows that those characters we have generated is very close to real characters, using Structural Similarity index and Peak Signal-to-Noise Ratio evaluation criterions.;arxiv
1863;Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional distributions in Statistics and Machine learning.;arxiv
1864;"HMC is known to run very efficiently in practice and its popular second-order ""leapfrog"" implementation has long been conjectured to run in $d^{1/4}$ gradient evaluations.";arxiv
1865;Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data.;arxiv
1866;Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone.;arxiv
1867;"Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an ""incoherence"" property.";arxiv
1868;Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like $d^{{1}/{2}}$ gradient evaluations with the dimension.;arxiv
1869;Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors -- both in terms of accuracy and in terms of the number of gradient evaluations it requires.;arxiv
1870;Recent attempts at Super-Resolution for medical images used deep learning techniques such as Generative Adversarial Networks (GANs) to achieve perceptually realistic single image Super-Resolution.;arxiv
1871;Yet, they are constrained by their inability to generalise to different scale factors.;arxiv
1872;This involves high storage and energy costs as every integer scale factor involves a separate neural network.;arxiv
1873;A recent paper has proposed a novel meta-learning technique that uses a Weight Prediction Network to enable Super-Resolution on arbitrary scale factors using only a single neural network.;arxiv
1874;In this paper, we propose a new network that combines that technique with SRGAN, a state-of-the-art GAN-based architecture, to achieve arbitrary scale, high fidelity Super-Resolution for medical images.;arxiv
1875;By using this network to perform arbitrary scale magnifications on images from the Multimodal Brain Tumor Segmentation Challenge (BraTS) dataset, we demonstrate that it is able to outperform traditional interpolation methods by up to 20$\%$ on SSIM scores whilst retaining generalisability on brain MRI images.;arxiv
1876;We show that performance across scales is not compromised, and that it is able to achieve competitive results with other state-of-the-art methods such as EDSR whilst being fifty times smaller than them.;arxiv
1877;Combining efficiency, performance, and generalisability, this can hopefully become a new foundation for tackling Super-Resolution on medical images.;arxiv
1878;Check out the webapp here: https://metasrgan.herokuapp.com/ Check out the github tutorial here: https://github.com/pancakewaffles/metasrgan-tutorial;arxiv
1879;An orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions and absolute values, over pairs of coefficients.;arxiv
1880;It provides a simple mathematical model for unsupervised deep network learning.;arxiv
1881;It implements non-linear contractions, which are optimized for classification, with an unsupervised pair matching algorithm, of polynomial complexity.;arxiv
1882;A structured Haar scattering over graph data computes permutation invariant representations of groups of connected points in the graph.;arxiv
1883;If the graph connectivity is unknown, unsupervised Haar pair learning can provide a consistent estimation of connected dyadic groups of points.;arxiv
1884;Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown.;arxiv
1885;Normalization methods are a central building block in the deep learning toolbox.;arxiv
1886;They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules.;arxiv
1887;When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced.;arxiv
1888;As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features.;arxiv
1889;We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.;arxiv
1890;Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection.;arxiv
1891;As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model.;arxiv
1892;The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones.;arxiv
1893;This article studies a class of linear screening methods and establishes consistency theory for this special class.;arxiv
1894;In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency.;arxiv
1895;In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of $SIS$.;arxiv
1896;This work proposes PatchNet, an automated tool based on hierarchical deep learning for classifying patches by extracting features from commit messages and code changes.;arxiv
1897;PatchNet contains a deep hierarchical structure that mirrors the hierarchical and sequential structure of a code change, differentiating it from the existing deep learning models on source code.;arxiv
1898;PatchNet provides several options allowing users to select parameters for the training process.;arxiv
1899;The tool has been validated in the context of automatic identification of stable-relevant patches in the Linux kernel and is potentially applicable to automate other software engineering tasks that can be formulated as patch classification problems.;arxiv
1900;A video demonstrating PatchNet is available at https://goo.gl/CZjG6X. The PatchNet implementation is available at https://github.com/hvdthong/PatchNetTool.;arxiv
1901;Approximate inference in complex probabilistic models such as deep Gaussian processes requires the optimisation of doubly stochastic objective functions.;arxiv
1902;These objectives incorporate randomness both from mini-batch subsampling of the data and from Monte Carlo estimation of expectations.;arxiv
1903;If the gradient variance is high, the stochastic optimisation problem becomes difficult with a slow rate of convergence.;arxiv
1904;Control variates can be used to reduce the variance, but past approaches do not take into account how mini-batch stochasticity affects sampling stochasticity, resulting in sub-optimal variance reduction.;arxiv
1905;We propose a new approach in which we use a recognition network to cheaply approximate the optimal control variate for each mini-batch, with no additional model gradient computations.;arxiv
1906;We illustrate the properties of this proposal and test its performance on logistic regression and deep Gaussian processes.;arxiv
1907;Objectives: Present a novel deep learning-based skull stripping algorithm for magnetic resonance imaging (MRI) that works directly in the information rich k-space.;arxiv
1908;Materials and Methods: Using two datasets from different institutions with a total of 36,900 MRI slices, we trained a deep learning-based model to work directly with the complex raw k-space data.;arxiv
1909;Skull stripping performed by HD-BET (Brain Extraction Tool) in the image domain were used as the ground truth.;arxiv
1910;Results: Both datasets were very similar to the ground truth (DICE scores of 92\%-98\% and Hausdorff distances of under 5.5 mm).;arxiv
1911;Results on slices above the eye-region reach DICE scores of up to 99\%, while the accuracy drops in regions around the eyes and below, with partially blurred output.;arxiv
1912;The output of k-strip often smoothed edges at the demarcation to the skull.;arxiv
1913;Binary masks are created with an appropriate threshold.;arxiv
1914;Conclusion: With this proof-of-concept study, we were able to show the feasibility of working in the k-space frequency domain, preserving phase information, with consistent results.;arxiv
1915;Future research should be dedicated to discovering additional ways the k-space can be used for innovative image analysis and further workflows.;arxiv
1916;Traditional recommendation systems are faced with two long-standing obstacles, namely, data sparsity and cold-start problems, which promote the emergence and development of Cross-Domain Recommendation (CDR).;arxiv
1917;The core idea of CDR is to leverage information collected from other domains to alleviate the two problems in one domain.;arxiv
1918;Over the last decade, many efforts have been engaged for cross-domain recommendation.;arxiv
1919;Recently, with the development of deep learning and neural networks, a large number of methods have emerged.;arxiv
1920;However, there is a limited number of systematic surveys on CDR, especially regarding the latest proposed methods as well as the recommendation scenarios and recommendation tasks they address.;arxiv
1921;In this survey paper, we first proposed a two-level taxonomy of cross-domain recommendation which classifies different recommendation scenarios and recommendation tasks.;arxiv
1922;We then introduce and summarize existing cross-domain recommendation approaches under different recommendation scenarios in a structured manner.;arxiv
1923;We conclude this survey by providing several potential research directions about this field.;arxiv
1924;While rule-based attribution methods have proven useful for providing local explanations for Deep Neural Networks, explaining modern and more varied network architectures yields new challenges in generating trustworthy explanations, since the established rule sets might not be sufficient or applicable to novel network structures.;arxiv
1925;As an elegant solution to the above issue, network canonization has recently been introduced.;arxiv
1926;This procedure leverages the implementation-dependency of rule-based attributions and restructures a model into a functionally identical equivalent of alternative design to which established attribution rules can be applied.;arxiv
1927;However, the idea of canonization and its usefulness have so far only been explored qualitatively.;arxiv
1928;In this work, we quantitatively verify the beneficial effects of network canonization to rule-based attributions on VGG-16 and ResNet18 models with BatchNorm layers and thus extend the current best practices for obtaining reliable neural network explanations.;arxiv
1929;While adversarial training and its variants have shown to be the most effective algorithms to defend against adversarial attacks, their extremely slow training process makes it hard to scale to large datasets like ImageNet.;arxiv
1930;The key idea of recent works to accelerate adversarial training is to substitute multi-step attacks (e.g., PGD) with single-step attacks (e.g., FGSM).;arxiv
1931;However, these single-step methods suffer from catastrophic overfitting, where the accuracy against PGD attack suddenly drops to nearly 0% during training, destroying the robustness of the networks.;arxiv
1932;In this work, we study the phenomenon from the perspective of training instances.;arxiv
1933;We show that catastrophic overfitting is instance-dependent and fitting instances with larger gradient norm is more likely to cause catastrophic overfitting.;arxiv
1934;Based on our findings, we propose a simple but effective method, Adversarial Training with Adaptive Step size (ATAS).;arxiv
1935;ATAS learns an instancewise adaptive step size that is inversely proportional to its gradient norm.;arxiv
1936;The theoretical analysis shows that ATAS converges faster than the commonly adopted non-adaptive counterparts.;arxiv
1937;Empirically, ATAS consistently mitigates catastrophic overfitting and achieves higher robust accuracy on CIFAR10, CIFAR100 and ImageNet when evaluated on various adversarial budgets.;arxiv
1938;Importance-weighting is a popular and well-researched technique for dealing with sample selection bias and covariate shift.;arxiv
1939;It has desirable characteristics such as unbiasedness, consistency and low computational complexity.;arxiv
1940;However, weighting can have a detrimental effect on an estimator as well.;arxiv
1941;In this work, we empirically show that the sampling distribution of an importance-weighted estimator can be skewed.;arxiv
1942;These over- and underestimates of the risk lead to suboptimal regularization parameters when used for importance-weighted validation.;arxiv
1943;Constructing reliable prediction sets is an obstacle for applications of neural models: Distribution-free conditional coverage is theoretically impossible, and the exchangeability assumption underpinning the coverage guarantees of standard split-conformal approaches is violated on domain shifts.;arxiv
1944;Given these challenges, we propose and analyze a data-driven procedure for obtaining empirically reliable approximate conditional coverage, calculating unique quantile thresholds for each label for each test point.;arxiv
1945;We achieve this via the strong signals for prediction reliability from KNN-based model approximations over the training set and approximations over constrained samples from the held-out calibration set.;arxiv
1946;Cross-validation (CV) is a technique used to estimate generalization error for prediction models.;arxiv
1947;While theoretically sound, following this recommendation can lead to high computational costs when a pipeline modeling algorithm includes computationally expensive operations, e.g. imputation of missing values.;arxiv
1948;We empirically assessed whether conducting unsupervised imputation prior to CV would result in biased estimates of generalization error or result in poorly selected tuning parameters and thus degrade the external performance of downstream models.;arxiv
1949;Results show that despite optimistic bias, the reduced variance of imputation before CV compared to imputation during each replicate of CV leads to a lower overall root mean squared error for estimation of the true external R-squared and the performance of models tuned using CV with imputation before versus during each replication is minimally different.;arxiv
1950;In conclusion, unsupervised imputation before CV appears valid in certain settings and may be a helpful strategy that enables analysts to use more flexible imputation techniques without incurring high computational costs.;arxiv
1951;In (Franceschi et al., 2018) we proposed a unified mathematical framework, grounded on bilevel programming, that encompasses gradient-based hyperparameter optimization and meta-learning.;arxiv
1952;We formulated an approximate version of the problem where the inner objective is solved iteratively, and gave sufficient conditions ensuring convergence to the exact problem.;arxiv
1953;In this work we show how to optimize learning rates, automatically weight the loss of single examples and learn hyper-representations with Far-HO, a software package based on the popular deep learning framework TensorFlow that allows to seamlessly tackle both HO and ML problems.;arxiv
1954;Images acquired from rainy scenes usually suffer from bad visibility which may damage the performance of computer vision applications.;arxiv
1955;The rainy scenarios can be categorized into two classes: moderate rain and heavy rain scenes.;arxiv
1956;Moderate rain scene mainly consists of rain streaks while heavy rain scene contains both rain streaks and the veiling effect (similar to haze).;arxiv
1957;Although existing methods have achieved excellent performance on these two cases individually, it still lacks a general architecture to address both heavy rain and moderate rain scenarios effectively.;arxiv
1958;In this paper, we construct a hierarchical multi-direction representation network by using the contourlet transform (CT) to address both moderate rain and heavy rain scenarios.;arxiv
1959;The CT divides the image into the multi-direction subbands (MS) and the semantic subband (SS).;arxiv
1960;First, the rain streak information is retrieved to the MS based on the multi-orientation property of the CT.;arxiv
1961;Second, a hierarchical architecture is proposed to reconstruct the background information including damaged semantic information and the veiling effect in the SS.;arxiv
1962;Last, the multi-level subband discriminator with the feedback error map is proposed.;arxiv
1963;By this module, all subbands can be well optimized.;arxiv
1964;This is the first architecture that can address both of the two scenarios effectively.;arxiv
1965;Event detection is a critical task for timely decision-making in graph analytics applications.;arxiv
1966;Despite the recent progress towards deep learning on graphs, event detection on dynamic graphs presents particular challenges to existing architectures.;arxiv
1967;Real-life events are often associated with sudden deviations of the normal behavior of the graph.;arxiv
1968;However, existing approaches for dynamic node embedding are unable to capture the graph-level dynamics related to events.;arxiv
1969;In this paper, we propose DyGED, a simple yet novel deep learning model for event detection on dynamic graphs.;arxiv
1970;Moreover, our approach combines structural and temporal self-attention mechanisms to account for application-specific node and time importances effectively.;arxiv
1971;Our experimental evaluation, using a representative set of datasets, demonstrates that DyGED outperforms competing solutions in terms of event detection accuracy by up to 8.5% while being more scalable than the top alternatives.;arxiv
1972;We also present case studies illustrating key features of our model.;arxiv
1973;For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations.;arxiv
1974;"The system links two recurrent networks that \help"" each other reconstruct same target image patches using complementary portions of spatial context that communicate via gradient signals.";arxiv
1975;This dual agent system builds upon prior work that proposed the iterative refinement algorithm for recurrent neural network (RNN)based decoding which improved image reconstruction compared to standard decoding techniques.;arxiv
1976;Our approach, which works with any encoder, neural or non-neural, This system progressively reduces image patch reconstruction error over a fixed number of steps.;arxiv
1977;Experiment with variants of RNN memory cells, with and without future information, find that our model consistently creates lower distortion images of higher perceptual quality compared to other approaches.;arxiv
1978;Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 1:64 decibel (dB) gain over JPEG, a 1:46 dB gain over JPEG 2000, a 1:34 dB gain over the GOOG neural baseline, 0:36 over E2E (a modern competitive neural compression model), and 0:37 over a single iterative neural decoder.;arxiv
1979;With the effective application of deep learning in computer vision, breakthroughs have been made in the research of super-resolution images reconstruction.;arxiv
1980;However, many researches have pointed out that the insufficiency of the neural network extraction on image features may bring the deteriorating of newly reconstructed image.;arxiv
1981;On the other hand, the generated pictures are sometimes too artificial because of over-smoothing.;arxiv
1982;In order to solve the above problems, we propose a novel self-calibrated convolutional generative adversarial networks.;arxiv
1983;The generator consists of feature extraction and image reconstruction.;arxiv
1984;Feature extraction uses self-calibrated convolutions, which contains four portions, and each portion has specific functions.;arxiv
1985;It can not only expand the range of receptive fields, but also obtain long-range spatial and inter-channel dependencies.;arxiv
1986;Then image reconstruction is performed, and finally a super-resolution image is reconstructed.;arxiv
1987;We have conducted thorough experiments on different datasets including set5, set14 and BSD100 under the SSIM evaluation method.;arxiv
1988;The experimental results prove the effectiveness of the proposed network.;arxiv
1989;Adversarial attacks have been expanded to speaker recognition (SR).;arxiv
1990;However, existing attacks are often assessed using different SR models, recognition tasks and datasets, and only few adversarial defenses borrowed from computer vision are considered.;arxiv
1991;Yet,these defenses have not been thoroughly evaluated against adaptive attacks.;arxiv
1992;Thus, there is still a lack of quantitative understanding about the strengths and limitations of adversarial attacks and defenses.;arxiv
1993;More effective defenses are also required for securing SR systems.;arxiv
1994;To bridge this gap, we present SEC4SR, the first platform enabling researchers to systematically and comprehensively evaluate adversarial attacks and defenses in SR.;arxiv
1995;SEC4SR incorporates 4 white-box and 2 black-box attacks, 24 defenses including our novel feature-level transformations.;arxiv
1996;It also contains techniques for mounting adaptive attacks.;arxiv
1997;Using SEC4SR, we conduct thus far the largest-scale empirical study on adversarial attacks and defenses in SR, involving 23 defenses, 15 attacks and 4 attack settings.;arxiv
1998;Extensive experiments demonstrate capabilities and advantages of SEC4SR which can benefit future research in SR.;arxiv
1999;Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information.;arxiv
2000;Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam.;arxiv
2001;We dispute this argument by showing that the empirical Fisher---unlike the Fisher---does not generally capture second-order information.;arxiv
2002;We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.;arxiv
2003;The redundancy is widely recognized in Convolutional Neural Networks (CNNs), which enables to remove unimportant filters from convolutional layers so as to slim the network with acceptable performance drop.;arxiv
2004;Inspired by the linear and combinational properties of convolution, we seek to make some filters increasingly close and eventually identical for network slimming.;arxiv
2005;To this end, we propose Centripetal SGD (C-SGD), a novel optimization method, which can train several filters to collapse into a single point in the parameter hyperspace.;arxiv
2006;When the training is completed, the removal of the identical filters can trim the network with NO performance loss, thus no finetuning is needed.;arxiv
2007;By doing so, we have partly solved an open problem of constrained filter pruning on CNNs with complicated structure, where some layers must be pruned following others.;arxiv
2008;Our experimental results on CIFAR-10 and ImageNet have justified the effectiveness of C-SGD-based filter pruning.;arxiv
2009;Moreover, we have provided empirical evidences for the assumption that the redundancy in deep neural networks helps the convergence of training by showing that a redundant CNN trained using C-SGD outperforms a normally trained counterpart with the equivalent width.;arxiv
2010;We explore a new perspective on adapting the learning rate (LR) schedule to improve the performance of the ReLU-based network as it is iteratively pruned.;arxiv
2011;Our work and contribution consist of four parts: (i);arxiv
2012;We find that, as the ReLU-based network is iteratively pruned, the distribution of weight gradients tends to become narrower.;arxiv
2013;This leads to the finding that as the network becomes more sparse, a larger value of LR should be used to train the pruned network.;arxiv
2014;(ii) Motivated by this finding, we propose a novel LR schedule, called S-Cyclical (S-Cyc) which adapts the conventional cyclical LR schedule by gradually increasing the LR upper bound (max_lr) in an S-shape as the network is iteratively pruned.;arxiv
2015;We highlight that S-Cyc is a method agnostic LR schedule that applies to many iterative pruning methods.;arxiv
2016;(iii) We evaluate the performance of the proposed S-Cyc and compare it to four LR schedule benchmarks.;arxiv
2017;Our experimental results on three state-of-the-art networks (e.g., VGG-19, ResNet-20, ResNet-50) and two popular datasets (e.g., CIFAR-10, ImageNet-200) demonstrate that S-Cyc consistently outperforms the best performing benchmark with an improvement of 2.1% - 3.4%, without substantial increase in complexity.;arxiv
2018;Backdoor attacks impose a new threat in Deep Neural Networks (DNNs), where a backdoor is inserted into the neural network by poisoning the training dataset, misclassifying inputs that contain the adversary trigger.;arxiv
2019;The major challenge for defending against these attacks is that only the attacker knows the secret trigger and the target class.;arxiv
2020;"The problem is further exacerbated by the recent introduction of ""Hidden Triggers"", where the triggers are carefully fused into the input, bypassing detection by human inspection and causing backdoor identification through anomaly detection to fail.";arxiv
2021;We propose PiDAn, an algorithm based on coherence optimization purifying the poisoned data.;arxiv
2022;Our analysis shows that representations of poisoned data and authentic data in the target class are still embedded in different linear subspaces, which implies that they show different coherence with some latent spaces.;arxiv
2023;"Based on this observation, the proposed PiDAn algorithm learns a sample-wise weight vector to maximize the projected coherence of weighted samples, where we demonstrate that the learned weight vector has a natural ""grouping effect"" and is distinguishable between authentic data and poisoned data.";arxiv
2024;This enables the systematic detection and mitigation of backdoor attacks.;arxiv
2025;Based on our theoretical analysis and experimental results, we demonstrate the effectiveness of PiDAn in defending against backdoor attacks that use different settings of poisoned samples on GTSRB and ILSVRC2012 datasets.;arxiv
2026;Our PiDAn algorithm can detect more than 90% infected classes and identify 95% poisoned samples.;arxiv
2027;Machine learning pipelines often rely on optimization procedures to make discrete decisions (e.g., sorting, picking closest neighbors, or shortest paths).;arxiv
2028;Although these discrete decisions are easily computed, they break the back-propagation of computational graphs.;arxiv
2029;In order to expand the scope of learning problems that can be solved in an end-to-end fashion, we propose a systematic method to transform optimizers into operations that are differentiable and never locally constant.;arxiv
2030;Our approach relies on stochastically perturbed optimizers, and can be used readily together with existing solvers.;arxiv
2031;Their derivatives can be evaluated efficiently, and smoothness tuned via the chosen noise amplitude.;arxiv
2032;We also show how this framework can be connected to a family of losses developed in structured prediction, and give theoretical guarantees for their use in learning tasks.;arxiv
2033;We demonstrate experimentally the performance of our approach on various tasks.;arxiv
2034;Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed.;arxiv
2035;First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices.;arxiv
2036;Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.;arxiv
2037;Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).;arxiv
2038;Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.;arxiv
2039;Computational auditory scene analysis is gaining interest in the last years.;arxiv
2040;Trailing behind the more mature field of speech recognition, it is particularly general sound event detection that is attracting increasing attention.;arxiv
2041;Crucial for training and testing reasonable models is having available enough suitable data -- until recently, general sound event databases were hardly found.;arxiv
2042;All sound events are strongly labeled with perceptual on- and offset times, paying attention to omitting in-between silences.;arxiv
2043;The amount of isolated sound events, the quality of annotations, and the particular general sound class distinguish NIGENS from other databases.;arxiv
2044;Big Data concern large-volume, growing data sets that are complex and have multiple autonomous sources.;arxiv
2045;Earlier technologies were not able to handle storage and processing of huge data thus Big Data concept comes into existence.;arxiv
2046;This is a tedious job for users unstructured data.;arxiv
2047;So, there should be some mechanism which classify unstructured data into organized form which helps user to easily access required data.;arxiv
2048;Classification techniques over big transactional database provide required data to the users from large datasets more simple way.;arxiv
2049;There are two main classification techniques, supervised and unsupervised.;arxiv
2050;In this paper we focused on to study of different supervised classification techniques.;arxiv
2051;Further this paper shows a advantages and limitations.;arxiv
2052;Neural networks represent data as projections on trained weights in a high dimensional manifold.;arxiv
2053;The trained weights act as a knowledge base consisting of causal class dependencies.;arxiv
2054;Inference built on features that identify these dependencies is termed as feed-forward inference.;arxiv
2055;Such inference mechanisms are justified based on classical cause-to-effect inductive reasoning models.;arxiv
2056;Inductive reasoning based feed-forward inference is widely used due to its mathematical simplicity and operational ease.;arxiv
2057;Nevertheless, feed-forward models do not generalize well to untrained situations.;arxiv
2058;To alleviate this generalization challenge, we propose using an effect-to-cause inference model that reasons abductively.;arxiv
2059;Here, the features represent the change from existing weight dependencies given a certain effect.;arxiv
2060;We term this change as contrast and the ensuing reasoning mechanism as contrastive reasoning.;arxiv
2061;In this paper, we formalize the structure of contrastive reasoning and propose a methodology to extract a neural network's notion of contrast.;arxiv
2062;We demonstrate the value of contrastive reasoning in two stages of a neural network's reasoning pipeline : in inferring and visually explaining decisions for the application of object recognition.;arxiv
2063;We illustrate the value of contrastively recognizing images under distortions by reporting an improvement of 3.47%, 2.56%, and 5.48% in average accuracy under the proposed contrastive framework on CIFAR-10C, noisy STL-10, and VisDA datasets respectively.;arxiv
2064;Difficult image segmentation problems, for instance left atrium MRI, can be addressed by incorporating shape priors to find solutions that are consistent with known objects.;arxiv
2065;Nonetheless, a single multivariate Gaussian is not an adequate model in cases with significant nonlinear shape variation or where the prior distribution is multimodal.;arxiv
2066;Nonparametric density estimation is more general, but has a ravenous appetite for training samples and poses serious challenges in optimization, especially in high dimensional spaces.;arxiv
2067;Here, we propose a maximum-a-posteriori formulation that relies on a generative image model by incorporating both local intensity and global shape priors.;arxiv
2068;We use deep autoencoders to capture the complex intensity distribution while avoiding the careful selection of hand-crafted features.;arxiv
2069;We formulate the shape prior as a mixture of Gaussians and learn the corresponding parameters in a high-dimensional shape space rather than pre-projecting onto a low-dimensional subspace.;arxiv
2070;In segmentation, we treat the identity of the mixture component as a latent variable and marginalize it within a generalized expectation-maximization framework.;arxiv
2071;We present a conditional maximization-based scheme that alternates between a closed-form solution for component-specific shape parameters that provides a global update-based optimization strategy, and an intensity-based energy minimization that translates the global notion of a nonlinear shape prior into a set of local penalties.;arxiv
2072;We demonstrate our approach on the left atrial segmentation from gadolinium-enhanced MRI, which is useful in quantifying the atrial geometry in patients with atrial fibrillation.;arxiv
2073;The Social Internet of Things (SIoT), integration of the Internet of Things and Social Networks paradigms, has been introduced to build a network of smart nodes that are capable of establishing social links.;arxiv
2074;In order to deal with misbehaving service provider nodes, service requestor nodes must evaluate their trustworthiness levels.;arxiv
2075;In this paper, we propose a novel trust management mechanism in the SIoT to predict the most reliable service providers for each service requestor, which leads to reduce the risk of being exposed to malicious nodes.;arxiv
2076;We model the SIoT with a flexible bipartite graph (containing two sets of nodes: service providers and service requestors), then build a social network among the service requestor nodes, using the Hellinger distance.;arxiv
2077;Afterward, we develop a social trust model using nodes' centrality and similarity measures to extract trust behaviors among the social network nodes.;arxiv
2078;Finally, a matrix factorization technique is designed to extract latent features of SIoT nodes, find trustworthy nodes, and mitigate the data sparsity and cold start problems.;arxiv
2079;We analyze the effect of parameters in the proposed trust prediction mechanism on prediction accuracy.;arxiv
2080;The results indicate that feedbacks from the neighboring nodes of a specific service requestor with high Hellinger similarity in our mechanism outperforms the best existing methods.;arxiv
2081;We also show that utilizing the social trust model, which only considers a similarity measure, significantly improves the accuracy of the prediction mechanism.;arxiv
2082;Furthermore, we evaluate the effectiveness of the proposed trust management system through a real-world SIoT use case.;arxiv
2083;Our results demonstrate that the proposed mechanism is resilient to different types of network attacks, and it can accurately find the most proper and trustworthy service provider.;arxiv
2084;In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes.;arxiv
2085;This article introduces a neural network based on-device learning approach to address this issue without going deep.;arxiv
2086;This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module.;arxiv
2087;Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.;arxiv
2088;Learning to take actions based on observations is a core requirement for artificial agents to be able to be successful and robust at their task.;arxiv
2089;Reinforcement Learning (RL) is a well-known technique for learning such policies.;arxiv
2090;However, current RL algorithms often have to deal with reward shaping, have difficulties generalizing to other environments and are most often sample inefficient.;arxiv
2091;In this paper, we explore active inference and the free energy principle, a normative theory from neuroscience that explains how self-organizing biological systems operate by maintaining a model of the world and casting action selection as an inference problem.;arxiv
2092;We apply this concept to a typical problem known to the RL community, the mountain car problem, and show how active inference encompasses both RL and learning from demonstrations.;arxiv
2093;Typically, a data owner (API provider) develops a model, often over proprietary data, and leverages the infrastructure services of a cloud vendor for hosting and serving API requests.;arxiv
2094;Clearly, this model assumes complete trust between the API Provider and cloud vendor.;arxiv
2095;On the other hand, a malicious/buggy cloud vendor may copy the APIs and offer an identical service, under-report model usage metrics, or unfairly discriminate between different API providers by offering them a nominal share of the revenue.;arxiv
2096;In this work, we present the design of a blockchain based decentralized trustless API marketplace that enables all the stakeholders in the API ecosystem to audit the behavior of the parties without having to trust a single centralized entity.;arxiv
2097;In particular, our system divides an AI model into multiple pieces and deploys them among multiple cloud vendors who then collaboratively execute the APIs.;arxiv
2098;Our design ensures that cloud vendors cannot collude with each other to steal the combined model, while individual cloud vendors and clients cannot repudiate their input or model executions.;arxiv
2099;We show that model compression can improve the population risk of a pre-trained model, by studying the tradeoff between the decrease in the generalization error and the increase in the empirical risk with model compression.;arxiv
2100;We then characterize the increase in empirical risk with model compression using rate distortion theory.;arxiv
2101;These results imply that the population risk could be improved by model compression if the decrease in generalization error exceeds the increase in empirical risk.;arxiv
2102;We show through a linear regression example that such a decrease in population risk due to model compression is indeed possible.;arxiv
2103;Our theoretical results further suggest that the Hessian-weighted $K$-means clustering compression approach can be improved by regularizing the distance between the clustering centers.;arxiv
2104;We provide experiments with neural networks to support our theoretical assertions.;arxiv
2105;In the visual decoding domain, visually reconstructing presented images given the corresponding human brain activity monitored by functional magnetic resonance imaging (fMRI) is difficult, especially when reconstructing viewed natural images.;arxiv
2106;Visual reconstruction is a conditional image generation on fMRI data and thus generative adversarial network (GAN) for natural image generation is recently introduced for this task.;arxiv
2107;Although GAN-based methods have greatly improved, the fidelity and naturalness of reconstruction are still unsatisfactory due to the small number of fMRI data samples and the instability of GAN training.;arxiv
2108;In this study, we proposed a new GAN-based Bayesian visual reconstruction method (GAN-BVRM) that includes a classifier to decode categories from fMRI data, a pre-trained conditional generator to generate natural images of specified categories, and a set of encoding models and evaluator to evaluate generated images.;arxiv
2109;GAN-BVRM employs the pre-trained generator of the prevailing BigGAN to generate masses of natural images, and selects the images that best matches with the corresponding brain activity through the encoding models as the reconstruction of the image stimuli.;arxiv
2110;In this process, the semantic and detailed contents of reconstruction are controlled by decoded categories and encoding models, respectively.;arxiv
2111;GAN-BVRM used the Bayesian manner to avoid contradiction between naturalness and fidelity from current GAN-based methods and thus can improve the advantages of GAN.;arxiv
2112;Experimental results revealed that GAN-BVRM improves the fidelity and naturalness, that is, the reconstruction is natural and similar to the presented image stimuli.;arxiv
2113;This paper proposes a novel end-to-end deep learning framework that simultaneously identifies demand baselines and the incentive-based agent demand response model, from the net demand measurements and incentive signals.;arxiv
2114;These two intermediate predictions are integrated, to form the net demand forecast.;arxiv
2115;We then propose a gradient-descent approach that backpropagates the net demand forecast errors to update the weights of the agent model and the weights of baseline demand forecast, jointly.;arxiv
2116;We demonstrate the effectiveness of our approach through computation experiments with synthetic demand response traces and a large-scale real world demand response dataset.;arxiv
2117;Our results show that the approach accurately identifies the demand response model, even without any prior knowledge about the baseline demand.;arxiv
2118;Accurate hardware performance models are critical to efficient code generation.;arxiv
2119;They can be used by compilers to make heuristic decisions, by superoptimizers as a minimization objective, or by autotuners to find an optimal configuration for a specific program.;arxiv
2120;However, they are difficult to develop because contemporary processors are complex, and the recent proliferation of deep learning accelerators has increased the development burden.;arxiv
2121;We demonstrate a method of learning performance models from a corpus of tensor computation graph programs for Tensor Processing Unit (TPU) instances.;arxiv
2122;We show that our learned model outperforms a heavily-optimized analytical performance model on two tasks -- tile-size selection and operator fusion -- and that it helps an autotuner discover faster programs in a setting where access to TPUs is limited or expensive.;arxiv
2123;We design and analyze an algorithm for first-order stochastic optimization of a large class of functions on $\mathbb{R}^d$. In particular, we consider the \emph{variationally coherent} functions which can be convex or non-convex.;arxiv
2124;The iterates of our algorithm on variationally coherent functions converge almost surely to the global minimizer $\boldsymbol{x}^*$. Additionally, the very same algorithm with the same hyperparameters, after $T$ iterations guarantees on convex functions that the expected suboptimality gap is bounded by $\widetilde{O}(\|\boldsymbol{x}^* - \boldsymbol{x}_0\| T^{-1/2+\epsilon})$ for any $\epsilon>0$.;arxiv
2125;It is the first algorithm to achieve both these properties at the same time.;arxiv
2126;Also, the rate for convex functions essentially matches the performance of parameter-free algorithms.;arxiv
2127;Our algorithm is an instance of the Follow The Regularized Leader algorithm with the added twist of using \emph{rescaled gradients} and time-varying linearithmic regularizers.;arxiv
2128;In this paper, we proposed and validated a fully automatic pipeline for hippocampal surface generation via 3D U-net coupled with active shape modeling (ASM).;arxiv
2129;Principally, the proposed pipeline consisted of three steps.;arxiv
2130;In the beginning, for each magnetic resonance image, a 3D U-net was employed to obtain the automatic hippocampus segmentation at each hemisphere.;arxiv
2131;Secondly, ASM was performed on a group of pre-obtained template surfaces to generate mean shape and shape variation parameters through principal component analysis.;arxiv
2132;Ultimately, hybrid particle swarm optimization was utilized to search for the optimal shape variation parameters that best match the segmentation.;arxiv
2133;The hippocampal surface was then generated from the mean shape and the shape variation parameters.;arxiv
2134;The proposed pipeline was observed to provide hippocampal surfaces at both hemispheres with high accuracy, correct anatomical topology, and sufficient smoothness.;arxiv
2135;Image repurposing is a commonly used method for spreading misinformation on social media and online forums, which involves publishing untampered images with modified metadata to create rumors and further propaganda.;arxiv
2136;While manual verification is possible, given vast amounts of verified knowledge available on the internet, the increasing prevalence and ease of this form of semantic manipulation call for the development of robust automatic ways of assessing the semantic integrity of multimedia data.;arxiv
2137;In this paper, we present a novel method for image repurposing detection that is based on the real-world adversarial interplay between a bad actor who repurposes images with counterfeit metadata and a watchdog who verifies the semantic consistency between images and their accompanying metadata, where both players have access to a reference dataset of verified content, which they can use to achieve their goals.;arxiv
2138;The proposed method exhibits state-of-the-art performance on location-identity, subject-identity and painting-artist verification, showing its efficacy across a diverse set of scenarios.;arxiv
2139;There is increasing interest in learning algorithms that involve interaction between human and machine.;arxiv
2140;Comparison-based queries are among the most natural ways to get feedback from humans.;arxiv
2141;A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers.;arxiv
2142;The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query.;arxiv
2143;In this paper, we introduce a new weak oracle model, where a non-malicious user responds to a pairwise comparison query only when she is quite sure about the answer.;arxiv
2144;This model is able to mimic the behavior of a human in noise-prone regions.;arxiv
2145;We also consider the application of this weak oracle model to the problem of content search (a variant of the nearest neighbor search problem) through comparisons.;arxiv
2146;More specifically, we aim at devising efficient algorithms to locate a target object in a database equipped with a dissimilarity metric via invocation of the weak comparison oracle.;arxiv
2147;We propose two algorithms termed WORCS-I and WORCS-II (Weak-Oracle Comparison-based Search), which provably locate the target object in a number of comparisons close to the entropy of the target distribution.;arxiv
2148;While WORCS-I provides better theoretical guarantees, WORCS-II is applicable to more technically challenging scenarios where the algorithm has limited access to the ranking dissimilarity between objects.;arxiv
2149;A series of experiments validate the performance of our proposed algorithms.;arxiv
2150;This study addresses the issue of predicting the glaucomatous visual field loss from patient disease datasets.;arxiv
2151;Our goal is to accurately predict the progress of the disease in individual patients.;arxiv
2152;As very few measurements are available for each patient, it is difficult to produce good predictors for individuals.;arxiv
2153;A recently proposed clustering-based method enhances the power of prediction using patient data with similar spatiotemporal patterns.;arxiv
2154;Each patient is categorized into a cluster of patients, and a predictive model is constructed using all of the data in the class.;arxiv
2155;Predictions are highly dependent on the quality of clustering, but it is difficult to identify the best clustering method.;arxiv
2156;Thus, we propose a method for aggregating cluster-based predictors to obtain better prediction accuracy than from a single cluster-based prediction.;arxiv
2157;Further, the method shows very high performances by hierarchically aggregating experts generated from several cluster-based methods.;arxiv
2158;We use real datasets to demonstrate that our method performs significantly better than conventional clustering-based and patient-wise regression methods, because the hierarchical aggregating strategy has a mechanism whereby good predictors in a small community can thrive.;arxiv
2159;Long Short-Term Memory networks trained with gradient descent and back-propagation have received great success in various applications.;arxiv
2160;However, point estimation of the weights of the networks is prone to over-fitting problems and lacks important uncertainty information associated with the estimation.;arxiv
2161;However, exact Bayesian neural network methods are intractable and non-applicable for real-world applications.;arxiv
2162;In this study, we propose an approximate estimation of the weights uncertainty using Ensemble Kalman Filter, which is easily scalable to a large number of weights.;arxiv
2163;Furthermore, we optimize the covariance of the noise distribution in the ensemble update step using maximum likelihood estimation.;arxiv
2164;To assess the proposed algorithm, we apply it to outlier detection in five real-world events retrieved from the Twitter platform.;arxiv
2165;Interpretable machine learning has gained much attention recently.;arxiv
2166;Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system.;arxiv
2167;However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations.;arxiv
2168;We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation.;arxiv
2169;VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations.;arxiv
2170;For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive).;arxiv
2171;We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics;arxiv
2172;We introduce a new method for generating text, and in particular song lyrics, based on the speech-like acoustic qualities of a given audio file.;arxiv
2173;We repurpose a vocal source separation algorithm and an acoustic model trained to recognize isolated speech, instead inputting instrumental music or environmental sounds.;arxiv
2174;"Feeding the ""mistakes"" of the vocal separator into the recognizer, we obtain a transcription of words \emph{imagined} to be spoken in the input audio.";arxiv
2175;We describe the key components of our approach, present initial analysis, and discuss the potential of the method for machine-in-the-loop collaboration in creative applications.;arxiv
2176;Denoising is a fundamental challenge in scientific imaging.;arxiv
2177;Deep convolutional neural networks (CNNs) provide the current state of the art in denoising natural images, where they produce impressive results.;arxiv
2178;However, their potential has barely been explored in the context of scientific imaging.;arxiv
2179;Denoising CNNs are typically trained on real natural images artificially corrupted with simulated noise.;arxiv
2180;In contrast, in scientific applications, noiseless ground-truth images are usually not available.;arxiv
2181;To address this issue, we propose a simulation-based denoising (SBD) framework, in which CNNs are trained on simulated images.;arxiv
2182;We test the framework on data obtained from transmission electron microscopy (TEM), an imaging technique with widespread applications in material science, biology, and medicine.;arxiv
2183;SBD outperforms existing techniques by a wide margin on a simulated benchmark dataset, as well as on real data.;arxiv
2184;Apart from the denoised images, SBD generates likelihood maps to visualize the agreement between the structure of the denoised image and the observed data.;arxiv
2185;Our results reveal shortcomings of state-of-the-art denoising architectures, such as their small field-of-view: substantially increasing the field-of-view of the CNNs allows them to exploit non-local periodic patterns in the data, which is crucial at high noise levels.;arxiv
2186;In addition, we analyze the generalization capability of SBD, demonstrating that the trained networks are robust to variations of imaging parameters and of the underlying signal structure.;arxiv
2187;Finally, we release the first publicly available benchmark dataset of TEM images, containing 18,000 examples.;arxiv
2188;We consider the problem of high-dimensional Ising (graphical) model selection.;arxiv
2189;We propose a simple algorithm for structure estimation based on the thresholding of the empirical conditional variation distances.;arxiv
2190;We introduce a novel criterion for tractable graph families, where this method is efficient, based on the presence of sparse local separators between node pairs in the underlying graph.;arxiv
2191;For such graphs, the proposed algorithm has a sample complexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number of variables, and $J_{\min}$ is the minimum (absolute) edge potential in the model.;arxiv
2192;We also establish nonasymptotic necessary and sufficient conditions for structure estimation.;arxiv
2193;Melanoma is considered to be the most aggressive form of skin cancer.;arxiv
2194;Due to the similar shape of malignant and benign cancerous lesions, doctors spend considerably more time when diagnosing these findings.;arxiv
2195;At present, the evaluation of malignancy is performed primarily by invasive histological examination of the suspicious lesion.;arxiv
2196;Developing an accurate classifier for early and efficient detection can minimize and monitor the harmful effects of skin cancer and increase patient survival rates.;arxiv
2197;This paper proposes a multi-class classification task using the CoAtNet architecture, a hybrid model that combines the depthwise convolution matrix operation of traditional convolutional neural networks with the strengths of Transformer models and self-attention mechanics to achieve better generalization and capacity.;arxiv
2198;The proposed multi-class classifier achieves an overall precision of 0.901, recall 0.895, and AP 0.923, indicating high performance compared to other state-of-the-art networks.;arxiv
2199;Simulation has the potential to transform the development of robust algorithms for mobile agents deployed in safety-critical scenarios.;arxiv
2200;However, the poor photorealism and lack of diverse sensor modalities of existing simulation engines remain key hurdles towards realizing this potential.;arxiv
2201;Here, we present VISTA, an open source, data-driven simulator that integrates multiple types of sensors for autonomous vehicles.;arxiv
2202;Using high fidelity, real-world datasets, VISTA represents and simulates RGB cameras, 3D LiDAR, and event-based cameras, enabling the rapid generation of novel viewpoints in simulation and thereby enriching the data available for policy learning with corner cases that are difficult to capture in the physical world.;arxiv
2203;Using VISTA, we demonstrate the ability to train and test perception-to-control policies across each of the sensor types and showcase the power of this approach via deployment on a full scale autonomous vehicle.;arxiv
2204;The policies learned in VISTA exhibit sim-to-real transfer without modification and greater robustness than those trained exclusively on real-world data.;arxiv
2205;Poor sitting habits have been identified as a risk factor to musculoskeletal disorders and lower back pain especially on the elderly, disabled people, and office workers.;arxiv
2206;In the current computerized world, even while involved in leisure or work activity, people tend to spend most of their days sitting at computer desks.;arxiv
2207;This can result in spinal pain and related problems.;arxiv
2208;Therefore, a means to remind people about their sitting habits and provide recommendations to counterbalance, such as physical exercise, is important.;arxiv
2209;Posture recognition for seated postures have not received enough attention as most works focus on standing postures.;arxiv
2210;Wearable sensors, pressure or force sensors, videos and images were used for posture recognition in the literature.;arxiv
2211;The aim of this study is to build Machine Learning models for classifying sitting posture of a person by analyzing data collected from a chair platted with two 32 by 32 pressure sensors at its seat and backrest.;arxiv
2212;"Models were built using five algorithms: Random Forest (RF), Gaussian Na\""ive Bayes, Logistic Regression, Support Vector Machine and Deep Neural Network (DNN).";arxiv
2213;All the models are evaluated using KFold cross-validation technique.;arxiv
2214;This paper presents experiments conducted using the two separate datasets, controlled and realistic, and discusses results achieved at classifying six sitting postures.;arxiv
2215;Average classification accuracies of 98% and 97% were achieved on the controlled and realistic datasets, respectively.;arxiv
2216;One-shot Neural Architecture Search (NAS) aims to minimize the computational expense of discovering state-of-the-art models.;arxiv
2217;However, in the past year attention has been drawn to the comparable performance of naive random search across the same search spaces used by leading NAS algorithms.;arxiv
2218;To address this, we explore the effects of drastically relaxing the NAS search space, and we present Bonsai-Net, an efficient one-shot NAS method to explore our relaxed search space.;arxiv
2219;Bonsai-Net is built around a modified differential pruner and can consistently discover state-of-the-art architectures that are significantly better than random search with fewer parameters than other state-of-the-art methods.;arxiv
2220;Additionally, Bonsai-Net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully-trained models from scratch.;arxiv
2221;Many important data analysis applications present with severely imbalanced datasets with respect to the target variable.;arxiv
2222;A typical example is medical image analysis, where positive samples are scarce, while performance is commonly estimated against the correct detection of these positive examples.;arxiv
2223;We approach this challenge by formulating the problem as anomaly detection with generative models.;arxiv
2224;In this position paper, we present the use of state-of-the-art deep generative models (GAN and VAE) for the estimation of a likelihood of the data.;arxiv
2225;On the other hand, for the NLST case, neither GANs nor VAEs were able to capture the complexity of the data and discriminate anomalies at the level that this task requires.;arxiv
2226;These results show that even though there are a number of successes presented in the literature for using generative models in similar applications, there remain further challenges for broad successful implementation.;arxiv
2227;We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem.;arxiv
2228;Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than $d$ hops to arrive, where $d$ is a delay parameter.;arxiv
2229;We introduce \textsc{Exp3-Coop}, a cooperative version of the {\sc Exp3} algorithm and prove that with $K$ actions and $N$ agents the average per-agent regret after $T$ rounds is at most of order $\sqrt{\bigl(d+1 + \tfrac{K}{N}\alpha_{\le d}\bigr)(T\ln K)}$, where $\alpha_{\le d}$ is the independence number of the $d$-th power of the connected communication graph $G$.;arxiv
2230;We then show that for any connected graph, for $d=\sqrt{K}$ the regret bound is $K^{1/4}\sqrt{T}$, strictly better than the minimax regret $\sqrt{KT}$ for noncooperating agents.;arxiv
2231;More informed choices of $d$ lead to bounds which are arbitrarily close to the full information minimax regret $\sqrt{T\ln K}$ when $G$ is dense.;arxiv
2232;When $G$ has sparse components, we show that a variant of \textsc{Exp3-Coop}, allowing agents to choose their parameters according to their centrality in $G$, strictly improves the regret.;arxiv
2233;Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.;arxiv
2234;Prediction markets are used in real life to predict outcomes of interest such as presidential elections.;arxiv
2235;This paper presents a mathematical theory of artificial prediction markets for supervised learning of conditional probability estimators.;arxiv
2236;The artificial prediction market is a novel method for fusing the prediction information of features or trained classifiers, where the fusion result is the contract price on the possible outcomes.;arxiv
2237;The market can be trained online by updating the participants' budgets using training examples.;arxiv
2238;Inspired by the real prediction markets, the equations that govern the market are derived from simple and reasonable assumptions.;arxiv
2239;Efficient numerical algorithms are presented for solving these equations.;arxiv
2240;The obtained artificial prediction market is shown to be a maximum likelihood estimator.;arxiv
2241;It generalizes linear aggregation, existent in boosting and random forest, as well as logistic regression and some kernel methods.;arxiv
2242;Furthermore, the market mechanism allows the aggregation of specialized classifiers that participate only on specific instances.;arxiv
2243;Experimental comparisons show that the artificial prediction markets often outperform random forest and implicit online learning on synthetic data and real UCI datasets.;arxiv
2244;Moreover, an extensive evaluation for pelvic and abdominal lymph node detection in CT data shows that the prediction market improves adaboost's detection rate from 79.6% to 81.2% at 3 false positives/volume.;arxiv
2245;Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019.;arxiv
2246;Data collection and annotation are time-consuming in machine learning, expecially for large scale problem.;arxiv
2247;A common approach for this problem is to transfer knowledge from a related labeled domain to a target one.;arxiv
2248;There are two popular ways to achieve this goal: adversarial learning and self training.;arxiv
2249;In this article, we first analyze the training unstablity problem and the mistaken confusion issue in adversarial learning process.;arxiv
2250;Then, inspired by domain confusion and self-ensembling methods, we propose a combined model to learn feature and class jointly invariant representation, namely Domain Confusion with Self Ensembling (DCSE).;arxiv
2251;The experiments verified that our proposed approach can offer better performance than empirical art in a variety of unsupervised domain adaptation benchmarks.;arxiv
2252;Deep generative modeling using flows has gained popularity owing to the tractable exact log-likelihood estimation with efficient training and synthesis process.;arxiv
2253;However, flow models suffer from the challenge of having high dimensional latent space, the same in dimension as the input space.;arxiv
2254;An effective solution to the above challenge as proposed by Dinh et al. (2016) is a multi-scale architecture, which is based on iterative early factorization of a part of the total dimensions at regular intervals.;arxiv
2255;Prior works on generative flow models involving a multi-scale architecture perform the dimension factorization based on static masking.;arxiv
2256;We propose a novel multi-scale architecture that performs data-dependent factorization to decide which dimensions should pass through more flow layers.;arxiv
2257;To facilitate the same, we introduce a heuristic based on the contribution of each dimension to the total log-likelihood which encodes the importance of the dimensions.;arxiv
2258;Our proposed heuristic is readily obtained as part of the flow training process, enabling the versatile implementation of our likelihood contribution based multi-scale architecture for generic flow models.;arxiv
2259;We present such implementations for several state-of-the-art flow models and demonstrate improvements in log-likelihood score and sampling quality on standard image benchmarks.;arxiv
2260;We also conduct ablation studies to compare the proposed method with other options for dimension factorization.;arxiv
2261;Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are increasingly being deployed across multiple functionalities, ranging from healthcare devices and wearables to critical infrastructures, e.g., nuclear power plants, autonomous vehicles, smart cities, and smart homes.;arxiv
2262;These devices are inherently not secure across their comprehensive software, hardware, and network stacks, thus presenting a large attack surface that can be exploited by hackers.;arxiv
2263;In this article, we present an innovative technique for detecting unknown system vulnerabilities, managing these vulnerabilities, and improving incident response when such vulnerabilities are exploited.;arxiv
2264;The novelty of this approach lies in extracting intelligence from known real-world CPS/IoT attacks, representing them in the form of regular expressions, and employing machine learning (ML) techniques on this ensemble of regular expressions to generate new attack vectors and security vulnerabilities.;arxiv
2265;Our results show that 10 new attack vectors and 122 new vulnerability exploits can be successfully generated that have the potential to exploit a CPS or an IoT ecosystem.;arxiv
2266;The ML methodology achieves an accuracy of 97.4% and enables us to predict these attacks efficiently with an 87.2% reduction in the search space.;arxiv
2267;We demonstrate the application of our method to the hacking of the in-vehicle network of a connected car.;arxiv
2268;To defend against the known attacks and possible novel exploits, we discuss a defense-in-depth mechanism for various classes of attacks and the classification of data targeted by such attacks.;arxiv
2269;This defense mechanism optimizes the cost of security measures based on the sensitivity of the protected resource, thus incentivizing its adoption in real-world CPS/IoT by cybersecurity practitioners.;arxiv
2270;ICD coding is the international standard for capturing and reporting health conditions and diagnosis for revenue cycle management in healthcare.;arxiv
2271;Manually assigning ICD codes is prone to human error due to the large code vocabulary and the similarities between codes.;arxiv
2272;Since machine learning based approaches require ground truth training data, the inconsistency among human coders is manifested as noise in labeling, which makes the training and evaluation of ICD classifiers difficult in presence of such noise.;arxiv
2273;This paper investigates the characteristics of such noise in manually-assigned ICD-10 codes and furthermore, proposes a method to train robust ICD-10 classifiers in the presence of labeling noise.;arxiv
2274;Our research concluded that the nature of such noise is systematic.;arxiv
2275;Most of the existing methods for handling label noise assume that the noise is completely random and independent of features or labels, which is not the case for ICD data.;arxiv
2276;Therefore, we develop a new method for training robust classifiers in the presence of systematic noise.;arxiv
2277;We compared our method with the baseline that does not handle label noise and the baseline methods that assume random noise, and demonstrated that our proposed method outperforms all baselines when evaluated on expert validated labels.;arxiv
2278;In this paper, we consider the problem of $\ell_{p}$ norm linear regression, which has several applications such as in sparse recovery, data clustering, and semi-supervised learning.;arxiv
2279;The problem, even though convex, does not enjoy a closed-form solution.;arxiv
2280;Subsequently, we show that the proposed algorithm can also be applied for the graph based semi-supervised learning problem.;arxiv
2281;We show through numerical simulations that the proposed algorithm converges to the optimal solution for any random initialization and also performs better than the state-of-the-art algorithms in terms of speed of convergence.;arxiv
2282;We also evaluate the performance of the proposed algorithm using simulated and real data for the graph based semi-supervised learning problem.;arxiv
2283;Analyzing the morphology of cells in microscopy images can provide insights into the mechanism of compounds or the function of genes.;arxiv
2284;We propose Treatment ExemplArs with Mixture-of-experts (TEAMs), an embedding learning approach that learns a set of experts that are specialized in capturing technical variations in our training set and then aggregates specialist's predictions at test time.;arxiv
2285;Thus, TEAMs can learn powerful embeddings with less technical variation bias by minimizing the noise from every expert.;arxiv
2286;To train our model, we leverage Treatment Exemplars that enable our approach to capture the distribution of the entire dataset in every minibatch while still fitting into GPU memory.;arxiv
2287;We evaluate our approach on three datasets for tasks like drug discovery, boosting performance on identifying the true mechanism of action of cell treatments by 5.5-11% over the state-of-the-art.;arxiv
2288;$(j,k)$-projective clustering is the natural generalization of the family of $k$-clustering and $j$-subspace clustering problems.;arxiv
2289;In this paper, we propose the first algorithm that returns an $L_\infty$ coreset of size polynomial in $d$. Moreover, we give the first strong coreset construction for general $M$-estimator regression.;arxiv
2290;Specifically, we show that our construction provides efficient coreset constructions for Cauchy, Welsch, Huber, Geman-McClure, Tukey, $L_1-L_2$, and;arxiv
2291;Fair regression, as well as general concave and power-bounded loss functions.;arxiv
2292;Finally, we provide experimental results based on real-world datasets, showing the efficacy of our approach.;arxiv
2293;The incredible effectiveness of adversarial attacks on fooling deep neural networks poses a tremendous hurdle in the widespread adoption of deep learning in safety and security-critical domains.;arxiv
2294;While adversarial defense mechanisms have been proposed since the discovery of the adversarial vulnerability issue of deep neural networks, there is a long path to fully understand and address this issue.;arxiv
2295;In this study, we hypothesize that part of the reason for the incredible effectiveness of adversarial attacks is their ability to implicitly tap into and exploit the gradient flow of a deep neural network.;arxiv
2296;This innate ability to exploit gradient flow makes defending against such attacks quite challenging.;arxiv
2297;Motivated by this hypothesis we argue that if a deep neural network architecture can explicitly tap into its own gradient flow during the training, it can boost its defense capability significantly.;arxiv
2298;Inspired by this fact, we introduce the concept of self-gradient networks, a novel deep neural network architecture designed to be more robust against adversarial perturbations.;arxiv
2299;Gradient flow information is leveraged within self-gradient networks to achieve greater perturbation stability beyond what can be achieved in the standard training process.;arxiv
2300;We conduct a theoretical analysis to gain better insights into the behaviour of the proposed self-gradient networks to illustrate the efficacy of leverage this additional gradient flow information.;arxiv
2301;The proposed self-gradient network architecture enables much more efficient and effective adversarial training, leading to faster convergence towards an adversarially robust solution by at least 10X. Experimental results demonstrate the effectiveness of self-gradient networks when compared with state-of-the-art adversarial learning strategies, with 10% improvement on the CIFAR10 dataset under PGD and CW adversarial perturbations.;arxiv
2302;In this paper we consider the problem of clustering collections of very short texts using subspace clustering.;arxiv
2303;This problem arises in many applications such as product categorisation, fraud detection, and sentiment analysis.;arxiv
2304;The main challenge lies in the fact that the vectorial representation of short texts is both high-dimensional, due to the large number of unique terms in the corpus, and extremely sparse, as each text contains a very small number of words with no repetition.;arxiv
2305;We propose a new, simple subspace clustering algorithm that relies on linear algebra to cluster such datasets.;arxiv
2306;Experimental results on identifying product categories from product names obtained from the US Amazon website indicate that the algorithm can be competitive against state-of-the-art clustering algorithms.;arxiv
2307;Machine learning models often pose a threat to the privacy of individuals whose data is part of the training set.;arxiv
2308;Several recent attacks have been able to infer sensitive information from trained models, including model inversion or attribute inference attacks.;arxiv
2309;These attacks are able to reveal the values of certain sensitive features of individuals who participated in training the model.;arxiv
2310;It has also been shown that several factors can contribute to an increased risk of model inversion, including feature influence.;arxiv
2311;We observe that not all features necessarily share the same level of privacy or sensitivity.;arxiv
2312;In many cases, certain features used to train a model are considered especially sensitive and therefore propitious candidates for inversion.;arxiv
2313;We present a solution for countering model inversion attacks in tree-based models, by reducing the influence of sensitive features in these models.;arxiv
2314;This is an avenue that has not yet been thoroughly investigated, with only very nascent previous attempts at using this as a countermeasure against attribute inference.;arxiv
2315;Our work shows that, in many cases, it is possible to train a model in different ways, resulting in different influence levels of the various features, without necessarily harming the model's accuracy.;arxiv
2316;We are able to utilize this fact to train models in a manner that reduces the model's reliance on the most sensitive features, while increasing the importance of less sensitive features.;arxiv
2317;Our evaluation confirms that training models in this manner reduces the risk of inference for those features, as demonstrated through several black-box and white-box attacks.;arxiv
2318;Children learn continually by asking questions about the concepts they are most curious about.;arxiv
2319;With robots becoming an integral part of our society, they must also learn unknown concepts continually by asking humans questions.;arxiv
2320;The paper analyzes a recent state-of-the-art approach for continual learning.;arxiv
2321;The paper further develops a self-supervised technique to find most of the uncertain objects in an environment by utilizing the cluster representation of the previously learned classes.;arxiv
2322;We test our approach on a benchmark dataset for continual learning on robots.;arxiv
2323;Our results show that our curiosity-driven continual learning approach beats random sampling and softmax-based uncertainty sampling in terms of classification accuracy and the total number of classes learned.;arxiv
2324;Adversarial Reinforcement Learning for Procedural Content Generation, which procedurally generates and tests previously unseen environments with an auxiliary input as a control variable.;arxiv
2325;Training RL agents over novel environments is a notoriously difficult task.;arxiv
2326;One popular approach is to procedurally generate different environments to increase the generalizability of the trained agents.;arxiv
2327;ARLPCG instead deploys an adversarial model with one PCG RL agent (called Generator) and one solving RL agent (called Solver).;arxiv
2328;The Generator receives a reward signal based on the Solver's performance, which encourages the environment design to be challenging but not impossible.;arxiv
2329;To further drive diversity and control of the environment generation, we propose using auxiliary inputs for the Generator.;arxiv
2330;The benefit is two-fold: Firstly, the Solver achieves better generalization through the Generator's generated challenges.;arxiv
2331;Secondly, the trained Generator can be used as a creator of novel environments that, together with the Solver, can be shown to be solvable.;arxiv
2332;We create two types of 3D environments to validate our model, representing two popular game genres: a third-person platformer and a racing game.;arxiv
2333;In these cases, we shows that ARLPCG has a significantly better solve ratio, and that the auxiliary inputs renders the levels creation controllable to a certain degree.;arxiv
2334;For a video compilation of the results please visit https://youtu.be/z7q2PtVsT0I.;arxiv
2335;Confidence scores that minimize standard metrics such as the expected calibration error (ECE) accurately measure the reliability on average across the entire population.;arxiv
2336;However, it is in general impossible to measure the reliability of an individual prediction.;arxiv
2337;In this work, we propose the local calibration error (LCE) to span the gap between average and individual reliability.;arxiv
2338;For each individual prediction, the LCE measures the average reliability of a set of similar predictions, where similarity is quantified by a kernel function on a pretrained feature space and by a binning scheme over predicted model confidences.;arxiv
2339;We show theoretically that the LCE can be estimated sample-efficiently from data, and empirically find that it reveals miscalibration modes that are more fine-grained than the ECE can detect.;arxiv
2340;Our key result is a novel local recalibration method LoRe, to improve confidence scores for individual predictions and decrease the LCE.;arxiv
2341;Experimentally, we show that our recalibration method produces more accurate confidence scores, which improves downstream fairness and decision making on classification tasks with both image and tabular data.;arxiv
2342;The paper deals with the problem of finding the best alternatives on the basis of pairwise comparisons when these comparisons need not be transitive.;arxiv
2343;In this setting, we study a reinforcement urn model.;arxiv
2344;We prove convergence to the optimal solution when reinforcement of a winning alternative occurs each time after considering three random alternatives.;arxiv
2345;The simpler process, which reinforces the winner of a random pair does not always converges: it may cycle.;arxiv
2346;The rapid spread of information over social media influences quantitative trading and investments.;arxiv
2347;The growing popularity of speculative trading of highly volatile assets such as cryptocurrencies and meme stocks presents a fresh challenge in the financial realm.;arxiv
2348;"Investigating such ""bubbles"" - periods of sudden anomalous behavior of markets are critical in better understanding investor behavior and market dynamics.";arxiv
2349;However, high volatility coupled with massive volumes of chaotic social media texts, especially for underexplored assets like cryptocoins pose a challenge to existing methods.;arxiv
2350;Taking the first step towards NLP for cryptocoins, we present and publicly release CryptoBubbles, a novel multi-span identification task for bubble detection, and a dataset of more than 400 cryptocoins from 9 exchanges over five years spanning over two million tweets.;arxiv
2351;Further, we develop a set of sequence-to-sequence hyperbolic models suited to this multi-span identification task based on the power-law dynamics of cryptocurrencies and user behavior on social media.;arxiv
2352;"We further test the effectiveness of our models under zero-shot settings on a test set of Reddit posts pertaining to 29 ""meme stocks"", which see an increase in trade volume due to social media hype.";arxiv
2353;Through quantitative, qualitative, and zero-shot analyses on Reddit and Twitter spanning cryptocoins and meme-stocks, we show the practical applicability of CryptoBubbles and hyperbolic models.;arxiv
2354;Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability.;arxiv
2355;The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge.;arxiv
2356;We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior.;arxiv
2357;We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input.;arxiv
2358;Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.;arxiv
2359;We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.;arxiv
2360;Texture segmentation is the process of partitioning an image into regions with different textures containing a similar group of pixels.;arxiv
2361;Detecting the discontinuity of the filter's output and their statistical properties help in segmenting and classifying a given image with different texture regions.;arxiv
2362;In this proposed paper, chili x-ray image texture segmentation is performed by using Gabor filter.;arxiv
2363;The texture segmented result obtained from Gabor filter fed into three texture filters, namely Entropy, Standard Deviation and Range filter.;arxiv
2364;After performing texture analysis, features can be extracted by using Statistical methods.;arxiv
2365;In this paper Gray Level Co-occurrence Matrices and First order statistics are used as feature extraction methods.;arxiv
2366;Features extracted from statistical methods are given to Support Vector Machine (SVM) classifier.;arxiv
2367;Using this methodology, it is found that texture segmentation is followed by the Gray Level Co-occurrence Matrix feature extraction method gives a higher accuracy rate of 84% when compared with First order feature extraction method.;arxiv
2368;Key Words: Texture segmentation, Texture filter, Gabor filter, Feature extraction methods, SVM classifier.;arxiv
2369;Generative Adversarial Networks (GANs) have received a great deal of attention due in part to recent success in generating original, high-quality samples from visual domains.;arxiv
2370;However, most current methods only allow for users to guide this image generation process through limited interactions.;arxiv
2371;"In this work we develop a novel GAN framework that allows humans to be ""in-the-loop"" of the image generation process.";arxiv
2372;"Our technique iteratively accepts relative constraints of the form ""Generate an image more like image A than image B"".";arxiv
2373;After each constraint is given, the user is presented with new outputs from the GAN, informing the next round of feedback.;arxiv
2374;In our experiments, we show that our GAN framework is able to generate images that are of comparable quality to equivalent unsupervised GANs while satisfying a large number of the constraints provided by users, effectively changing a GAN into one that allows users interactive control over image generation without sacrificing image quality.;arxiv
2375;Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks.;arxiv
2376;In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark.;arxiv
2377;Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time.;arxiv
2378;Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms.;arxiv
2379;Machine learning (ML) has developed rapidly in the past few years and has successfully been utilized for a broad range of tasks, including phishing detection.;arxiv
2380;However, building an effective ML-based detection system is not a trivial task, and requires data scientists with knowledge of the relevant domain.;arxiv
2381;Automated Machine Learning (AutoML) frameworks have received a lot of attention in recent years, enabling non-ML experts in building a machine learning model.;arxiv
2382;This brings to an intriguing question of whether AutoML can outperform the results achieved by human data scientists.;arxiv
2383;Our paper compares the performances of six well-known, state-of-the-art AutoML frameworks on ten different phishing datasets to see whether AutoML-based models can outperform manually crafted machine learning models.;arxiv
2384;Our results indicate that AutoML-based models are able to outperform manually developed machine learning models in complex classification tasks, specifically in datasets where the features are not quite discriminative, and datasets with overlapping classes or relatively high degrees of non-linearity.;arxiv
2385;Challenges also remain in building a real-world phishing detection system using AutoML frameworks due to the current support only on supervised classification problems, leading to the need for labeled data, and the inability to update the AutoML-based models incrementally.;arxiv
2386;This indicates that experts with knowledge in the domain of phishing and cybersecurity are still essential in the loop of the phishing detection pipeline.;arxiv
2387;We investigate the problem of recovering a partially observed high-rank matrix whose columns obey a nonlinear structure such as a union of subspaces, an algebraic variety or grouped in clusters.;arxiv
2388;The recovery problem is formulated as the rank minimization of a nonlinear feature map applied to the original matrix, which is then further approximated by a constrained non-convex optimization problem involving the Grassmann manifold.;arxiv
2389;We propose two sets of algorithms, one arising from Riemannian optimization and the other as an alternating minimization scheme, both of which include first- and second-order variants.;arxiv
2390;In particular, for the alternating minimization, we establish global convergence and worst-case complexity bounds.;arxiv
2391;Additionally, using the Kurdyka-Lojasiewicz property, we show that the alternating minimization converges to a unique limit point.;arxiv
2392;We provide extensive numerical results for the recovery of union of subspaces and clustering under entry sampling and dense Gaussian sampling.;arxiv
2393;Our methods are competitive with existing approaches and, in particular, high accuracy is achieved in the recovery using Riemannian second-order methods.;arxiv
2394;We give an algorithm for completing an order-$m$ symmetric low-rank tensor from its multilinear entries in time roughly proportional to the number of tensor entries.;arxiv
2395;We apply our tensor completion algorithm to the problem of learning mixtures of product distributions over the hypercube, obtaining new algorithmic results.;arxiv
2396;If the centers of the product distribution are linearly independent, then we recover distributions with as many as $\Omega(n)$ centers in polynomial time and sample complexity.;arxiv
2397;In the general case, we recover distributions with as many as $\tilde\Omega(n)$ centers in quasi-polynomial time, answering an open problem of Feldman et al.;arxiv
2398;for the special case of distributions with incoherent bias vectors.;arxiv
2399;Our main algorithmic tool is the iterated application of a low-rank matrix completion algorithm for matrices with adversarially missing entries.;arxiv
2400;Unmanned Aerial Vehicles (UAVs) are used as aerial base-stations to relay time-sensitive packets from IoT devices to the nearby terrestrial base-station (TBS).;arxiv
2401;To address this, we propose Age-of-Information (AoI) scheduling algorithms for two-hop UAV-relayed IoT-networks.;arxiv
2402;First, we propose a low-complexity AoI scheduler, termed, MAF-MAD that employs Maximum AoI First (MAF) policy for sampling of IoT devices at UAV (hop-1) and Maximum AoI Difference (MAD) policy for updating sampled packets from UAV to the TBS (hop-2).;arxiv
2403;We prove that MAF-MAD is the optimal AoI scheduler under ideal conditions (lossless wireless channels and generate-at-will traffic-generation at IoT devices).;arxiv
2404;On the contrary, for general conditions (lossy channel conditions and varying periodic traffic-generation at IoT devices), a deep reinforcement learning algorithm, namely, Proximal Policy Optimization (PPO)-based scheduler is proposed.;arxiv
2405;Simulation results show that the proposed PPO-based scheduler outperforms other schedulers like MAF-MAD, MAF, and round-robin in all considered general scenarios.;arxiv
2406;Deep Learning as a field has been successfully used to solve a plethora of complex problems, the likes of which we could not have imagined a few decades back.;arxiv
2407;But as many benefits as it brings, there are still ways in which it can be used to bring harm to our society.;arxiv
2408;Deep fakes have been proven to be one such problem, and now more than ever, when any individual can create a fake image or video simply using an application on the smartphone, there need to be some countermeasures, with which we can detect if the image or video is a fake or real and dispose of the problem threatening the trustworthiness of online information.;arxiv
2409;Although the Deep fakes created by neural networks, may seem to be as real as a real image or video, it still leaves behind spatial and temporal traces or signatures after moderation, these signatures while being invisible to a human eye can be detected with the help of a neural network trained to specialize in Deep fake detection.;arxiv
2410;In this paper, we analyze several such states of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception Net) and compare them against each other, to find an optimal solution for various scenarios like real-time deep fake detection to be deployed in online social media platforms where the classification should be made as fast as possible or for a small news agency where the classification need not be in real-time but requires utmost accuracy.;arxiv
2411;The proper initialization of weights is crucial for the effective training and fast convergence of deep neural networks (DNNs).;arxiv
2412;Prior work in this area has mostly focused on balancing the variance among weights per layer to maintain stability of (i) the input data propagated forwards through the network and (ii) the loss gradients propagated backwards, respectively.;arxiv
2413;This prevalent heuristic is however agnostic of dependencies among gradients across the various layers and captures only firstorder effects.;arxiv
2414;In this paper, we propose and discuss an initialization principle that is based on a rigorous estimation of the global curvature of weights across layers by approximating and controlling the norm of their Hessian matrix.;arxiv
2415;The proposed approach is more systematic and recovers previous results for DNN activations such as smooth functions, dropouts, and ReLU.;arxiv
2416;Our experiments on Word2Vec and the MNIST/CIFAR image classification tasks confirm that tracking the Hessian norm is a useful diagnostic tool which helps to more rigorously initialize weights;arxiv
2417;While different methods exist to tackle distinct types of distribution shift, such as label shift (in the form of adversarial attacks) or domain shift, tackling the joint shift setting is still an open problem.;arxiv
2418;Through the study of a joint distribution shift manifesting both adversarial and domain-specific perturbations, we not only show that a joint shift worsens model performance compared to their individual shifts, but that the use of a similar domain worsens performance than a dissimilar domain.;arxiv
2419;To curb the performance drop, we study the use of perturbation sets motivated by input and parameter space bounds, and adopt a meta learning strategy (hypernetworks) to model parameters w.r.t.;arxiv
2420;A key aspect of human intelligence is their ability to convey their knowledge to others in succinct forms.;arxiv
2421;However, despite their predictive power, current machine learning models are largely blackboxes, making it difficult for humans to extract useful insights.;arxiv
2422;"Focusing on sequential decision-making, we design a novel machine learning algorithm that conveys its insights to humans in the form of interpretable ""tips"".";arxiv
2423;Our algorithm selects the tip that best bridges the gap in performance between human users and the optimal policy.;arxiv
2424;We evaluate our approach through a series of randomized controlled user studies where participants manage a virtual kitchen.;arxiv
2425;Our experiments show that the tips generated by our algorithm can significantly improve human performance relative to intuitive baselines.;arxiv
2426;In addition, we discuss a number of empirical insights that can help inform the design of algorithms intended for human-AI interfaces.;arxiv
2427;An important problem in deep learning is the privacy and security of neural networks (NNs).;arxiv
2428;To date, it is still poorly understood how privacy enhancing training affects the robustness of NNs.;arxiv
2429;This paper experimentally evaluates the impact of training with Differential Privacy (DP), a standard method for privacy preservation, on model vulnerability against a broad range of adversarial attacks.;arxiv
2430;The results suggest that private models are less robust than their non-private counterparts, and that adversarial examples transfer better among DP models than between non-private and private ones.;arxiv
2431;Furthermore, detailed analyses of DP and non-DP models suggest significant differences between their gradients.;arxiv
2432;Additionally, this work is the first to observe that an unfavorable choice of parameters in DP training can lead to gradient masking, and, thereby, results in a wrong sense of security.;arxiv
2433;Society has come to rely on algorithms like classifiers for important decision making, giving rise to the need for ethical guarantees such as fairness.;arxiv
2434;Fairness is typically defined by asking that some statistic of a classifier be approximately equal over protected groups within a population.;arxiv
2435;In this paper, current approaches to fairness are discussed and used to motivate algorithmic proposals that incorporate fairness into genetic programming for classification.;arxiv
2436;The first is to incorporate a fairness objective into multi-objective optimization.;arxiv
2437;The second is to adapt lexicase selection to define cases dynamically over intersections of protected groups.;arxiv
2438;We describe why lexicase selection is well suited to pressure models to perform well across the potentially infinitely many subgroups over which fairness is desired.;arxiv
2439;We use a recent genetic programming approach to construct models on four datasets for which fairness constraints are necessary, and empirically compare performance to prior methods utilizing game-theoretic solutions.;arxiv
2440;Methods are assessed based on their ability to generate trade-offs of subgroup fairness and accuracy that are Pareto optimal.;arxiv
2441;The result show that genetic programming methods in general, and random search in particular, are well suited to this task.;arxiv
2442;We explore the idea that authoring a piece of text is an act of maximizing one's expected utility.;arxiv
2443;To make this idea concrete, we consider the societally important decisions of the Supreme Court of the United States.;arxiv
2444;Extensive past work in quantitative political science provides a framework for empirically modeling the decisions of justices and how they relate to text.;arxiv
2445;"We incorporate into such a model texts authored by amici curiae (""friends of the court"" separate from the litigants) who seek to weigh in on the decision, then explicitly model their goals in a random utility model.";arxiv
2446;We demonstrate the benefits of this approach in improved vote prediction and the ability to perform counterfactual analysis.;arxiv
2447;Pathologist-defined labels are the gold standard for histopathological data sets, regardless of well-known limitations in consistency for some tasks.;arxiv
2448;To date, some datasets on mitotic figures are available and were used for development of promising deep learning-based algorithms.;arxiv
2449;In order to assess robustness of those algorithms and reproducibility of their methods it is necessary to test on several independent datasets.;arxiv
2450;The influence of different labeling methods of these available datasets is currently unknown.;arxiv
2451;To tackle this, we present an alternative set of labels for the images of the auxiliary mitosis dataset of the TUPAC16 challenge.;arxiv
2452;Additional to manual mitotic figure screening, we used a novel, algorithm-aided labeling process, that allowed to minimize the risk of missing rare mitotic figures in the images.;arxiv
2453;All potential mitotic figures were independently assessed by two pathologists.;arxiv
2454;The novel, publicly available set of labels contains 1,999 mitotic figures (+28.80%) and additionally includes 10,483 labels of cells with high similarities to mitotic figures (hard examples).;arxiv
2455;We found significant difference comparing F_1 scores between the original label set (0.549) and the new alternative label set (0.735) using a standard deep learning object detection architecture.;arxiv
2456;The models trained on the alternative set showed higher overall confidence values, suggesting a higher overall label consistency.;arxiv
2457;Findings of the present study show that pathologists-defined labels may vary significantly resulting in notable difference in the model performance.;arxiv
2458;Comparison of deep learning-based algorithms between independent datasets with different labeling methods should be done with caution.;arxiv
2459;Learning properties of large graphs from samples has been an important problem in statistical network analysis since the early work of Goodman \cite{Goodman1949} and Frank \cite{Frank1978}.;arxiv
2460;We revisit a problem formulated by Frank \cite{Frank1978} of estimating the number of connected components in a large graph based on the subgraph sampling model, in which we randomly sample a subset of the vertices and observe the induced subgraph.;arxiv
2461;The key question is whether accurate estimation is achievable in the \emph{sublinear} regime where only a vanishing fraction of the vertices are sampled.;arxiv
2462;We show that it is impossible if the parent graph is allowed to contain high-degree vertices or long induced cycles.;arxiv
2463;For the class of chordal graphs, where induced cycles of length four or above are forbidden, we characterize the optimal sample complexity within constant factors and construct linear-time estimators that provably achieve these bounds.;arxiv
2464;This significantly expands the scope of previous results which have focused on unbiased estimators and special classes of graphs such as forests or cliques.;arxiv
2465;Both the construction and the analysis of the proposed methodology rely on combinatorial properties of chordal graphs and identities of induced subgraph counts.;arxiv
2466;They, in turn, also play a key role in proving minimax lower bounds based on construction of random instances of graphs with matching structures of small subgraphs.;arxiv
2467;Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP.;arxiv
2468;These features are extracted based on prior knowledge such as, speech perception or/and speech production.;arxiv
2469;This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters.;arxiv
2470;Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework.;arxiv
2471;Thus, the network learns linearly separable features from raw speech.;arxiv
2472;We show that such system yields similar or better performance than MLP based system using cepstral-based features as input.;arxiv
2473;Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science.;arxiv
2474;In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss.;arxiv
2475;First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework.;arxiv
2476;We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest.;arxiv
2477;Next, we show a general method of reducing the problem of designing an LSH scheme for a Krein kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search.;arxiv
2478;We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.;arxiv
2479;Representation learning for knowledge graphs (KGs) has focused on the problem of answering simple link prediction queries.;arxiv
2480;In this work we address the more ambitious challenge of predicting the answers of conjunctive queries with multiple missing entities.;arxiv
2481;We propose Bi-Directional Query Embedding (BIQE), a method that embeds conjunctive queries with models based on bi-directional attention mechanisms.;arxiv
2482;Contrary to prior work, bidirectional self-attention can capture interactions among all the elements of a query graph.;arxiv
2483;We introduce a new dataset for predicting the answer of conjunctive query and conduct experiments that show BIQE significantly outperforming state of the art baselines.;arxiv
2484;The choice of approximate posterior distribution is one of the core problems in variational inference.;arxiv
2485;Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations.;arxiv
2486;This restriction has a significant impact on the quality of inferences made using variational methods.;arxiv
2487;We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions.;arxiv
2488;Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained.;arxiv
2489;We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations.;arxiv
2490;We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.;arxiv
2491;Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance.;arxiv
2492;Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates.;arxiv
2493;To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains.;arxiv
2494;We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains.;arxiv
2495;Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.;arxiv
2496;While computer vision has received increasing attention in computer science over the last decade, there are few efforts in applying this to leverage engineering design research.;arxiv
2497;Existing datasets and technologies allow researchers to capture and access more observations and video files, hence analysis is becoming a limiting factor.;arxiv
2498;Therefore, this paper is investigating the application of machine learning, namely object detection methods to aid in the analysis of physical porotypes.;arxiv
2499;With access to a large dataset of digitally captured physical prototypes from early-stage development projects (5950 images from 850 prototypes), the authors investigate applications that can be used for analysing this dataset.;arxiv
